<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Transformers PEFT库支持的高效微调方法介绍</title>
    <link href="/posts/9cdded66/"/>
    <url>/posts/9cdded66/</url>
    
    <content type="html"><![CDATA[<h1 id="peft简介">PEFT简介</h1><p>自BERT问世后，NLP任务的主流范式变为：<strong>预训练语言模型（PLMs）+微调。</strong>但是现在PLMs的趋势就是模型参数量越来越大，BERT-base只有0.1B参数，而现在的大模型起步就要6、7B。因此，对大规模PLMs进行微调的成本十分高昂。</p><p>参数高效微调（Parameter-EfficientFine-Tuning，PEFT）是一种微调策略，<strong>旨在仅训练少量参数使模型适应到下游任务</strong>，在一些场景下甚至不输于全量微调，极大降低了计算和存储成本。</p><p>PEFT通过<strong>冻结预训练模型的某些层，仅微调特定于下游任务的最后几层来实现这种效率</strong>，只对模型的一小部分参数（这部分可能来源于模型自身，也可能是外部引入的）进行训练，在计算资源有限的情况下十分有用。</p><figure><img src="/images/10-PEFT经典方法介绍/1723519595024-09e2ee05-7900-44a3-a851-e7d13bce3f6b.png" alt="《Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning》：常见的 PEFT 方法分类"><figcaption aria-hidden="true">《Scaling Down to Scale Up: A Guide toParameter-Efficient Fine-Tuning》：常见的 PEFT 方法分类</figcaption></figure><p>如图，<a href="https://arxiv.org/pdf/2303.15647">论文</a>将PEFT技术大致分为三类：</p><ol type="1"><li><strong>引入额外参数（additive）</strong>：这部分又包括适配器（adapters）和软提示（softprompts），当然hardprompts也属于这类。要注意的是关于adapter，在PEFT里并没有现成的实现。</li><li><strong>选择部分参数（selective）</strong>：选择参数进行更新。</li><li><strong>引入重参数（reparametrization-based）</strong>：最著名的当属LoRa。</li></ol><blockquote><p>微调 &amp; 参数高效微调：</p><ul><li>微调是在预训练好的模型上，用新的数据在新的任务上进一步训练，所有参数都会被训练。</li><li>参数高效微调是只训练预训练语言模型参数的子集，更新这些关键参数。</li></ul></blockquote><p>下面以一个生成式对话机器人（Bloom模型）为例，比较几个经典PEFT方法下需要被更新的参数量。数据集：<a href="https://huggingface.co/datasets/shibing624/alpaca-zh">https://huggingface.co/datasets/shibing624/alpaca-zh</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br></code></pre></td></tr></table></figure><p>先估算一下如果要对这个模型进行全量微调所需要额显存，可以看到这个模型参数可达到1.3B（B是billion，十亿），粗略算一下跑整个模型要占用的显存至少需要20个G。</p><p><img src="/images/10-PEFT经典方法介绍/1723531317335-73dc2c02-592e-4de1-9eb8-4257bc08388f.png"></p><h1 id="bitfit">1. BitFit</h1><blockquote><p><em>《BitFit: Simple Parameter-efficient Fine-tuning orTransformer-based Masked Language-models》</em></p></blockquote><p><code>BitFit</code>是一种稀疏的微调方法，它<strong>选择模型里的 bias参数</strong>进行更新（属于selective类）。实现方式就是将所有非 bias部分的是否可求导<code>param.requires_grad</code>设置为 False。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;bias&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> name:<br>        param.requires_grad = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">else</span>:<br>        num_param += param.numel()<br></code></pre></td></tr></table></figure><p>可以看到，使用<code>BitFit</code>方法微调涉及到的参数只有54万，大概占总参数的0.04%。</p><p><img src="/images/10-PEFT经典方法介绍/1723646508218-c58a99b9-af45-4422-9790-3ae704599f00.png"></p><p>未经过<code>BitFit</code>微调前，模型的推理效果如下，可以看到模型的回答会出现反复的情况。</p><p><img src="/images/10-PEFT经典方法介绍/1723646969962-2cfd76b9-a92d-48a9-bb9d-79012abb4fd0.png"></p><p>经过<code>BitFit</code>微调后训练一段时间，每 step 打印 log，可以看到loss 还是能收敛的。</p><p><img src="/images/10-PEFT经典方法介绍/1723702086290-678f1fb7-ea90-457c-a48a-2547084ed38b.png"></p><p><img src="/images/10-PEFT经典方法介绍/1723702105413-37a8be11-cf49-4adb-ad63-2a2dfa507e63.png"></p><p>模型效果也会得到提升。</p><p><img src="/images/10-PEFT经典方法介绍/1723702145302-5bd417f9-4b93-4906-88df-35db097a240d.png"></p><blockquote><p>PEFT库里没有实现<code>BitFit</code>，实现的大致思路是把可训练的部分参数训练完取出来save，在下次加载模型的时候除了加载原模型，还要用save的参数替换原模型的参数。</p></blockquote><h1 id="prompt-tuning">2. Prompt-Tuning</h1><blockquote><p><em>《The Power of Scale for Parameter-Efficient PromptTuning》</em></p></blockquote><p><code>Prompt-Tuning</code>会冻结主模型的全部参数，在训练数据前加入一小段prompt，<strong>只训练prompt对应的embedding</strong>，然后将 PromptEmbedding 与 Input Embedding 拼接起来一起送入 TransformerBlocks（属于additive类）。prompt 又分为 hard prompt 和 soft prompt。</p><ul><li><strong>hardprompt</strong>：prompt内容是人为定义的自然语言，比如“对输入内容进行文本摘要”。</li><li><strong>softprompt</strong>：prompt内容是一组可学习的参数，通常是一个小的嵌入向量，在训练过程中不断被优化。</li></ul><figure><img src="/images/10-PEFT经典方法介绍/1723708945754-b43fcd4d-9940-4069-8603-422a68d2c6ba.png" alt="图来自：https://github.com/zyds/transformers-code/"><figcaption aria-hidden="true">图来自：https://github.com/zyds/transformers-code/</figcaption></figure><p>在加载原始模型之后创建 peft model，主要分为两步：首先构造配置信息，然后根据配置信息创建模型。先导入必要的包：</p><ul><li><code>PromptTuningConfig</code>与<code>get_peft_model</code>结合使用来加载peft model；</li><li><code>TaskType</code>用于指定任务类型；</li><li><code>CAUSAL_LM</code>也就是本次例子使用的模型类型，即CausalLanguage Model（因果语言模型）；</li><li><code>PromptTuningInit</code>可以用于控制是 hard prompt 还是 softprompt 。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PromptTuningConfig, get_peft_model, TaskType, PromptTuningInit<br></code></pre></td></tr></table></figure><blockquote><p><code>TaskType</code>里还支持很多类型的任务，不过<code>CAUSAL_LM</code>基本支持各类peft 方法，其他任务不一定。</p><p><img src="/images/10-PEFT经典方法介绍/1723712272747-ac38e08d-be33-408a-974e-a6dc215545de.png"></p></blockquote><h2 id="soft-prompt">2.1 soft prompt</h2><p>先来说 soft prompt 的配置信息，它的 prompt内容不需要人为指定，是让模型自己去学的，所以用<code>num_virtual_tokens</code>指定prompt长度就可以创建简单的soft prompt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载原始 model</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><span class="hljs-comment"># 构造配置信息</span><br>config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 创建 peft model</span><br>model = get_peft_model(model, config)<br></code></pre></td></tr></table></figure><p>此时<code>PromptTuningInit</code>未指定，默认是<code>RANDOM</code>随机初始化。</p><p><img src="/images/10-PEFT经典方法介绍/1723778043601-a6606ba4-3f05-4c7d-83c6-138f0beee39e.png"></p><p>可以看到，在<code>get_peft_model</code>后，模型外套了一个PeftModel，并且在末尾可以看到一个embedding层维度是10(num_virtual_tokens)×2048，对应的就是soft prompt。</p><p><img src="/images/10-PEFT经典方法介绍/1723714404724-84451b18-dac1-440d-8541-6df2a9b022b7.png"></p><p><img src="/images/10-PEFT经典方法介绍/1723777764691-49a36ad5-b015-4e7b-9278-772982e1c53d.png"></p><p>此时，模型要训练的参数量就是 soft prompt 的 embedding涉及的参数量，即20480，占总参数的0.001%。</p><p><img src="/images/10-PEFT经典方法介绍/1723777866137-420becc9-cca2-4a4e-afeb-650af8077a28.png"></p><p>但是因为这种方式涉及的参数太少，loss可能会降得很慢，可以看到刚开始一直徘徊在3左右，所以需要训练更多的轮数来达到比较好的效果。</p><p><img src="/images/10-PEFT经典方法介绍/1723781569308-b0f18538-3ae4-4b23-8d90-92a564d7764b.png"></p><p>训练后加载 peft model，这里需要重新执行一下之前加载原始 model的单元格，<code>model_id</code>指向我们用<code>Prompt-Tuning</code>方法冻结参数训练后保存的模型路径。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel<br><br>peft_model = PeftModel.from_pretrained(model=model, model_id=<span class="hljs-string">&quot;./chatbot/checkpoint-20/&quot;</span>)<br></code></pre></td></tr></table></figure><p>模型推理效果（如果这一步报错 NoneType，要检查模型和数据是否都在cpu/gpu 上）。</p><p><img src="/images/10-PEFT经典方法介绍/1723781740014-f1391158-188b-4379-9f91-ed26cdbfba6a.png"></p><h2 id="hard-prompt">2.2 hard prompt</h2><p>再说hard prompt 的配置信息，它的 prompt内容是人为指定的自然语言，所以这里就需要通过<code>prompt_tuning_init</code>指定初始化方式为<code>TEXT</code>，并<code>prompt_tuning_init_text</code>指定prompt 内容。此时，<code>num_virtual_tokens</code>的大小就是 prompt分词后的长度，用<code>tokenizer_name_or_path</code>指定 prompt使用的分词器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载原始模型</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><span class="hljs-comment"># 构造配置信息</span><br>config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM,<br>                            prompt_tuning_init=PromptTuningInit.TEXT,<br>                            prompt_tuning_init_text=<span class="hljs-string">&quot;下面是一段人与机器人的对话。&quot;</span>,<br>                            num_virtual_tokens=<span class="hljs-built_in">len</span>(tokenizer(<span class="hljs-string">&quot;下面是一段人与机器人的对话。&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]),<br>                            tokenizer_name_or_path=<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><span class="hljs-comment"># 创建 peft model</span><br>model = get_peft_model(model, config)<br></code></pre></td></tr></table></figure><p>此时，模型的<code>num_virtual_tokens=8</code>，模型要训练的参数量是8×2048 =16384，占总参数量的0.001%。</p><p><img src="/images/10-PEFT经典方法介绍/1723782211299-0a61ef6e-0360-4fc3-800f-f8d0b73cf15a.png"></p><p>可以看到 hard prompt 方式的 loss 下降得更快。</p><p><img src="/images/10-PEFT经典方法介绍/1723788226239-ce3d76fa-0a5e-4715-abee-0e2458ad084a.png"></p><p><img src="/images/10-PEFT经典方法介绍/1723788697443-9bee7e2d-31eb-4a2d-bc1e-9a5f43ac43ec.png"></p><blockquote><p>此外，Prompt Tuning 还提出了 Prompt Ensembling，也就是<strong>在一个Batch 里同时训练同一个任务的不同prompt（即采用多种不同方式询问同一个问题）</strong>，这样相当于训练了不同模型，比模型集成的成本小很多。</p></blockquote><h1 id="p-tuning">3. P-Tuning</h1><blockquote><p><em>《GPT Understands, Too》</em></p></blockquote><p><code>P-Tuning</code> 只支持 soft prompt，它把 Prompt Embedding变为一个 Prompt Encoder，在 Prompt-Tuning 的基础上<strong>对 Prompt部分进行编码计算</strong>，来解决前面 soft prompt优化太慢的问题，加速收敛。PEFT 中支持两种编码方式，LSTM 或者是 MLP。</p><figure><img src="/images/10-PEFT经典方法介绍/1723779614610-64a2ddcb-ab73-44cf-82ce-8f89c1e953be.png" alt="图来自：https://github.com/zyds/transformers-code/"><figcaption aria-hidden="true">图来自：https://github.com/zyds/transformers-code/</figcaption></figure><p>相比于之前，这里要用到的包是<code>PromptEncoderConfig</code>和<code>PromptEncoderReparameterizationType</code>，如果不指定后者的话，默认是使用MLP 编码，会有三个全连接层；如果指定是 LSTM 就是 LSTM加两个全连接层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PromptEncoderConfig, TaskType, get_peft_model, PromptEncoderReparameterizationType<br><br><span class="hljs-comment"># 加载原始模型</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><span class="hljs-comment"># 构造配置信息</span><br>config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 创建 peft model</span><br>model = get_peft_model(model, config)<br></code></pre></td></tr></table></figure><h2 id="lstm">LSTM</h2><p>LSTM层可以控制的参数：<code>encoder_dropout</code>、<code>encoder_num_layers</code>、<code>encoder_hidden_size</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=<span class="hljs-number">10</span>,<br>                             encoder_reparameterization_type=PromptEncoderReparameterizationType.LSTM,<br>                             encoder_dropout=<span class="hljs-number">0.1</span>, encoder_num_layers=<span class="hljs-number">5</span>, encoder_hidden_size=<span class="hljs-number">1024</span>)<br></code></pre></td></tr></table></figure><p>使用 LSTM编码涉及到的参数量更大，比重达到9%，很难把显存控制在8G以内。</p><p><img src="/images/10-PEFT经典方法介绍/1723791493659-6668eb20-254e-4dd6-a0cf-6dbe8dd1ffba.png"></p><h2 id="mlp">MLP</h2><p>MLP层可以控制的参数：<code>encoder_dropout</code>、<code>encoder_hidden_size</code>，<code>encoder_num_layers</code>一直为2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=<span class="hljs-number">10</span>,<br>                             encoder_reparameterization_type=PromptEncoderReparameterizationType.MLP,<br>                             encoder_dropout=<span class="hljs-number">0.1</span>, encoder_hidden_size=<span class="hljs-number">1024</span>)<br></code></pre></td></tr></table></figure><p>使用 MLP 编码所涉及到的参数量占总参数量的0.4%。</p><p><img src="/images/10-PEFT经典方法介绍/1723789960517-4e5a41b3-12ac-4277-bf09-5e3c6263fbf4.png"></p><h1 id="p-tuning-v2">4. P-Tuning v2</h1><blockquote><p><em>《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuningUniversally Across Scales and Tasks》</em></p></blockquote><p><code>P-Tuning v2</code>具体做法基本同<code>Prefix-Tuning</code>，可以看作是将文本生成的<code>Prefix-Tuning</code>技术适配到NLU（自然语言理解）任务中，<strong>在每一层都加入了Promptstokens作为输入，而不是仅仅加在输入层，且位置也不局限于前缀，也可以是中间或尾部，可以是离散的token或连续的向量</strong>。这带来两个方面的好处：</p><ul><li>更多可学习的参数（从P-tuning和PromptTuning的0.01%增加到0.1%-3%）。</li><li>加入到更深层结构中的Prompt能给模型预测带来更直接的影响。</li></ul><p><code>P-Tuning v2</code>还做了一些改进：</p><ul><li>移除了重参数化的编码器（如 Prefix-Tuning 中的 MLP、P-Tuning 中的LSTM）；</li><li>针对不同任务采用不同的提示长度；</li><li>引入多任务学习；</li><li>回归传统的分类标签范式，而不是映射器（verbalizer）。</li></ul><h1 id="prefix-tuning">5. Prefix-Tuning</h1><blockquote><p><em>《Prefix-Tuning: Optimizing Continuous Prompts forGeneration》</em></p></blockquote><p><code>P-Tuning</code>是训练一个额外的 prompt编码器来调整模型的输入表示，<strong>仅限于输入层</strong>；而<code>Prefix-Tuning</code>是将其作为可学习的前缀放在transformer blocks的<strong>每一层输入</strong>中，从而引导模型的计算（属于additive类）。</p><p><img src="/images/10-PEFT经典方法介绍/1724520076971-573dc8b7-fde9-461c-8a5d-592aa2f591a9.png"></p><p>具体来说，prefix 会影响当前时间步 key 和 value的计算，计算结果随后会变成后续时间步的 past_key 和 past_values继续影响后面注意力的计算，引导模型生成更符合特定任务的输出。</p><blockquote><p><code>past_key_value</code>是生成式模型在处理长序列时提高效率的一个trick。在解码时模型是根据历史输入预测下一个 token的，在这个过程中会产生大量重复的计算，因为历史输入是不受后面的词影响的，所以可以将过去计算的key 和 value 缓存下来，作为 past_key_value输入到下一次计算中，这一技术又被称为<code>kv_cache</code>。</p><p><img src="/images/10-PEFT经典方法介绍/1724521153163-042f5a27-5913-455f-95dd-11d12fc60140.png"></p></blockquote><p>增加 prefix 并不会影响输入序列 <span class="math inline">\(X\)</span>与 <span class="math inline">\({ Q, K, V }\)</span> 计算后的维度。假设<span class="math inline">\(X\)</span> 的原始维度为 <span class="math inline">\(m \times n\)</span>，<span class="math inline">\(m\)</span> 时 token 的数量，<span class="math inline">\(n\)</span> 是 token 嵌入后向量的长度；增加的prefix 维度是 <span class="math inline">\(p\timesn\)</span>，则最后新的输入序列 <span class="math inline">\(X&#39;\)</span> 维度变为 <span class="math inline">\((m+p)\times n\)</span>，经过线性变换后得到的 <span class="math inline">\({ Q, K, V }\)</span> 维度也变为 <span class="math inline">\((m+p)\times n\)</span>，注意力计算的结果维度依旧是<span class="math inline">\((m+p)\times n\)</span>，与 <span class="math inline">\(X\)</span> 对应。</p><blockquote><p>关于注意力计算过程详见之前的文章<a href="https://jiangcara.github.io/posts/bda47da5/">Transform学习笔记</a>。</p></blockquote><p><code>Prefix-Tuning</code>要使用到的包是<code>PrefixTuningConfig</code>。</p><ul><li><code>num_virtual_tokens</code>：指定可学习前缀的长度；</li><li><code>prefix_projection</code>：用于控制全连接层（两层MLP）要不要插入可学习前缀，默认是False。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PrefixTuningConfig, get_peft_model, TaskType<br><br><span class="hljs-comment"># 加载原始模型</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><span class="hljs-comment"># 构造配置信息</span><br>config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=<span class="hljs-number">10</span>, prefix_projection=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 创建 peft model</span><br>model = get_peft_model(model, config)<br></code></pre></td></tr></table></figure><p>此时模型新增的部分如下图。</p><p><img src="/images/10-PEFT经典方法介绍/1724684661618-bfe5cae4-742c-49fd-8bf5-0ccc65afc8ff.png"></p><p>涉及的参数量占总参数的 0.07%。</p><p><img src="/images/10-PEFT经典方法介绍/1724684698696-c701e258-6ffd-4888-ae3f-0f9420b1b615.png"></p><p>如果默认<code>prefix_projection=False</code>，可以发现训练的时候 loss收敛的会比较慢，类似 Prompt-Tuning 里的 softprompt。因此可以设置<code>prefix_projection=True</code>，将重参数层打开，加快loss收敛，当然涉及到的参数量也会增加很多，显存占用也会增多。<code>out_features</code>大小可以用参数<code>encoder_hidden_size</code>控制。</p><p><img src="/images/10-PEFT经典方法介绍/1724685459500-73bd1097-46e7-43ed-96bd-2f673c6f1066.png"></p><p>要注意的是，针对不同的模型结构，需要构造不同的Prefix。</p><ul><li><strong>针对自回归架构模型</strong>：<strong>在句子前面添加前缀</strong>，得到<code>z = [PREFIX; x; y]</code>，合适的上文能够在固定 LM的情况下去引导生成下文（比如：GPT3的上下文学习）。</li><li><strong>针对编码器-解码器架构模型</strong>：<strong>Encoder和Decoder都增加了前缀</strong>，得到<code>z = [PREFIX; x; PREFIX0; y]</code>。Encoder端增加前缀是为了引导输入部分的编码，Decoder端增加前缀是为了引导后续token的生成。</li></ul><h1 id="lora">6. LoRA</h1><blockquote><p><em>《LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS》</em></p></blockquote><p><code>LoRA</code>的核心思想就是<strong>通过低秩分解来模拟参数的改变量，</strong>从而以极小的参数量来实现大模型的间接训练（属于重参数类）<strong>。</strong></p><p><img src="/images/10-PEFT经典方法介绍/1729348503266-45c559ee-752c-4aba-b00f-b1579c94c1a4.png"></p><p>在涉及到矩阵相乘的模块，在原始的PLM旁边增加一个新的通路，通过前后两个矩阵<span class="math inline">\(A,B\)</span> 相乘，第一个矩阵 <span class="math inline">\(A\)</span> 负责降维，第二个矩阵 <span class="math inline">\(B\)</span> 负责升维，中间层维度为 <span class="math inline">\(r\)</span>，即 <span class="math inline">\(h=W_0x+ΔWx=W_0x+{BA}x\)</span>。</p><p><img src="/images/10-PEFT经典方法介绍/1729348518523-529ba377-97fa-40e6-9547-e1dfcb027291.png"></p><p>可训练层维度和预训练模型层维度一致为 <span class="math inline">\(d\)</span>，先将维度<span class="math inline">\(d\)</span>通过全连接层降维至 <span class="math inline">\(r\)</span>，再从 <span class="math inline">\(r\)</span> 通过全连接层映射回 <span class="math inline">\(d\)</span> 维度，其中 <span class="math inline">\(r&lt;&lt;d\)</span>，<span class="math inline">\(r\)</span> 是矩阵的秩。这样矩阵计算就从 <span class="math inline">\(d \times d\)</span> 变为 <span class="math inline">\(d \times r + r \times d\)</span>，减少参数量。</p><p><code>LoRA</code>使用到的包是<code>LoraConfig</code>。</p><ul><li><code>target_module</code>：指定新增通路的部分，比如默认的模块是<code>['query_key_value']</code>；</li><li><code>r</code>：指定中间层的维度，默认是8；</li><li><code>modules_to_save</code>：指定除了lora微调的部分还想要训练的部分；</li><li><code>lora_alpha</code>：缩放因子，控制lora微调参数的权重。</li></ul><p>具体参数说明可以看<code>LoraConfig</code>里的说明。</p><p><img src="/images/10-PEFT经典方法介绍/1729355405804-9045ff5f-5704-47c0-85ef-1b16a8540d7e.png"></p><p>通过正则指定对 module name 中 1 开头的 module 进行微调，同时训练word_embeddings 部分的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, get_peft_model, TaskType<br><br><span class="hljs-comment"># 加载原始模型</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><span class="hljs-comment"># 构造配置信息</span><br>config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=<span class="hljs-string">&quot;.*\.1.*query_key_value&quot;</span>, modules_to_save=[<span class="hljs-string">&quot;word_embeddings&quot;</span>])<br><span class="hljs-comment"># 创建 peft model</span><br>model = get_peft_model(model, config)<br></code></pre></td></tr></table></figure><p><img src="/images/10-PEFT经典方法介绍/1729354204923-2a8cb10c-d2df-432d-9577-4c92ab1222bf.png"></p><p>执行成功后的模型信息如图，可以看到 query_key_value 部分包含 loraA 和loraB两块，word_embedding层也发生了改变。</p><p><img src="/images/10-PEFT经典方法介绍/1729356192045-dfafa17f-82b2-446c-9531-5bc38fb6e371.png"></p><p><img src="/images/10-PEFT经典方法介绍/1729355698102-bdad15ba-cb87-43f6-a4d1-b5eb62b30996.png"></p><p>训练20%左右的推理效果如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model = model.cuda()<br>ipt = tokenizer(<span class="hljs-string">&quot;Human: &#123;&#125;\n&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">&quot;考试有哪些技巧？&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).strip() + <span class="hljs-string">&quot;\n\nAssistant: &quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)<br>tokenizer.decode(model.generate(**ipt, max_length=<span class="hljs-number">128</span>, do_sample=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/10-PEFT经典方法介绍/1729356467076-146391a3-6752-4312-aee4-a07e7c24b8da.png"></p><p>保存 lora 微调后的模型，并与原始模型合并保存到本地。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel<br><br><span class="hljs-comment"># 加载基础模型</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>, low_cpu_mem_usage=<span class="hljs-literal">True</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Langboat/bloom-1b4-zh&quot;</span>)<br><br><span class="hljs-comment"># 加载 lora 模型</span><br>p_model = PeftModel.from_pretrained(model, model_id=<span class="hljs-string">&quot;./chatbot/checkpoint-500/&quot;</span>)<br><span class="hljs-comment"># 模型合并并保存到本地</span><br>merge_model = p_model.merge_and_unload()<br>merge_model.save_pretrained(<span class="hljs-string">&quot;./chatbot/merge_model&quot;</span>)<br><br><span class="hljs-comment"># 使用模型进行推理</span><br>ipt = tokenizer(<span class="hljs-string">&quot;Human: &#123;&#125;\n&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">&quot;考试有哪些技巧？&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).strip() + <span class="hljs-string">&quot;\n\nAssistant: &quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>tokenizer.decode(merge_model.generate(**ipt, max_length=<span class="hljs-number">128</span>, do_sample=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/10-PEFT经典方法介绍/1729356725547-b8b8a640-98c4-4a47-8292-e74e34e32a7f.png"></p><p><code>LoRA</code>支持的模型可以再<code>peft.utils</code>文件中找到，其中<code>TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING</code>包含了<code>LoRA</code>支持的模型中默认支持<code>LoRA</code>的module。</p><p><img src="/images/10-PEFT经典方法介绍/1729356942879-12ca9138-9753-40cd-8a84-a19360db725c.png"></p><p><img src="/images/10-PEFT经典方法介绍/1729357007346-6f559ec5-8b47-4ad0-81a7-2fb6b13ca396.png"></p><p>如果微调的模型不在默认支持列表里，需要在这里定义一下模型和默认 LoRA微调的部分。</p><h2 id="adalora">AdaLoRA</h2><blockquote><p><em>《ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENTFINE-TUNING》</em></p></blockquote><p><code>AdaLoRA</code>是对<code>LoRA</code>的一种改进，它根据<strong>重要性评分动态分配参数预算给权重矩阵</strong>。具体做法如下：</p><ul><li><strong>调整增量矩分配</strong>。<code>AdaLoRA</code>将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合并节省计算预算。</li><li><strong>以奇异值分解的形式对增量更新进行参数化，并根据重要性指标裁剪掉不重要的奇异值，同时保留奇异向量</strong>。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法通过减少它们的参数预算来加速计算，同时，保留未来恢复的可能性并稳定训练。</li></ul><p><span class="math display">\[W=W(0)+Δ=W(0)+PΛQ.\]</span></p><ul><li><strong>在训练损失中添加了额外的惩罚项</strong>，规范奇异矩阵 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span>的正交性，避免SVD的大量计算并稳定训练。</li></ul><h2 id="qlora">QLoRA</h2><blockquote><p><em>《QLORA: Efficient Finetuning of Quantized LLMs》</em></p></blockquote><p><code>QLoRA</code>使用<strong>一种新颖的高精度技术将预训练模型量化为4bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调</strong>。</p><p><code>QLoRA</code>有一种低精度存储数据类型（4bit），还有一种计算数据类型（BFloat16）。实际上，这意味着无论何时使用QLoRA 权重张量，我们都会将张量反量化为 BFloat16，然后执行 16位矩阵乘法。<code>QLoRA</code>提出了两种技术实现高保真 4bit微调——<strong>4 bit NormalFloat(NF4)量化和双量化</strong>。此外，还引入了<strong>分页优化器</strong>，以防止梯度检查点期间的内存峰值，从而导致内存不足的错误，这些错误在过去使得大型模型难以在单台机器上进行微调。</p><ul><li><strong>4bitNormalFloat</strong>（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比4 bit整数和 4bit 浮点数更好的实证结果。</li><li><strong>双量化</strong>：对第一次量化后的那些常量再进行一次量化，减少存储空间。</li><li><strong>分页优化器</strong>：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的GPU 处理。该功能的工作方式类似于 CPU内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存，然后在GPU 内存不足时将其自动卸载到 CPU内存，并在优化器更新步骤需要时将其加载回 GPU 内存。</li></ul><p><img src="/images/10-PEFT经典方法介绍/1729514618459-ecb2d685-b5ff-467a-b159-d8566197dd9f.png"></p><h1 id="ia3">7. IA3</h1><blockquote><p><em>《Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaperthan In-Context Learning》</em></p></blockquote><p><code>IA3</code>是对模型的一些<strong>激活层进行抑制或放大</strong>，也就是<strong>通过点乘一个可学习向量的形式对模型的一部分参数进行加权</strong>。可以看到，<code>IA3</code>的参数分为两部分你，一部分是<span class="math inline">\(l_V\)</span> 和 <span class="math inline">\(l_K\)</span>，一部分是 FFN 部分的 <span class="math inline">\(l_{ff}\)</span>。</p><p><img src="/images/10-PEFT经典方法介绍/1729357455586-a03b5b24-00c5-41a6-a3f2-7843353b93fe.png"></p><p>这里要用到的包是<code>IA3Config</code>，其中<code>target_modules</code>和<code>modules_to_save</code>的作用与LoRA 一样。</p><ul><li><code>target_modules</code>：IA3的这部分包含两块，{'query_key_value','mlp.dense_4h_to_h'}；</li><li><code>feedforward_modules</code>：用于指定<code>target_modules</code>中哪块是feedforward 层。</li></ul><blockquote><p>如果是feedforward层，可学习向量与<strong>feedforward的输入相乘</strong>，再进入linear；</p><p>如果不是，可学习向量<strong>与注意力块的输出result相乘</strong>。</p></blockquote><p><img src="/images/10-PEFT经典方法介绍/1729358587959-447657ed-32c4-4bb9-a56e-7d6836cd3894.png"></p><p>涉及的参数量占总参数的 0.02%。</p><p><img src="/images/10-PEFT经典方法介绍/1729358249380-abc343af-0762-4173-885e-2a75b28bf83d.png"></p><p>原论文中提到<code>IA3</code>的最佳学习率是<code>3e-3</code>。</p><h1 id="adapter-tuning">Adapter-Tuning</h1><blockquote><p><em>《Parameter-Efficient Transfer Learning for NLP》</em></p></blockquote><p>预训练模型参数量越来越多，在训练下游任务时进行全量微调变得昂贵且耗时。基于此，作者提出了<code>Adapter</code>。<strong>在预训练模型每层中插入用于下游任务的参数</strong>（针对每个下游任务，仅增加3.6%的参数），<strong>在微调时将模型主体冻结，仅训练特定于任务的参数</strong>，从而减少了训练时的算力开销。</p><p>该方法<strong>设计了Adapter结构</strong>，并将其嵌入Transformer的结构里面，<strong>针对每一个Transformer层，增加了两个Adapter结构（分别是多头注意力的投影之后和第二个feed-forward层之后）</strong>，<strong>在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter 结构和 Layer Norm层进行微调，从而保证了训练的高效性</strong>。</p><p>每当出现新的下游任务，通过添加 Adapter模块来产生一个易于扩展的下游模型，从而避免全量微调与灾难性遗忘的问题。</p><p><img src="/images/10-PEFT经典方法介绍/1729515850473-da0d8718-d707-4acc-9176-54e6fb3ffe85.png"></p><h2 id="adapter-fusion">Adapter Fusion</h2><blockquote><p><em>《AdapterFusion:Non-Destructive Task Composition for TransferLearning》</em></p></blockquote><p><code>Adapter Fusion</code>，<strong>一种融合多任务信息的Adapter的变体</strong>，在<code>Adapter</code>的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。</p><ul><li><strong>知识提取阶段</strong>：在不同任务下引入各自的 Adapter模块，用于学习特定任务的信息。</li><li><strong>知识组合阶段</strong>：将预训练模型参数与特定于任务的Adapter参数固定，<strong>引入新参数（AdapterFusion）来学习组合多个Adapter中的知识，以提高模型在目标任务中的表现</strong>。</li></ul><p>通过AdapterFusion，模型可以为不同的任务对应的adapter分配不同的权重，聚合N个任务的信息，从而为特定任务输出更合适的结果。通过将适配器的训练分为知识提取和知识组合两部分，<strong>解决了灾难性遗忘、任务间干扰和训练不稳定的问题</strong>。但是，Adapter模块的添加也导致模型整体参数量的增加，降低了模型推理时的性能。</p><h2 id="adapterdrop"><strong>AdapterDrop</strong></h2><blockquote><p><em>《AdapterDrop: On the Efficiency of Adapters inTransformers》</em></p></blockquote><p>作者通过对<code>Adapter</code>的计算效率进行分析，<strong>发现与全量微调相比，Adapter在训练时快60%，但是在推理时慢4%-6%</strong>。基于此，作者提出了<code>AdapterDrop</code>方法缓解该问题。</p><p><code>AdapterDrop</code>在不影响任务性能的情况下，<strong>对Adapter动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率</strong>。</p><p><code>AdapterDrop</code><strong>通过从较低的 Transformer层删除可变数量的</strong><code>Adaper</code><strong>来提升推理速度。</strong>当对多个任务执行推理时，动态地减少了运行时的计算开销，并在很大程度上保持了任务性能。</p><h2 id="unipelt">UniPELT</h2><blockquote><p><em>《UNIPELT: A Unified Framework for Parameter-Efficient LanguageModel Tuning》</em></p></blockquote><p>近年来，涌现出了许多针对语言模型的参数高效微调方法，在模型训练参数极大的减少的情况下，模型效果与全量微调相当。但是不同的PELT方法在同一个任务上表现差异可能都非常大，这让针对特定任务选择合适的方法非常繁琐。</p><p>基于此，作者提出了<code>UniPELT</code>方法，<strong>将不同的PELT方法作为子模块，并通过门控机制学习激活最适合当前数据或任务的方法。UniPELT是LoRA、Prefix Tuning和Adapter的门控组合。</strong></p><p>更具体地说，LoRA 重参数化用于 <span class="math inline">\(W_Q\)</span> 和 <span class="math inline">\(W_V\)</span>注意力矩阵，<code>Prefix Tuning</code>应用于每一 Transformer 层的 key 和value，并在 Transformer 块的 feed-forward子层之后添加<code>Adapter</code>。</p><ul><li>对于每个模块，门控被实现为线性层，通过 <span class="math inline">\(G_P\)</span>参数控制<code>Prefix-tuning</code>方法的开关，<span class="math inline">\(G_L\)</span>控制<code>LoRA</code>方法的开关，<span class="math inline">\(G_A\)</span>控制<code>Adapter</code>方法的开关。</li><li>可训练参数包括 LoRA 矩阵 <span class="math inline">\(W_{A/Down}\)</span> 和 <span class="math inline">\(W_{B/up}\)</span>，提示调优参数 <span class="math inline">\(P_K\)</span> 和 <span class="math inline">\(P_V\)</span>、Adapter参数和门函数权重。即图中蓝颜色的参数为可学习的参数。</li></ul><p><img src="/images/10-PEFT经典方法介绍/1729347066826-85c8fc71-7a9a-48c5-b493-80d1c03af17d.png"></p><p><strong>本方法始终优于常规的全量微调以及它在不同设置下包含的子模块，通常超过在每个任务中单独使用每个子模块的最佳性能的上限</strong>；并且，通过研究结果表明，多种PELT 方法的混合涉及到PLM的不同部分可能对模型有效性和鲁棒性都有好处。</p><h1 id="总结">总结</h1><figure><img src="/images/10-PEFT经典方法介绍/image-20241022131301381.png" alt="PEFT方法总结"><figcaption aria-hidden="true">PEFT方法总结</figcaption></figure><blockquote><p>Reference：</p><p><a href="https://www.bilibili.com/video/BV1Xu4y1k7Ls/">【HuggingFaceTransformers-实战篇】参数高效微调</a>、<a href="https://github.com/zyds/transformers-code/tree/master/pptx">PPT</a>；</p><p><a href="https://arxiv.org/pdf/2303.15647">A Guide toParameter-Efficient Fine-Tuning</a>；</p><p><a href="https://github.com/wdndev/llm_interview_note/tree/main/05.%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83">llm面试：有监督微调</a>。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-实战篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>基于 Transformers 的 NER</title>
    <link href="/posts/91e7aae8/"/>
    <url>/posts/91e7aae8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1gW4y197CT/">【HuggingFaceTransformers-实战篇】命名实体识别</a></p></blockquote><h1 id="任务简介">任务简介</h1><p><strong>命名实体识别</strong>（Named Entity Recognition,NER）是信息抽取的一个子任务，旨在提取文本信息中的关键实体并识别其类型，常见的通用类型有PER（人物）、LOC（位置）、ORG（组织）等。<img src="/images/09-基于Transformers的NER/1723189853687-099234f5-8b41-4afc-9e6d-5de0ebfed5d9.png">目前常见的数据标注方法有：IOB1、IOB2、IOOES、IOE等。</p><p>以IOB2为例，I表示实体内部，O表示实体外部，B表示实体开始，一般的数据格式如下：乔，B-PER、布，I-PER、斯，I-PER。如果是IOBES标注，E表示实体的结束，S表示一个词单独构成一个实体。有时会用M替代I，本质上是一个意思。</p><h1 id="任务评估指标">任务评估指标</h1><ul><li><strong>Precision</strong> (P) ：预测正确的实体个数 /预测出的实体个数（预测出的不一定对）；</li><li><strong>Recall</strong> (R) ：预测正确的实体个数 /实际上的实体个数；</li><li><strong>F1</strong>：F1认为准确率P和召回率R同等重要，<span class="math inline">\(F_1=\frac{2PR}{P+R}\)</span>。（还有F2和F0.5）</li></ul><h1 id="基于transformers的中文命名实体识别">基于Transformers的中文命名实体识别</h1><p>对于中文来说，token是每个字，实际上模型是对每个token进行标签分类，模型结构选择<code>ModelForTokenClassification</code>。数据集选择<code>peoples_daily_ner</code>，模型选择<code>hfl/chinese-macbert-base</code>。</p><h2 id="step1-导入相关包">Step1 导入相关包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> evaluate<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification<br></code></pre></td></tr></table></figure><h2 id="step2-加载数据集">Step2 加载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从本地加载数据集</span><br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> DatasetDict<br>ner_datasets = DatasetDict.load_from_disk(<span class="hljs-string">&quot;ner_data&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/09-基于Transformers的NER/1722497993966-32c4e76f-43a1-4a3a-a137-19e3c4c033b7.png"></p><p>可以看到，在这个数据集中已经做了标签的映射，厦门、金门分别对应5、6分别对应B-LOC、I-LOC。</p><p><img src="/images/09-基于Transformers的NER/1722498130012-136bc90a-ec21-41f9-92f7-08dca47e3a83.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取标签列表</span><br>label_list = ner_datasets[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;ner_tags&quot;</span>].feature.names<br></code></pre></td></tr></table></figure><p><img src="/images/09-基于Transformers的NER/1722498245911-54f6fd26-2669-411b-b6a1-3f8b0034d27c.png"></p><h2 id="step3-数据集预处理">Step3 数据集预处理</h2><p>对于分词后的数据（中文就是分字），要指定<code>is_split_into_words=True</code>，不然会对每个字都包裹[CLS] 和[SEP] ，实际上对每句话包裹 [CLS] 和 [SEP] 就可以了。</p><figure><img src="/images/09-基于Transformers的NER/1723206603917-fe986ce7-8550-4ea6-b3c1-b20c84ab2541.png" title="未指定is_split_into_words=True" alt="未指定is_split_into_words=True"><figcaption aria-hidden="true">未指定is_split_into_words=True</figcaption></figure><figure><img src="/images/09-基于Transformers的NER/1723206645006-afda5b6d-760b-4056-a6ae-1c475b47b377.png" title="指定is_split_into_words=True" alt="指定is_split_into_words=True"><figcaption aria-hidden="true">指定is_split_into_words=True</figcaption></figure><p>此外，在计算的过程中，我们要让模型知道哪些是有实际语义、哪些是[CLS]、[SEP] 这种特殊符号，因此还要遍历<code>word_ids</code>，将[CLS]、[SEP]对应的None值置为-100，这个值是交叉熵损失函数计算时会自动忽略的值。将每句话最后得到的编码结果用<code>label_ids</code>存储起来，写入字段<code>labels</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;hfl/chinese-macbert-base&quot;</span>)<br><br><span class="hljs-comment"># 借助 word_ids 实现标签映射</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_function</span>(<span class="hljs-params">examples</span>):<br>    tokenized_exmaples = tokenizer(examples[<span class="hljs-string">&quot;tokens&quot;</span>], max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)<br>    labels = []<br>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">&quot;ner_tags&quot;</span>]):<br>        word_ids = tokenized_exmaples.word_ids(batch_index=i)<br>        label_ids = []<br>        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> word_ids:<br>            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                label_ids.append(-<span class="hljs-number">100</span>)<br>            <span class="hljs-keyword">else</span>:<br>                label_ids.append(label[word_id])<br>        labels.append(label_ids)<br>    tokenized_exmaples[<span class="hljs-string">&quot;labels&quot;</span>] = labels<br>    <span class="hljs-keyword">return</span> tokenized_exmaples<br><br>tokenized_datasets = ner_datasets.<span class="hljs-built_in">map</span>(process_function, batched=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>这里借助<code>word_ids</code>来进行标签映射的原因是，对于中文，一个字对应一个label；但对于英文单词的分词，比如说interestingword，虽然是两个单词但分词后是对应了五个位置的编码的，所以要借助<code>word_ids</code>来区分这五个编码里哪几个是interesting，哪几个是word。即使interesting被分成很多词，<code>label_ids.append(label[word_id])</code>会让它的所有分词结果对应同一个标签。</p><figure><img src="/images/09-基于Transformers的NER/1723207581749-f1c27f17-76fb-4fe6-a060-613640b4b5ca.png" title="interesting word的编码结果" alt="interesting word的编码结果"><figcaption aria-hidden="true">interesting word的编码结果</figcaption></figure><figure><img src="/images/09-基于Transformers的NER/1723207733477-41ecef7e-b839-495a-b7d7-eb7762e502b3.png" title="interesting word的word_ids" alt="interesting word的word_ids"><figcaption aria-hidden="true">interesting word的word_ids</figcaption></figure><p>最后，数据集的特征除了原始字段，多了tokenizer带来的三个字段，以及每句话的编码结果labels。</p><p><img src="/images/09-基于Transformers的NER/1722499653407-f015e09a-0aea-4ad2-93a1-1c9a0fec1a0a.png"></p><h2 id="step4-创建模型">Step4 创建模型</h2><p>分词完成后，创建一个<code>AutoModelForTokenClassification</code>模型。<strong>对于所有的非二分类任务，切记要指定num_labels，否则会报device错误。</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;hfl/chinese-macbert-base&quot;</span>, num_labels=<span class="hljs-built_in">len</span>(label_list))<br></code></pre></td></tr></table></figure></p><h2 id="step5-创建评估函数">Step5 创建评估函数</h2><p>命名实体识别任务要用到一个序列评估函数<code>seqeval</code>，可以pip安装也可以从下载后从本地加载。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 本地加载 seqeval</span><br>seqeval = evaluate.load(<span class="hljs-string">&quot;seqeval_metric.py&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_metric</span>(<span class="hljs-params">pred</span>):<br>    predictions, labels = pred<br>    predictions = np.argmax(, axis=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 将id转换为原始的字符串类型的标签</span><br>    true_predictions = [<br>        [label_list[p] <span class="hljs-keyword">for</span> p, l <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]<br>        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels) <br>    ]<br><br>    true_labels = [<br>        [label_list[l] <span class="hljs-keyword">for</span> p, l <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]<br>        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels) <br>    ]<br><br>    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=<span class="hljs-string">&quot;strict&quot;</span>, scheme=<span class="hljs-string">&quot;IOB2&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;f1&quot;</span>: result[<span class="hljs-string">&quot;overall_f1&quot;</span>]<br>    &#125;<br></code></pre></td></tr></table></figure></p><h2 id="step6-配置训练参数">Step6 配置训练参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;models_for_ner&quot;</span>,<br>    per_device_train_batch_size=<span class="hljs-number">64</span>,<br>    per_device_eval_batch_size=<span class="hljs-number">128</span>,<br>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br>    metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,<br>    load_best_model_at_end=<span class="hljs-literal">True</span>,<br>    logging_steps=<span class="hljs-number">50</span>,<br>    num_train_epochs=<span class="hljs-number">1</span><br>)<br></code></pre></td></tr></table></figure><h2 id="step7-创建训练器">Step7 创建训练器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer = Trainer(<br>    model=model,<br>    args=args,<br>    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],<br>    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],<br>    compute_metrics=eval_metric,<br>    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)<br>)<br></code></pre></td></tr></table></figure><h2 id="step8-模型训练">Step8 模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer.train()<br></code></pre></td></tr></table></figure><p><img src="/images/09-基于Transformers的NER/1723208364966-210dc1da-8071-4d6f-8ed1-ff9eeea134ee.png"></p><h2 id="step9-模型评估">Step9 模型评估</h2><p>可以在测试集上对模型进行评估。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer.evaluate(eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>])<br></code></pre></td></tr></table></figure></p><p><img src="/images/09-基于Transformers的NER/1723208395335-14d1075c-9b30-4a81-a5fc-9535f6101305.png"></p><h2 id="step10-模型预测">Step10 模型预测</h2><p>使用pipeline使用模型进行预测。可以发现预测结果是按token返回的，并且对应的标签是LABEL_3，这是因为模型config里默认的id2label是这样对应的。</p><p><img src="/images/09-基于Transformers的NER/1723209423167-fbc05c45-d893-422b-bfd8-f2521ee93fbf.png"></p><p><img src="/images/09-基于Transformers的NER/1723209184319-3de5720a-9b64-452b-8ff5-1da4dd65b721.png"></p><p>重新写一个id2label对默认的进行覆盖。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.config.id2label = &#123;idx: label <span class="hljs-keyword">for</span> idx, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(label_list)&#125;<br></code></pre></td></tr></table></figure><p><img src="/images/09-基于Transformers的NER/1723209466998-baa5e017-2435-4720-b29b-0c9fe9941ebf.png"></p><p>此外，对于NER任务，如果不希望token的标签是一个一个返回，可以指定<code>aggregation_strategy="simple"</code>让模型返回完整实体。如果模型是基于GPU训练的，那么推理时要指定<code>device=0</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>ner_pipe = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>, model=model, tokenizer=tokenizer, device=<span class="hljs-number">0</span>, aggregation_strategy=<span class="hljs-string">&quot;simple&quot;</span>)<br>res = ner_pipe(<span class="hljs-string">&quot;小明在北京上班&quot;</span>)<br></code></pre></td></tr></table></figure><p>最后得到的结果是这样的，可以发现词与词之间使用空格的，因为tokenizer在解码时默认会用空格连接token。</p><p><img src="/images/09-基于Transformers的NER/1723209542895-6f1ffe92-0839-4b7d-bb42-c2370ad4ec1e.png">可以通过<code>start</code>和<code>end</code>进行取词。进一步规整预测结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 根据start和end取实际的结果</span><br>ner_result = &#123;&#125;<br>x = <span class="hljs-string">&quot;小明在北京上班&quot;</span><br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> res:<br>    <span class="hljs-keyword">if</span> r[<span class="hljs-string">&quot;entity_group&quot;</span>] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> ner_result:<br>        ner_result[r[<span class="hljs-string">&quot;entity_group&quot;</span>]] = []<br>    ner_result[r[<span class="hljs-string">&quot;entity_group&quot;</span>]].append(x[r[<span class="hljs-string">&quot;start&quot;</span>]: r[<span class="hljs-string">&quot;end&quot;</span>]])<br></code></pre></td></tr></table></figure><p><img src="/images/09-基于Transformers的NER/1723209494197-bc7a7183-0b38-4372-ba33-a646487b8e9e.png"></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-实战篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 显存优化策略</title>
    <link href="/posts/7a209f8f/"/>
    <url>/posts/7a209f8f/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV18N411C71F/">【HuggingFaceTransformers-实战篇】显存优化策略</a>，<a href="https://www.cnblogs.com/comor/p/10577128.html">字长模型：LP32、ILP32</a>，<a href="https://github.com/zyds/transformers-code/tree/master/pptx">PPT</a></p></blockquote><h1 id="显存占用因素">显存占用因素</h1><ol type="1"><li><strong>模型权重：</strong>即模型的参数量。BERT-Base模型参数量为110M（0.1B），BERT-Large模型参数量为330M（约0.3B）。它们都是LP32数据模型，因此每个参数需要占四个字节，<code>总空间 = 四字节 × 模型的参数量</code>。</li><li><strong>优化器状态：</strong>优化器的状态也需要存储在显存中。以常用的优化器Adam为例，它需要为每个参数存储八个字节的信息。</li><li><strong>梯度：</strong>在进行反向传播时，梯度信息也需要存储，每个梯度占用四个字节。</li><li><strong>前向激活值：</strong>即计算过程中的中间变量也需要存储，其显存占用取决于序列长度、隐层维度、batchsize等多个因素。</li></ol><p>除此之外，一些其他的因素，如临时缓冲区也会占用显存。</p><h1 id="显存优化策略">显存优化策略</h1><p>为了在4G显存的情况下也能跑BERT-Large，可以尝试以下几种优化策略：</p><table><thead><tr><th style="text-align: center;"><strong>优化策略</strong></th><th style="text-align: center;"><strong>优化对象</strong></th></tr></thead><tbody><tr><td style="text-align: center;">梯度积累（Gradient Accumulation,GA）</td><td style="text-align: center;">前向激活值</td></tr><tr><td style="text-align: center;">梯度选择性保存（Gradient Checkpoints,GC）</td><td style="text-align: center;">前向激活值</td></tr><tr><td style="text-align: center;">选择显存占用更少的优化器（如Adafactor）</td><td style="text-align: center;">优化器状态</td></tr><tr><td style="text-align: center;">参数冻结（Freeze Model）</td><td style="text-align: center;">前向激活值、梯度</td></tr><tr><td style="text-align: center;">数据长度（Data MaxLength）</td><td style="text-align: center;">前向激活值</td></tr></tbody></table><h1 id="对比试验">对比试验</h1><p>使用hfl/chinese-macbert-large进行优化策略前后的效果对比，每次更新策略后要重启kernel，不然会有中间变量留存。优化的主要思想就是<strong>用时间换空间</strong>。（因为我没GPU，训练的时候CPU占用率都是100%没有对比效果，主要看时长变化吧。使用GPU训练的可以打开任务管理器看优化效果，参考<a href="https://github.com/zyds/transformers-code/tree/master/pptx">up给出的表</a>。）</p><p><img src="/images/08-Transformers显存优化策略/1722428866566-25ff18bb-e1a8-46a6-bb68-6fb86db11635.png"></p><blockquote><p>原视频github项目地址：<a href="https://github.com/zyds/transformers-code/">https://github.com/zyds/transformers-code/</a></p></blockquote><h2 id="maxlength128-batchsizebs32">maxlength=128, batchsize(BS)=32</h2><p>处理数据时将padding限制为最大长度，不再动态填充。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_function</span>(<span class="hljs-params">examples</span>):<br>    tokenized_examples = tokenizer(examples[<span class="hljs-string">&quot;review&quot;</span>], max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>)<br>    tokenized_examples[<span class="hljs-string">&quot;labels&quot;</span>] = examples[<span class="hljs-string">&quot;label&quot;</span>]<br>    <span class="hljs-keyword">return</span> tokenized_examples<br></code></pre></td></tr></table></figure>将TrainingArguments里的训练参数进行调整，batchsize为32，只训练一个epoch看效果。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. base</span><br>train_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;./checkpoints&quot;</span>,      <span class="hljs-comment"># 输出文件夹</span><br>                               per_device_train_batch_size=<span class="hljs-number">32</span>,  <span class="hljs-comment"># 训练时的batch_size</span><br>                               per_device_eval_batch_size=<span class="hljs-number">32</span>,   <span class="hljs-comment"># 验证时的batch_size</span><br>                               num_train_epochs=<span class="hljs-number">1</span>,              <span class="hljs-comment"># 训练轮数</span><br>                               logging_steps=<span class="hljs-number">10</span>,                <span class="hljs-comment"># log 打印的频率</span><br>                               evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,     <span class="hljs-comment"># 评估策略</span><br>                               save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,           <span class="hljs-comment"># 保存策略</span><br>                               save_total_limit=<span class="hljs-number">3</span>,              <span class="hljs-comment"># 最大保存数</span><br>                               learning_rate=<span class="hljs-number">2e-5</span>,              <span class="hljs-comment"># 学习率</span><br>                               weight_decay=<span class="hljs-number">0.01</span>,               <span class="hljs-comment"># weight_decay</span><br>                               metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,      <span class="hljs-comment"># 设定评估指标</span><br>                               load_best_model_at_end=<span class="hljs-literal">True</span>)     <span class="hljs-comment"># 训练完成后加载最优模型</span><br></code></pre></td></tr></table></figure>什么优化策略都没有的情况下，一轮训练预计三个半小时，甚至开始跑的时候我电脑已经有点死机了orz……</p><p><img src="/images/08-Transformers显存优化策略/1722415356344-ddbe9c4e-7afd-4e6b-9a72-1b743e8c769b.png"></p><h2 id="maxlength128-bs1-ga32">maxlength=128, BS=1, GA=32</h2><p>batchsize越大对机器计算性能要求较高，但batchsize设置过低可能会影响模型效果，所以可以采取缓解措施，即设置梯度积累<code>gradient_accumulation_steps=32</code>。这样即使batchsize=1，他会每计算32条数据以后一起进行一次优化。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># test_1：使用梯度积累策略</span><br>train_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;./checkpoints&quot;</span>,      <span class="hljs-comment"># 输出文件夹</span><br>                               per_device_train_batch_size=<span class="hljs-number">1</span>,   <span class="hljs-comment"># 训练时的batch_size</span><br>                               gradient_accumulation_steps=<span class="hljs-number">32</span>,  <span class="hljs-comment"># *** 1. 梯度累加 ***                               </span><br>                               per_device_eval_batch_size=<span class="hljs-number">1</span>,    <span class="hljs-comment"># 验证时的batch_size</span><br>                               num_train_epochs=<span class="hljs-number">1</span>,              <span class="hljs-comment"># 训练轮数</span><br>                               logging_steps=<span class="hljs-number">10</span>,                <span class="hljs-comment"># log 打印的频率</span><br>                               evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,     <span class="hljs-comment"># 评估策略</span><br>                               save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,           <span class="hljs-comment"># 保存策略</span><br>                               save_total_limit=<span class="hljs-number">3</span>,              <span class="hljs-comment"># 最大保存数</span><br>                               learning_rate=<span class="hljs-number">2e-5</span>,              <span class="hljs-comment"># 学习率</span><br>                               weight_decay=<span class="hljs-number">0.01</span>,               <span class="hljs-comment"># weight_decay</span><br>                               metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,      <span class="hljs-comment"># 设定评估指标</span><br>                               load_best_model_at_end=<span class="hljs-literal">True</span>)     <span class="hljs-comment"># 训练完成后加载最优模型</span><br></code></pre></td></tr></table></figure> 我这时间减少了些。</p><p><img src="/images/08-Transformers显存优化策略/1722416427732-b05610a0-dab1-4ca4-9d54-5b501f423ff7.png"></p><h2 id="maxlength128-bs1-ga32-gctrue">maxlength=128, BS=1, GA=32,GC=True</h2><p>还可以针对保存中间变量的策略进行优化，设置<code>gradient_checkpointing=True</code>，不保存全部中间变量，而是选择性的存一些。对于没存下来的激活值，可以在反向传播计算梯度的时候重新计算，小模型可能优化效果不明显但是放到大语言模型上是很有用的。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># test_2：增加梯度检查点</span><br>train_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;./checkpoints&quot;</span>,      <span class="hljs-comment"># 输出文件夹</span><br>                               per_device_train_batch_size=<span class="hljs-number">1</span>,   <span class="hljs-comment"># 训练时的batch_size</span><br>                               gradient_accumulation_steps=<span class="hljs-number">32</span>,  <span class="hljs-comment"># *** 1. 梯度累加 ***</span><br>                               gradient_checkpointing=<span class="hljs-literal">True</span>,     <span class="hljs-comment"># *** 2. 梯度检查点 ***                             </span><br>                               per_device_eval_batch_size=<span class="hljs-number">1</span>,    <span class="hljs-comment"># 验证时的batch_size</span><br>                               num_train_epochs=<span class="hljs-number">1</span>,              <span class="hljs-comment"># 训练轮数</span><br>                               logging_steps=<span class="hljs-number">10</span>,                <span class="hljs-comment"># log 打印的频率</span><br>                               evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,     <span class="hljs-comment"># 评估策略</span><br>                               save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,           <span class="hljs-comment"># 保存策略</span><br>                               save_total_limit=<span class="hljs-number">3</span>,              <span class="hljs-comment"># 最大保存数</span><br>                               learning_rate=<span class="hljs-number">2e-5</span>,              <span class="hljs-comment"># 学习率</span><br>                               weight_decay=<span class="hljs-number">0.01</span>,               <span class="hljs-comment"># weight_decay</span><br>                               metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,      <span class="hljs-comment"># 设定评估指标</span><br>                               load_best_model_at_end=<span class="hljs-literal">True</span>)     <span class="hljs-comment"># 训练完成后加载最优模型</span><br></code></pre></td></tr></table></figure>我的时间会变长，可能对于CPU来说反向传播时重新计算中间变量耗时更长（猜测）。</p><p><img src="/images/08-Transformers显存优化策略/1722416651614-43346222-b46a-4b8a-9a3e-2effb0d94020.png"></p><h2 id="maxlength128-bs1-ga32-gctrue-adafactor">maxlength=128, BS=1,GA=32, GC=True, Adafactor</h2><p>选择显存占用更少的优化器<code>Adafactor</code>，TrainingArguments默认使用的是AdamW。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># test_3：替换占用显存更小的优化器</span><br>train_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;./checkpoints&quot;</span>,      <span class="hljs-comment"># 输出文件夹</span><br>                               per_device_train_batch_size=<span class="hljs-number">1</span>,   <span class="hljs-comment"># 训练时的batch_size</span><br>                               gradient_accumulation_steps=<span class="hljs-number">32</span>,  <span class="hljs-comment"># *** 1. 梯度累加 ***</span><br>                               gradient_checkpointing=<span class="hljs-literal">True</span>,     <span class="hljs-comment"># *** 2. 梯度检查点 ***</span><br>                               optim=<span class="hljs-string">&quot;adafactor&quot;</span>,               <span class="hljs-comment"># *** 3. adafactor优化器 *** </span><br>                               per_device_eval_batch_size=<span class="hljs-number">1</span>,    <span class="hljs-comment"># 验证时的batch_size</span><br>                               num_train_epochs=<span class="hljs-number">1</span>,              <span class="hljs-comment"># 训练轮数</span><br>                               logging_steps=<span class="hljs-number">10</span>,                <span class="hljs-comment"># log 打印的频率</span><br>                               evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,     <span class="hljs-comment"># 评估策略</span><br>                               save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,           <span class="hljs-comment"># 保存策略</span><br>                               save_total_limit=<span class="hljs-number">3</span>,              <span class="hljs-comment"># 最大保存数</span><br>                               learning_rate=<span class="hljs-number">2e-5</span>,              <span class="hljs-comment"># 学习率</span><br>                               weight_decay=<span class="hljs-number">0.01</span>,               <span class="hljs-comment"># weight_decay</span><br>                               metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,      <span class="hljs-comment"># 设定评估指标</span><br>                               load_best_model_at_end=<span class="hljs-literal">True</span>)     <span class="hljs-comment"># 训练完成后加载最优模型</span><br>train_args<br></code></pre></td></tr></table></figure>在同样重新计算中间变量的情况下时间还是减少了一些吧（或许）。</p><p><img src="/images/08-Transformers显存优化策略/1722416933061-36264587-6596-46ca-a9cf-bb3cef7d94c1.png"></p><h2 id="maxlength128-bs1-ga32-gctrue-adafactor-freeze">maxlength=128,BS=1, GA=32, GC=True, Adafactor, Freeze</h2><p>以文本分类任务为例，模型的结构是Bert+全连接层，那么其实可以把Bert的参数冻结住，只训练全连接层的参数来降低显存。即<strong>冻结特征提取模块参数，只训练任务层参数</strong>，不仅显著降显存还能提搞训练速度（当然模型效果可能也会受影响）。代码实现就是遍历Bert模型里的所有参数，设置<code>requires_grad = False</code>。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># test_4: 参数冻结 </span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.bert.named_parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span><br><br>trainer = Trainer(model=model, <br>                  args=train_args, <br>                  train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>], <br>                  eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>], <br>                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),<br>                  compute_metrics=eval_metric)<br></code></pre></td></tr></table></figure>对于CPU来说显著提升运行速度，只要四十多分钟……对GPU来说，到这一步模型所占用的显存已经降到4G以下了，可以实现4G显存跑Bert-Large了。</p><p><img src="/images/08-Transformers显存优化策略/1722417220295-a5b16f5b-b306-4b67-aa3a-7c04fce94b9d.png"></p><h2 id="maxlength32-bs1-ga32-gctrue-adafactor-freeze">maxlength=32,BS=1, GA=32, GC=True, Adafactor, Freeze</h2><p>最后，还可以在数据预处理部分减小datalength，即减小<code>max_length</code>的值。这个操作对于模型的效果影响很大，因为把输入截得很短的话会很影响模型效果，主要还是按需选择。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># test_5：减小maxlength</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_function</span>(<span class="hljs-params">examples</span>):<br>    tokenized_examples = tokenizer(examples[<span class="hljs-string">&quot;review&quot;</span>], max_length=<span class="hljs-number">32</span>, truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>)<br>    tokenized_examples[<span class="hljs-string">&quot;labels&quot;</span>] = examples[<span class="hljs-string">&quot;label&quot;</span>]<br>    <span class="hljs-keyword">return</span> tokenized_examples<br></code></pre></td></tr></table></figure> 计算速度是显著提高了的。</p><p><img src="/images/08-Transformers显存优化策略/1722418027339-988fc37a-2df7-440d-bf05-fceb75a6e908.png"></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-实战篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Python nlp 简单的数据处理</title>
    <link href="/posts/24988785/"/>
    <url>/posts/24988785/</url>
    
    <content type="html"><![CDATA[<h1 id="简单字符串处理">简单字符串处理</h1><h2 id="去除与替换">去除与替换</h2><h3 id="strip">strip()</h3><p><code>strip()</code>用于去除<strong>首</strong><code>lstrip()</code><strong>尾</strong><code>rstrip()</code>的字符，若括号内为空则默认去除空格。</p><ul><li>若是单个字符，则是从首/尾字母开始匹配要去除的内容，到第一个匹配失败停止。</li></ul><p><img src="/images/数据预处理/1722245686917-c0f88893-5ddb-4dc9-aeef-e846cd197651.png"></p><ul><li>若是多个字符，去除字符时与括号内的字符顺序无关。</li></ul><p><img src="/images/数据预处理/1722245656828-02223365-8f88-45e3-94ad-24d7381d830e.png"></p><h3 id="replace">replace()</h3><p><code>replace()</code>需要两个参数，一个是要被替换的字符，另一个是用于替换的字符。（即替换前和替换后）</p><p><img src="/images/数据预处理/1722320854940-1d1ad3d0-f035-4c6d-8973-0c332478a26b.png"></p><h2 id="查找与判断">查找与判断</h2><h3 id="find">find()</h3><p><code>find()</code>会返回要目标字符的索引（从0开始）。</p><p><img src="/images/数据预处理/1722321155648-3977a996-7078-4a57-a57a-77743513f973.png"></p><h3 id="isxxx">isXXX()</h3><p>python官方定义的字母是英文字母+汉字；官方定义的数字是阿拉伯数字+带圈的数字。</p><p><img src="/images/数据预处理/1722321713096-71b971a2-9226-45dc-b6a4-c0ed17870416.png"></p><p><img src="/images/数据预处理/1722321397295-c17269cd-b16b-43ed-b78a-998cfb0ecbee.png"></p><h2 id="分割与合并">分割与合并</h2><h3 id="split">split()</h3><p><code>split()</code>根据传入的分割字符进行分割，返回一个list。</p><p><img src="/images/数据预处理/1722322405475-d759914c-e93b-42c5-b119-06783571a8a7.png"></p><h3 id="join">join()</h3><p><code>join()</code>以指定的字符进行字符串合并，在分割符相同的情况下和<code>split()</code>为互逆操作。</p><p><img src="/images/数据预处理/1722322430915-4508ef6f-25df-4f24-867b-474b7ca227c6.png"></p><h1 id="正则表达式">正则表达式</h1><h2 id="匹配方法">匹配方法</h2><ul><li><code>match()</code>：从开始位置进行匹配，开头没有返回NoneType不会继续往后搜索，返回MatchObject；</li><li><code>search()</code>：开头不匹配则会跳过开头继续查找，返回MatchObject；</li><li><code>findall()</code>：搜索整个字符串，返回一个list。</li><li><h2 id="字符匹配">字符匹配</h2>正则表达式是一个字符一个字符匹配的，用pattern存储“规则”（或者说是模板）。</li></ul><table><thead><tr><th></th><th><strong>说明</strong></th><th><strong>示例</strong></th><th><strong>匹配结果</strong></th></tr></thead><tbody><tr><td><code>一般字符</code></td><td>匹配字符本身</td><td>abc</td><td>abc</td></tr><tr><td><code>.</code></td><td>匹配除了换行符外的任意一个字符</td><td>a.c</td><td>abc, adc, a/c</td></tr><tr><td><code>\\</code></td><td>匹配有特殊作用的字符，如 . 或 *</td><td>a\.c</td><td>a.c</td></tr><tr><td><code>[...]</code></td><td>匹配中括号内的<strong>任意一个</strong>字符</td><td>a[bcd]e</td><td>abe, ace, ade</td></tr></tbody></table><h3 id="section">.</h3><p><img src="/images/数据预处理/1722324599675-615a1b7c-dfe6-4566-b003-e88bc3a41743.png"></p><h3 id="section-1">\</h3><p><img src="/images/数据预处理/1722324611177-ce467b57-58fa-4cdb-9857-610367ab3761.png"></p><h3 id="section-2">[...]</h3><ul><li><code>[abc]</code>：匹配字符集里的字符；</li><li><code>[a-zA-Z]</code>：匹配所有英文字母不区分大小写；</li><li><code>[^a-zA-Z]</code>：<code>^</code>表示对当前正则表达式取否，即匹配不是英文字母的字符；</li><li><code>|</code>：可以连接两个规则，如<code>[a-zA-Z]|[0-9]</code>=<code>[a-zA-Z0-9]</code>。</li></ul><p><img src="/images/数据预处理/1722324636773-43c14451-092e-4830-bfb6-865d136d5cd3.png"></p><p><img src="/images/数据预处理/1722325331399-b0e0da30-f03c-469f-a810-3a1846ec7a85.png"></p><h2 id="预定义字符集">预定义字符集</h2><p><code>\大写字母</code>是<code>\小写字母</code>的否定。</p><ul><li><code>\d, \D</code>：数字与非数字；</li><li><code>\s, \S</code>：空白字符（间隔符）<code>[&lt;空格&gt;\t\r\n\f\v]</code>与非空白字符；</li><li><code>\w, \W</code>：单词字符<code>[A-Za-z0-9_]</code>与非单词字符；</li><li><code>\b</code>：<code>\w</code>能匹配的到的字符的边界，具体可参考<a href="https://blog.csdn.net/u012961419/article/details/107462741">这篇博客</a>，部分例子如下。</li></ul><p><img src="/images/数据预处理/1722340960904-14412fb5-1038-485e-95a2-1ab605f092a9.png"></p><h2 id="数量词">数量词</h2><table><thead><tr><th></th><th><strong>说明</strong></th><th><strong>示例</strong></th><th><strong>匹配结果</strong></th></tr></thead><tbody><tr><td><code>*</code></td><td>匹配前一个字符[0, ∞)次</td><td>abc*</td><td>ab, abccc</td></tr><tr><td><code>+</code></td><td>匹配前一个字符[1, ∞)次</td><td>abc+</td><td>abc, abccc</td></tr><tr><td><code>?</code></td><td>匹配前一个字符[0, 1)次</td><td>abc?</td><td>ab, abc</td></tr><tr><td><code>&#123;m&#125;</code></td><td>匹配前一个字符m次</td><td>abc{3}</td><td>abccc</td></tr><tr><td><code>&#123;m,n&#125;</code></td><td>匹配前一个字符[m, n]次</td><td>ab{1,2}c</td><td>anc, abbc</td></tr></tbody></table><h2 id="字符串的替换和修改">字符串的替换和修改</h2><ul><li><code>sub(rule, replace, target[,count])</code>：第一个参数是正则表达式，第二个参数指定用于替换的字符串，第三个参数是要被替换处理的字符串，第四个参数是最多替换次数。</li><li><code>subn(rule, replace, target[,count])</code>：参数说明同上，不同的是，<code>sub()</code>返回一个被替换后的字符串，<code>subn()</code>返回的是一个元组，第一个元素是替换后的字符串，第二个元素是产生替换的次数。</li></ul><h2 id="resplit">re+split()</h2><p>使用指定的正则规则在目标字符串中查找匹配的字符串，并以此为界进行分割。</p><h2 id="p...">(?P&lt;...&gt;)</h2><p><code>(?P&lt;name&gt;)</code>允许给不同的规则取名，通过名字来决定使用哪个规则。</p><p><img src="/images/数据预处理/1722327806112-60d99b3f-192f-45e3-8177-bcffcf114339.png">需要注意的是，这里使用search匹配到的第一个结果是“机器学习”而非“自然语言处理”。这是因为使用命名组时，search先匹配的是符合两个规则的子串，然后在这个子串中进行匹配。在这个例子中，第一个被找到的匹配整个正则表达式的子串是“234机器学习”，所以，没有“自然语言处理”。如果要匹配数字之前的文本，要调换两个组的顺序。</p><p><img src="/images/数据预处理/1722328229592-12047ac9-7401-41b5-a8aa-c41c4f3ef83c.png"></p><h1 id="nltk">NLTK</h1><p>适用于英文文本的处理。<code>pip install nltk</code></p><h2 id="分词">分词</h2><p>英文分词的时候token是不区分大小写的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> word_tokenize<br><br>input_str = <span class="hljs-string">&quot;Today&#x27;s weather is good, very windy and sunny, we have no classes in the afternoon,We have to play basketball tomorrow.&quot;</span><br>tokens = word_tokenize(input_str)<br>tokens = [word.lower() <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tokens]<span class="hljs-comment"># 不区分大小写</span><br></code></pre></td></tr></table></figure><h2 id="text对象">Text对象</h2><p><code>Text</code>类里封装了很多方法。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.text <span class="hljs-keyword">import</span> Text<br><br>input_str = <span class="hljs-string">&quot;Today&#x27;s weather is good, very windy and sunny, we have no classes in the afternoon,We have to play basketball tomorrow.&quot;</span><br>tokens = word_tokenize(input_str)<br>t = Text(tokens)<br>t.count(<span class="hljs-string">&#x27;good&#x27;</span>)<span class="hljs-comment"># 统计 good 的出现次数</span><br>t.index(<span class="hljs-string">&#x27;good&#x27;</span>)<span class="hljs-comment"># 获取 good 的索引</span><br>t.plot(<span class="hljs-number">8</span>)<span class="hljs-comment"># 将出现频率前八的单词可视化</span><br></code></pre></td></tr></table></figure></p><p><img src="/images/数据预处理/1722393297455-cbef3436-9178-480f-be3c-1e1feda7551f.png"></p><h2 id="停用词过滤">停用词过滤</h2><p>停用词即在句子中权重低的词（非关键词），如is/the/we/their等，这些出现频率太高、不包含重要语义的词。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords<br><br>input_str = <span class="hljs-string">&quot;Today&#x27;s weather is good, very windy and sunny, we have no classes in the afternoon,We have to play basketball tomorrow.&quot;</span><br>tokens = word_tokenize(input_str)<br>test_words = [word.lower() <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tokens]<br><br><span class="hljs-comment"># 将分词后的结果变为无重复元素的集合</span><br>test_words_set = <span class="hljs-built_in">set</span>(test_words)<br><span class="hljs-comment"># 查看 test_words_set 有的停用词</span><br>test_words_set.intersection(<span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)))<br><span class="hljs-comment"># 过滤停用词</span><br>filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> test_words_set <span class="hljs-keyword">if</span>(w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>))]<br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-comment"># [&#x27;today&#x27;, &#x27;good&#x27;, &#x27;windy&#x27;, &#x27;sunny&#x27;, &#x27;afternoon&#x27;, &#x27;play&#x27;, &#x27;basketball&#x27;, &#x27;tomorrow&#x27;, &#x27;weather&#x27;, &#x27;classes&#x27;, &#x27;,&#x27;, &#x27;.&#x27;, &quot;&#x27;s&quot;]</span><br></code></pre></td></tr></table></figure></p><h2 id="词性标注">词性标注</h2><p>标注后的词性可以参考<a href="https://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/">这里</a>给出的解释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> pos_tag<br><br>tags = pos_tag(tokens)<br></code></pre></td></tr></table></figure><p><img src="/images/数据预处理/1722394241722-21116477-67c5-409e-8e5b-f742e3d90f80.png"></p><table><thead><tr><th><strong>POS Tag</strong></th><th><strong>指代</strong></th></tr></thead><tbody><tr><td>CC</td><td>并列连词</td></tr><tr><td>CD</td><td>基数词</td></tr><tr><td>DT</td><td>限定符</td></tr><tr><td>EX</td><td>存在词</td></tr><tr><td>FW</td><td>外来词</td></tr><tr><td>IN</td><td>介词或从属连词</td></tr><tr><td>JJ</td><td>形容词</td></tr><tr><td>JJR</td><td>比较级的形容词</td></tr><tr><td>JJS</td><td>最高级的形容词</td></tr><tr><td>LS</td><td>列表项标记</td></tr><tr><td>MD</td><td>情态动词</td></tr><tr><td>NN</td><td>名词单数</td></tr><tr><td>NNS</td><td>名词复数</td></tr><tr><td>NNP</td><td>专有名词</td></tr><tr><td>PDT</td><td>前置限定词</td></tr><tr><td>POS</td><td>所有格结尾</td></tr><tr><td>PRP</td><td>人称代词</td></tr><tr><td>PRP$</td><td>所有格代词</td></tr><tr><td>RB</td><td>副词</td></tr><tr><td>RBR</td><td>副词比较级</td></tr><tr><td>RBS</td><td>副词最高级</td></tr><tr><td>RP</td><td>小品词</td></tr><tr><td>UH</td><td>感叹词</td></tr><tr><td>VB</td><td>动词原型</td></tr><tr><td>VBD</td><td>动词过去式</td></tr><tr><td>VBG</td><td>动名词或现在分词</td></tr><tr><td>VBN</td><td>动词过去分词</td></tr><tr><td>VBP</td><td>非第三人称单数的现在时</td></tr><tr><td>VBZ</td><td>第三人称单数的现在时</td></tr><tr><td>WDT</td><td>以wh开头的限定词</td></tr></tbody></table><h2 id="分块">分块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.chunk <span class="hljs-keyword">import</span> RegexpParser<br><br>sentence = [(<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;DT&#x27;</span>),(<span class="hljs-string">&#x27;little&#x27;</span>,<span class="hljs-string">&#x27;JJ&#x27;</span>),(<span class="hljs-string">&#x27;yellow&#x27;</span>,<span class="hljs-string">&#x27;JJ&#x27;</span>),(<span class="hljs-string">&#x27;dog&#x27;</span>,<span class="hljs-string">&#x27;NN&#x27;</span>),(<span class="hljs-string">&#x27;died&#x27;</span>,<span class="hljs-string">&#x27;VBD&#x27;</span>)]<br>grammer = <span class="hljs-string">&quot;MY_NP: &#123;&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;&#125;&quot;</span><br>cp = nltk.RegexpParser(grammer) <span class="hljs-comment">#生成规则</span><br>result = cp.parse(sentence) <span class="hljs-comment">#进行分块</span><br><span class="hljs-built_in">print</span>(result)<br><br>result.draw() <span class="hljs-comment">#调用matplotlib库画出来，树状结构</span><br></code></pre></td></tr></table></figure><h2 id="命名实体识别">命名实体识别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> ne_chunk<br><br>sentence = <span class="hljs-string">&quot;Edison went to Tsinghua University today.&quot;</span><br><span class="hljs-built_in">print</span>(ne_chunk(pos_tag(word_tokenize(sentence))))<br></code></pre></td></tr></table></figure><p><img src="/images/数据预处理/1722394487120-00fc0347-ae29-471a-ae55-51684b23f8da.png"></p><h2 id="数据清洗实例">数据清洗实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords<br><span class="hljs-comment"># 输入数据</span><br>s = <span class="hljs-string">&#x27;    RT @Amila #Test\nTom\&#x27;s newly listed Co  &amp;amp; Mary\&#x27;s unlisted     Group to supply tech for nlTK.\nh $TSLA $AAPL https:// t.co/x34afsfQsh&#x27;</span><br><br><span class="hljs-comment">#指定停用词</span><br>cache_english_stopwords = stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">text_clean</span>(<span class="hljs-params">text</span>):    <br>    <span class="hljs-comment"># 去掉HTML标签(&amp;amp, @Amila等)</span><br>    text_no_special_entities = re.sub(<span class="hljs-string">r&#x27;\&amp;\w*;|#\w*|@\w*&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, text)<br>    <br>    <span class="hljs-comment"># 去掉 $ 符号</span><br>    text_no_tickers = re.sub(<span class="hljs-string">r&#x27;\$\w*&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, text_no_special_entities) <br>    <br>    <span class="hljs-comment"># 去掉超链接</span><br>    text_no_hyperlinks = re.sub(<span class="hljs-string">r&#x27;https?:\/\/.*\/\w*&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, text_no_tickers)<br><br>    <span class="hljs-comment"># 去掉一些专门名词缩写，简单来说就是字母比较少的词</span><br>    text_no_small_words = re.sub(<span class="hljs-string">r&#x27;\b\w&#123;1,2&#125;\b&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, text_no_hyperlinks) <br>    <br>    <span class="hljs-comment"># 多余的空格变为一个空格，去掉句首的空格</span><br>    text_no_whitespace = re.sub(<span class="hljs-string">r&#x27;\s\s+&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, text_no_small_words)<br>    text_no_whitespace = text_no_whitespace.lstrip(<span class="hljs-string">&#x27; &#x27;</span>) <br>    <br>    <span class="hljs-comment"># 分词</span><br>    tokens = word_tokenize(text_no_whitespace)<br>          <br>    <span class="hljs-comment"># 去停用词</span><br>    list_no_stopwords = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> i <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> cache_english_stopwords]<br>    <br>    <span class="hljs-comment"># 过滤后结果</span><br>    text_filtered =<span class="hljs-string">&#x27; &#x27;</span>.join(list_no_stopwords)<br></code></pre></td></tr></table></figure><p><img src="/images/数据预处理/1722394690786-e4dcd0b8-3c1a-4b90-a1ce-9de07349493d.png"></p><h1 id="spacy">Spacy</h1><p><code>pip install spacy</code> &amp;<code>python -m spacy download en</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> spacy<br><br>nlp = spacy.load(<span class="hljs-string">&#x27;en&#x27;</span>)<br>doc = nlp(<span class="hljs-string">&#x27;Weather is good, very windy and sunny. We have no classes in the afternoon.&#x27;</span>)<br><br><span class="hljs-comment"># 分词</span><br><span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc:<br>    <span class="hljs-built_in">print</span> (token)<br>    <br><span class="hljs-comment"># 分句</span><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> doc.sents:<br>    <span class="hljs-built_in">print</span> (sent)<br><br><span class="hljs-comment"># 查看词性</span><br><span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc:<br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;&#123;&#125;-&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(token,token.pos_))<br><br><span class="hljs-comment"># 命名实体识别</span><br>doc_2 = nlp(<span class="hljs-string">&quot;I went to Paris where I met my old friend Jack from uni.&quot;</span>)<br><span class="hljs-keyword">for</span> ent <span class="hljs-keyword">in</span> doc_2.ents:<br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;&#123;&#125;-&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(ent,ent.label_))<br></code></pre></td></tr></table></figure><p>spacy提供实体标注可视化包<code>displacy</code>。</p><p><img src="/images/数据预处理/1722395779082-dde9fc75-a9b3-4116-ac79-86c03545deec.png"></p><h1 id="jieba">jieba</h1><p>jieba分词综合了<strong>基于字符串匹配</strong>的算法和<strong>基于统计</strong>的算法。</p><ul><li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图(DAG)；</li><li>采用了<strong>动态规划查找最大概率路径</strong>，找出基于词频的最大切分组合；</li><li>对于未登录词，采用了基于汉字成词能力的 <strong>HMM模型</strong>，使用了 <strong>Viterbi 算法。</strong></li></ul><h2 id="分词-1">分词</h2><p>jieba分词时可选择4种模式：精确模式、全模式、搜索引擎模式、paddle模式，最常用的还是前三种。</p><p><code>jieba.cut</code> 以及 <code>jieba.cut_for_search</code>返回的结构都是一个可迭代的 generator，可以使用 for循环来获得分词后得到的每一个词语(unicode)，<code>jieba.lcut</code> 以及<code>jieba.lcut_for_search</code> 直接返回 list。</p><ul><li><code>jieba.cut</code> 方法接受四个输入参数：<ol type="1"><li>需要分词的字符串；</li><li><code>cut_all</code>参数用来控制采用全模式/精确模式，默认是精确模式；</li><li><code>HMM</code> 参数用来控制是否使用 HMM 模型；</li><li><code>use_paddle</code>参数用来控制是否使用paddle模式下的分词模式，paddle模式采用延迟加载方式，通过enable_paddle接口安装paddlepaddle-tiny，并且import相关代码；</li></ol></li><li><code>jieba.cut_for_search</code> 方法接受两个参数：<ol type="1"><li>需要分词的字符串；</li><li>是否使用 HMM模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jieba<br><br>seg_list = jieba.cut(<span class="hljs-string">&quot;我来到北京清华大学&quot;</span>, cut_all=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 全模式</span><br>seg_list = jieba.cut(<span class="hljs-string">&quot;我来到北京清华大学&quot;</span>, cut_all=<span class="hljs-literal">False</span>)<span class="hljs-comment"># 精确模式</span><br>seg_list = jieba.cut(<span class="hljs-string">&quot;他来到了网易杭研大厦&quot;</span>)  <span class="hljs-comment"># 默认是精确模式</span><br>seg_list = jieba.cut_for_search(<span class="hljs-string">&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;</span>)  <span class="hljs-comment"># 搜索引擎模式</span><br></code></pre></td></tr></table></figure><p>全模式：我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</p><p>精确模式：我/ 来到/ 北京/ 清华大学</p><p>新词识别：他, 来到, 了, 网易, 杭研, 大厦(此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)</p><p>搜索引擎模式： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院,中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学,深造</p><h2 id="添加自定义词典">添加自定义词典</h2><p>对于一些专有名词，可以用jieba加载自定义词典（词典应保存为utf-8编码）。词典格式是一个词一行，每一行包括词语、词频（可省略）、词性（可省略）三部分，用空格隔开，顺序不可颠倒。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">jieba.load_userdict(<span class="hljs-string">&quot;./data/mydict.txt&quot;</span>) <br></code></pre></td></tr></table></figure><p>也可以直接添加词，但这个词只是临时保存了，再创建新的文本分词时词表里不会有这个词。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">jieba.add_word(word, freq=<span class="hljs-literal">None</span>, tag=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure></p><h2 id="关键词抽取">关键词抽取</h2><p><code>analyse.extract_tags</code>可以抽取文本里的关键词。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jieba.analyse<br><br>text = <span class="hljs-string">&quot;故宫的著名景点包括乾清宫、太和殿和黄琉璃瓦等&quot;</span><br>seg_list = jieba.cut(text, cut_all=<span class="hljs-literal">False</span>)<br>tags = jieba.analyse.extract_tags(text, topK=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure></p><p><img src="/images/数据预处理/1722397142095-bdb43f07-7fc2-4174-83c0-a6be938aee22.png"></p><p><code>withWeight</code>可以查看每个词的权重，<code>topK</code>限定查看的范围。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">tags = jieba.analyse.extract_tags(text, topK=<span class="hljs-number">5</span>, withWeight=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> word, weight <span class="hljs-keyword">in</span> tags:<br>    <span class="hljs-built_in">print</span>(word, weight)<br></code></pre></td></tr></table></figure><p><img src="/images/数据预处理/1722397220471-0f53ce3e-00ae-4de1-a851-17a03f8e0bab.png"></p><h2 id="词性标注-1">词性标注</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jieba.posseg <span class="hljs-keyword">as</span> pseg<br> <br>words = pseg.cut(<span class="hljs-string">&quot;我爱北京天安门&quot;</span>)<br><span class="hljs-keyword">for</span> word, flag <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;%s %s&quot;</span> % (word, flag))<br></code></pre></td></tr></table></figure><hr><blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1MY411e7pX">python-nlp（P2-P13）</a>，<a href="https://blog.csdn.net/u012961419/article/details/107462741">正则表达式-边界</a>，<a href="https://github.com/wdndev/llm_interview_note/blob/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/2.jieba%E5%88%86%E8%AF%8D%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8E%9F%E7%90%86/2.jieba%E5%88%86%E8%AF%8D%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8E%9F%E7%90%86.md">jieba分词用法及原理</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Trainer</title>
    <link href="/posts/5c24a1dc/"/>
    <url>/posts/5c24a1dc/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1KX4y1a7Jk/">【HuggingFaceTransformers-入门篇】基础组件之Trainer</a>，<a href="https://huggingface.co/docs/transformers/main_classes/trainer">Trainer-Huggingface官方说明文档</a></p></blockquote><p>Trainer内部封装了完整的训练以及评估逻辑，搭配TrainingArguments可以对训练过程中的各项参数进行配置。Trainer的参数非常多，<a href="https://huggingface.co/docs/transformers/main_classes/trainer">Trainer-Huggingface官方说明文档</a>提供了详细的参数说明。<img src="/images/07-基础组件之Trainer/1722348733581-7783604e-b768-498c-98b1-2783f4095287.png">Trainer对模型的输入输出是有限制的，要求模型返回一定是元组，或者是ModelOutput的一个子类。输入如果包含labels，Trainer要求模型能返回loss的结果，如果是元组，要求loss应该是元组的第一个值。</p><p><strong>因此，并非所有的自定义模型都能用Trainer。</strong></p><h1 id="创建评估函数">创建评估函数</h1><p>传入的<code>eval_predict</code>涉及到Trainer的源码，以后再做深究。在这里先定义好metric的计算方式。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">acc_metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>f1_metric = evaluate.load(<span class="hljs-string">&quot;f1&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_metric</span>(<span class="hljs-params">eval_predict</span>):<br>    predictions, labels = eval_predict<br>    predictions = predictions.argmax(axis=-<span class="hljs-number">1</span>)<br>    acc = acc_metric.compute(predictions=predictions, references=labels)<br>    f1 = f1_metric.compute(predictions=predictions, references=labels)<br>    acc.update(f1)<br>    <span class="hljs-keyword">return</span> acc<br></code></pre></td></tr></table></figure></p><h1 id="创建trainingarguments">创建TrainingArguments</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">train_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;./checkpoints&quot;</span>,      <span class="hljs-comment"># 输出文件夹</span><br>                               per_device_train_batch_size=<span class="hljs-number">64</span>,  <span class="hljs-comment"># 训练时的batch_size</span><br>                               per_device_eval_batch_size=<span class="hljs-number">128</span>,  <span class="hljs-comment"># 验证时的batch_size</span><br>                               logging_steps=<span class="hljs-number">10</span>,                <span class="hljs-comment"># log 打印的频率</span><br>                               evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,     <span class="hljs-comment"># 评估策略</span><br>                               save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,           <span class="hljs-comment"># 保存策略</span><br>                               save_total_limit=<span class="hljs-number">3</span>,              <span class="hljs-comment"># 最大保存数</span><br>                               learning_rate=<span class="hljs-number">2e-5</span>,              <span class="hljs-comment"># 学习率</span><br>                               weight_decay=<span class="hljs-number">0.01</span>,               <span class="hljs-comment"># weight_decay</span><br>                               metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,      <span class="hljs-comment"># 设定评估指标</span><br>                               load_best_model_at_end=<span class="hljs-literal">True</span>)     <span class="hljs-comment"># 训练完成后加载最优模型</span><br></code></pre></td></tr></table></figure><ul><li><code>output_dir</code>指定每轮训练模型的保存路径。</li><li><code>logstep</code>默认值是500，也可以自行指定。</li><li>如果希望每一轮训练做一次评估，可以通过<code>evaluation_strategy</code>指定评估策略，选择epoch或者是step。如果选择step，要制定<code>eval_steps</code>的值，即多少步做一次评估，其默认值为None。</li><li><code>save_strategy</code>用于指定保存策略，与<code>evaluation_strategy</code>相似。同时也可以通过<code>save_total_limit</code>设置最大保存的数量。</li><li>通过<code>learning_rate</code>自定义学习率。</li><li>通过<code>metric_for_best_model</code>指定模型的好坏最终取决于哪个metric。</li><li><code>load_best_model_at_end</code>若设为True，会在使用模型时加载整个训练过程中表现最好的模型，而非最后一次训练的模型。</li></ul><p>此外，模型训练的日志会保存到tensorboard，在checkpoints的目录下执行命令行<code>tensorboard --logdir runs</code>，可以得到可视化的日志。</p><p><img src="/images/07-基础组件之Trainer/1722352584967-0a04bc6c-d0c2-469c-89d9-5d4ebee62f3c.png"></p><p>Vscode里内置了tensorboard，<code>ctrl+shift+p</code>搜索tensorboard即可在vscode内查看。（我的没加载出来，还把我跑的模型中断了……可能是版本问题，以后有时间再细究)</p><p><img src="/images/07-基础组件之Trainer/1722352667284-56397fda-9074-4319-bb85-1dbd12f0a279.png"></p><h1 id="创建trainer">创建Trainer</h1><p>通过<code>args</code>将TrainingArguments指定的参数传入Trainer，<code>compute_metrics</code>将前面创建好的评估函数传入Trainer。还要通过<code>data_collator</code>指定dataloader，使用DataCollatorWithPadding进行数据堆叠。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer = Trainer(model=model, <br>                  args=train_args, <br>                  train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>], <br>                  eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>], <br>                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),<br>                  compute_metrics=eval_metric)<br></code></pre></td></tr></table></figure>Trainer如果不重新加载，<code>trainer.train()</code>的时候会在之前训练好的基础上继续做训练。</p><h1 id="基于transformers的nlp解决方案">基于Transformers的nlp解决方案</h1><p>up的ppt上已经总结好了，详见<a href="https://github.com/zyds">up的github主页</a>。 <img src="/images/07-基础组件之Trainer/1722418214362-07b8f7e6-d8a7-41f9-96ef-84e6268cb2d4.png"></p><h1 id="文本分类代码优化">【文本分类代码优化】</h1><p>与上一章的代码相比，不需要引入dataloader、optimizer，且Trainer里会自动判定能不能用cuda，所以不需要在代码中做额外的判断。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding<br><span class="hljs-keyword">import</span> evaluate<br><br><span class="hljs-comment"># 1. 读取数据 + 简单清洗数据</span><br>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=<span class="hljs-string">&quot;D:\\transformers_test\\01 Getting Started\\07 trainer\ChnSentiCorp_htl_all.csv&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br>dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;review&quot;</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br><br><span class="hljs-comment"># 2. 划分数据集</span><br>datasets = dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 3. 数据集预处理</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;hfl/rbt3&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_function</span>(<span class="hljs-params">examples</span>):<br>    tokenized_examples = tokenizer(examples[<span class="hljs-string">&quot;review&quot;</span>], max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)<br>    tokenized_examples[<span class="hljs-string">&quot;labels&quot;</span>] = examples[<span class="hljs-string">&quot;label&quot;</span>]<br>    <span class="hljs-keyword">return</span> tokenized_examples<br><br>tokenized_datasets = datasets.<span class="hljs-built_in">map</span>(process_function, batched=<span class="hljs-literal">True</span>, remove_columns=datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names)<br><br><span class="hljs-comment"># 4. 创建模型</span><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;hfl/rbt3&quot;</span>)<br><br><span class="hljs-comment"># 5. 创建评估函数</span><br>acc_metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>f1_metric = evaluate.load(<span class="hljs-string">&quot;f1&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_metric</span>(<span class="hljs-params">eval_predict</span>):<br>    predictions, labels = eval_predict<br>    predictions = predictions.argmax(axis=-<span class="hljs-number">1</span>)<br>    acc = acc_metric.compute(predictions=predictions, references=labels)<br>    f1 = f1_metric.compute(predictions=predictions, references=labels)<br>    acc.update(f1)<br>    <span class="hljs-keyword">return</span> acc<br><br><span class="hljs-comment"># 6. 创建TrainingArguments</span><br>train_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;./checkpoints&quot;</span>,      <span class="hljs-comment"># 输出文件夹</span><br>                               per_device_train_batch_size=<span class="hljs-number">64</span>,  <span class="hljs-comment"># 训练时的batch_size</span><br>                               per_device_eval_batch_size=<span class="hljs-number">128</span>,  <span class="hljs-comment"># 验证时的batch_size</span><br>                               logging_steps=<span class="hljs-number">10</span>,                <span class="hljs-comment"># log 打印的频率</span><br>                               evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,     <span class="hljs-comment"># 评估策略</span><br>                               save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,           <span class="hljs-comment"># 保存策略</span><br>                               save_total_limit=<span class="hljs-number">3</span>,              <span class="hljs-comment"># 最大保存数</span><br>                               learning_rate=<span class="hljs-number">2e-5</span>,              <span class="hljs-comment"># 学习率</span><br>                               weight_decay=<span class="hljs-number">0.01</span>,               <span class="hljs-comment"># weight_decay</span><br>                               metric_for_best_model=<span class="hljs-string">&quot;f1&quot;</span>,      <span class="hljs-comment"># 设定评估指标</span><br>                               load_best_model_at_end=<span class="hljs-literal">True</span>)     <span class="hljs-comment"># 训练完成后加载最优模型</span><br><br><span class="hljs-comment"># 7. 创建Trainer</span><br>trainer = Trainer(model=model, <br>                  args=train_args, <br>                  train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>], <br>                  eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>], <br>                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),<br>                  compute_metrics=eval_metric)<br><br><span class="hljs-comment"># 8. 模型训练</span><br>trainer.train()<br><br><span class="hljs-comment"># 9. 模型评估</span><br>trainer.evaluate(tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>])<br><br><span class="hljs-comment"># 10. 模型对测试集 test 进行预测</span><br>trainer.predict(tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>])<br></code></pre></td></tr></table></figure> <img src="/images/07-基础组件之Trainer/1722390692877-2f22aef0-a9f3-4ed8-8c09-dfd2789a2325.png"></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Evaluate</title>
    <link href="/posts/3a168516/"/>
    <url>/posts/3a168516/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1uk4y1W7tK/">【HuggingFaceTransformers-入门篇】基础组件之Evaluate</a></p></blockquote><p><a href="https://huggingface.co/evaluate/index">Evaluate库</a>包含各种机器学习模型的评估函数，可以很方便的加载各种任务的评估函数。</p><h1 id="查看支持的评估函数">查看支持的评估函数</h1><p><code>list_evaluation_modules</code>会给出Huggingface支持的评估函数，更完整的评估函数可以在<a href="https://huggingface.co/evaluate-metric">官网说明中</a>查看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> evaluate<br><br>evaluate.list_evaluation_modules()<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721745499930-10b7a42b-51a1-4860-9492-f5e04f855726.png"></p><p>对于支持的评估函数有两种实现：社区实现和官方实现。如果不想要社区实现，可以通过设置参数<code>include_community=False</code>来实现。想要知道更多细节的话，可以设置参数<code>with_details=True</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">evaluate.list_evaluation_modules(include_community=<span class="hljs-literal">False</span>, with_details=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721787287182-d2f2c96d-a2e2-46f6-a1f8-6e5c215ede28.png">如果要看每个任务对应的评估指标，可以去<a href="https://huggingface.co/tasks">huggingface的task面板</a>查看。选择相应的任务后，huggingface会提供一个demo，以及适合这个任务的models、datasets和metrics。<img src="/images/06-基础组件之Evaluate/1721787471258-5b2af258-c8b9-420c-9b16-6954fbfae8a8.png"></p><h1 id="加载评估函数">加载评估函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721788898144-f924bd16-488e-49bd-b003-615da5f58127.png"></p><h1 id="查看函数说明">查看函数说明</h1><p>查看函数说明可以去看源码，但还有更简便的方式，就是通过accuracy的属性<code>description</code>进行查看。他会对这个评估函数的计算方式进行简单介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(accuracy.description)<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721745538885-180b08b6-4773-4573-a265-2623ac0c95b5.png"></p><p>如果要更进一步知道这个评估函数的输入如何构造，可以查看<code>input_description</code>，他会更加详细地说明函数的参数说明，并且给出具体的示例。当然在jupyter里直接打印accuracy也是同样的效果。</p><p><img src="/images/06-基础组件之Evaluate/1721743477111-e1d005db-cf79-4dd3-8f66-37eb8af0417a.png"><img src="/images/06-基础组件之Evaluate/1721743487095-5b1ed95d-4f72-47fd-9425-77b8f28bc2c0.png"></p><h1 id="评估指标计算">评估指标计算</h1><h2 id="全局计算">全局计算</h2><p>把reference放一个list，prediction放一个list，然后整体做比对。这个例子的results={'accuracy':0.5}。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>results = accuracy.compute(references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><h2 id="迭代计算">迭代计算</h2><p>对于单条数据，可以使用<code>accuracy.add()</code>，这里面放一对label，即一个refer和对应predict的label。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br><span class="hljs-keyword">for</span> ref, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]):<br>    accuracy.add(references=ref, predictions=pred)<br>accuracy.compute() <span class="hljs-comment"># 0.5</span><br></code></pre></td></tr></table></figure><p>对于batch数据，使用<code>accuracy.add_batch()</code>进行计算，此时里面放的是n组label，一个refer和一个predict为一组（[0,1],[0,1]是一组refer+predict，[1,0],[0,1]是另一组）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br><span class="hljs-keyword">for</span> refs, preds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]]):<br>    accuracy.add_batch(references=refs, predictions=preds)<br>accuracy.compute()<br></code></pre></td></tr></table></figure><h2 id="多个指标联合计算">多个指标联合计算</h2><p>想要同时计算多个评估指标，可以分别计算评估指标，而后更新整合到一个字典。还可以使用<code>evaluate.combine()</code>进行更便捷的联合计算，只需传入要计算的评估函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">clf_metrics = evaluate.combine([<span class="hljs-string">&quot;accuracy&quot;</span>, <span class="hljs-string">&quot;f1&quot;</span>, <span class="hljs-string">&quot;recall&quot;</span>, <span class="hljs-string">&quot;precision&quot;</span>])<br>clf_metrics.compute(predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721744840866-e647ea5f-7652-4971-85ac-fc79ab0f7ac6.png"></p><h1 id="评估结果对比可视化">评估结果对比可视化</h1><p>目前只提供了雷达图的方式对不同模型的结果进行可视化对比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> evaluate.visualization <span class="hljs-keyword">import</span> radar_plot   <span class="hljs-comment"># 目前只支持雷达图</span><br><br>data = [<br>   &#123;<span class="hljs-string">&quot;accuracy&quot;</span>: <span class="hljs-number">0.99</span>, <span class="hljs-string">&quot;precision&quot;</span>: <span class="hljs-number">0.8</span>, <span class="hljs-string">&quot;f1&quot;</span>: <span class="hljs-number">0.95</span>, <span class="hljs-string">&quot;latency_in_seconds&quot;</span>: <span class="hljs-number">33.6</span>&#125;,<br>   &#123;<span class="hljs-string">&quot;accuracy&quot;</span>: <span class="hljs-number">0.98</span>, <span class="hljs-string">&quot;precision&quot;</span>: <span class="hljs-number">0.87</span>, <span class="hljs-string">&quot;f1&quot;</span>: <span class="hljs-number">0.91</span>, <span class="hljs-string">&quot;latency_in_seconds&quot;</span>: <span class="hljs-number">11.2</span>&#125;,<br>   &#123;<span class="hljs-string">&quot;accuracy&quot;</span>: <span class="hljs-number">0.98</span>, <span class="hljs-string">&quot;precision&quot;</span>: <span class="hljs-number">0.78</span>, <span class="hljs-string">&quot;f1&quot;</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">&quot;latency_in_seconds&quot;</span>: <span class="hljs-number">87.6</span>&#125;, <br>   &#123;<span class="hljs-string">&quot;accuracy&quot;</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">&quot;precision&quot;</span>: <span class="hljs-number">0.78</span>, <span class="hljs-string">&quot;f1&quot;</span>: <span class="hljs-number">0.81</span>, <span class="hljs-string">&quot;latency_in_seconds&quot;</span>: <span class="hljs-number">101.6</span>&#125;<br>   ]<br>model_names = [<span class="hljs-string">&quot;Model 1&quot;</span>, <span class="hljs-string">&quot;Model 2&quot;</span>, <span class="hljs-string">&quot;Model 3&quot;</span>, <span class="hljs-string">&quot;Model 4&quot;</span>]<br><br>plot = radar_plot(data=data, model_names=model_names)<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721743795988-4ee4f8e3-5da1-489c-9611-dd86dbdf209c.png"></p><h1 id="文本分类代码优化">【文本分类代码优化】</h1><p>对于Model章给出的文本分类代码的第六步进行优化。 <img src="/images/06-基础组件之Evaluate/1721745380920-0d3a34c7-824a-4df6-944d-921a1ae20e3b.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> evaluate<br><br><span class="hljs-comment"># 6. 训练与验证</span><br>clf_metrics = evaluate.combine([<span class="hljs-string">&quot;accuracy&quot;</span>, <span class="hljs-string">&quot;f1&quot;</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>():<br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> validloader:<br>            <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>                batch = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>            output = model(**batch)<br>            pred = torch.argmax(output.logits, dim=-<span class="hljs-number">1</span>)<br>            clf_metrics.add_batch(predictions=pred.long(), references=batch[<span class="hljs-string">&quot;labels&quot;</span>].long())<br>    <span class="hljs-keyword">return</span> clf_metrics.compute()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch=<span class="hljs-number">3</span>, log_step=<span class="hljs-number">100</span></span>):<br>    global_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        model.train()<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainloader:<br>            <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>                batch = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>            optimizer.zero_grad()<br>            output = model(**batch)<br>            output.loss.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">if</span> global_step % log_step == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ep: <span class="hljs-subst">&#123;ep&#125;</span>, global_step: <span class="hljs-subst">&#123;global_step&#125;</span>, loss: <span class="hljs-subst">&#123;output.loss.item()&#125;</span>&quot;</span>)<br>            global_step += <span class="hljs-number">1</span><br>        clf = evaluate()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ep: <span class="hljs-subst">&#123;ep&#125;</span>, <span class="hljs-subst">&#123;clf&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 7. 模型训练</span><br>train()<br></code></pre></td></tr></table></figure><p><img src="/images/06-基础组件之Evaluate/1721821945271-3df7aebc-58ab-430d-b5d4-6f370975d168.png"></p><p>多分类任务中如果想联合计算metric里的accuracy和f1，需要指定average这个参数，下面的方式是行不通的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">clf_metrics = evaluate.combine([<span class="hljs-string">&quot;accuracy&quot;</span>,<span class="hljs-string">&quot;f1&quot;</span>])<br></code></pre></td></tr></table></figure><p>可以通过这个方式解决：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">acc_metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>f1_metric = evaluate.load(<span class="hljs-string">&quot;f1&quot;</span>)<br></code></pre></td></tr></table></figure><blockquote><p>关于一些metric的计算方式和关系可以参考<a href="https://blog.csdn.net/lhxez6868/article/details/108150777">这篇内容</a>。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Datasets</title>
    <link href="/posts/29f83200/"/>
    <url>/posts/29f83200/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1Ph4y1b76w/">【HuggingFaceTransformers-入门篇】基础组件之Datasets</a></p></blockquote><h1 id="datasets基本使用">Datasets基本使用</h1><p>Huggingface Transformers提供的Datasets库可以很方便的加载本地或者<a href="https://huggingface.co/datasets">Huggingface官网提供的数据集</a>。</p><h2 id="远程加载数据集">远程加载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 1. 在线加载</span><br>datasets = load_dataset(<span class="hljs-string">&quot;madao33/new-title-chinese&quot;</span>)<br><br><span class="hljs-comment"># 2. 加载数据合集中的子任务</span><br>boolq_dataset = load_dataset(<span class="hljs-string">&quot;super_glue&quot;</span>, <span class="hljs-string">&quot;boolq&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 3. 按照数据集划分进行加载</span><br>dataset = load_dataset(<span class="hljs-string">&quot;madao33/new-title-chinese&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br>dataset = load_dataset(<span class="hljs-string">&quot;madao33/new-title-chinese&quot;</span>, split=<span class="hljs-string">&quot;train[10:100]&quot;</span>)<br>dataset = load_dataset(<span class="hljs-string">&quot;madao33/new-title-chinese&quot;</span>, split=<span class="hljs-string">&quot;train[:50%]&quot;</span>)<br>dataset = load_dataset(<span class="hljs-string">&quot;madao33/new-title-chinese&quot;</span>, split=[<span class="hljs-string">&quot;train[:50%]&quot;</span>, <span class="hljs-string">&quot;validation[50%:]&quot;</span>])<br></code></pre></td></tr></table></figure><ol type="1"><li><strong>在线加载：</strong>直接输入想要加载的在线模型在官网中的名字，可以看到是一个DatasetDict类，里面包括训练集和验证集的信息，<code>features</code>可以直观的理解为csv的表头，<code>num_rows</code>是数据数量。</li></ol><p><img src="/images/05-基础组件之Datasets/1721487535496-9bc6fa13-c507-40fb-bf02-fdf5fbdbf2df.png"></p><ol start="2" type="1"><li><strong>加载数据合集中的子任务：</strong>有的数据集（比如glue）包括很多的下游任务数据集，如果我们只想加载其中一个子任务的数据集，需要传入第二个参数，即子任务的名字，这里是"boolq"。之前提到过，<code>trust_remote_code=True</code>意味着信任从远程下载的配置文件。</li></ol><p><img src="/images/05-基础组件之Datasets/1721488338967-b9af2ffc-db10-4a80-ae9d-1f366902bd25.png"></p><ol start="3" type="1"><li><strong>按数据集划分进行加载：</strong><code>split</code>参数允许加载切割后的数据集。比如，可以只加载训练集train，也可以指定要加载的数据比例。<code>"train[10:100]"</code>是加载索引10-100的训练集数据，<code>"train[:50%]"</code>是加载训练集后50%的数据。<code>"train[:50%]", "validation[50%:]"</code>代表着分开加载，先加载训练集后50%的数据，再加载验证集前50%的数据，可以看到此时的返回的是list。</li></ol><figure><img src="/images/05-基础组件之Datasets/image-20240731223909620.png" alt="&quot;train[:50%]&quot;, &quot;validation[50%:]&quot;"><figcaption aria-hidden="true">"train[:50%]","validation[50%:]"</figcaption></figure><h2 id="查看数据集">查看数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 通过索引查看对应数据</span><br>datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]<br>datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">2</span>]<br>datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;title&quot;</span>][:<span class="hljs-number">5</span>]<br></code></pre></td></tr></table></figure><p>取出第一条数据、前两条数据、前五条数据的title。</p><p><img src="/images/05-基础组件之Datasets/1721489553482-f9f92f59-8b7f-430f-a640-72aa63401ce9.png"></p><p>还可以查看数据集的表头、数据属性等。</p><p><img src="/images/05-基础组件之Datasets/1721489826500-4c957538-0aab-4c0b-962a-0afefcd9f90c.png"></p><p><img src="/images/05-基础组件之Datasets/1721489779397-f43f745f-ed3a-49f6-b5ad-888e3a56b878.png"></p><h2 id="数据集划分">数据集划分</h2><p>可以通过<code>train_test_split()</code>来划分测试集，<code>test_size</code>用于指定数据比例，这里指定test的比例为0.1。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = datasets[<span class="hljs-string">&quot;train&quot;</span>]<br>dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure> 可以看到在训练集上以9：1的比例进行了数据划分。</p><p><img src="/images/05-基础组件之Datasets/1721490096576-42a5ec1d-2904-4bd4-9b1f-d998272d365c.png"></p><p>对于监督学习，如果希望划分后数据集label分布均匀，可以通过<code>stratify_by_column</code>指明标签所在的数据字段，保证划分出来的标签是均衡的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = boolq_dataset[<span class="hljs-string">&quot;train&quot;</span>]<br>dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>, stratify_by_column=<span class="hljs-string">&quot;label&quot;</span>)   <br></code></pre></td></tr></table></figure><p><img src="/images/05-基础组件之Datasets/1721490331890-2135b639-92b4-4136-b6d7-53d61cb9507b.png"></p><h2 id="数据的选取与过滤">数据的选取与过滤</h2><p>如果需要加载特定的数据，可以用<code>select()</code>根据数据索引进行选取，或者用<code>filter()</code>进行数据过滤，这就涉及到<code>lambda</code>函数。<code>lambda</code>的语法只包含一个语句，表现形式如下：<code>lambda [arg1 [,arg2,.....argn]]:expression</code>。其中，lambda是 Python 预留的关键字，[arg…] 和 expression 由用户自定义。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 选取</span><br>datasets[<span class="hljs-string">&quot;train&quot;</span>].select([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br><span class="hljs-comment"># 过滤：筛选出title中含有“中国”的数据</span><br>filter_dataset = datasets[<span class="hljs-string">&quot;train&quot;</span>].<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: <span class="hljs-string">&quot;中国&quot;</span> <span class="hljs-keyword">in</span> example[<span class="hljs-string">&quot;title&quot;</span>])<br></code></pre></td></tr></table></figure></p><p><img src="/images/05-基础组件之Datasets/1721561208111-3007aa92-577a-4786-9ff7-45f63d4842c1.png"></p><blockquote><p><code>lambda</code>的常见用法如下：</p><ol type="1"><li>赋值给变量：<code>add = lambda x, y: x+y</code>，这里定义了一个加法函数，输入是x和y，输出是它们的积x+y。此时add指向具有加法功能的函数，add(1,2)=3；</li><li>赋值给函数：<code>time.sleep=lambda x:None</code>，将lambda函数赋值给其他函数，从而用lambda函数替换其他函数。此时执行time.sleep(3)后程序不会休眠3秒钟，因为lambda的输出是None；</li><li>将lambda函数作为参数传递给其他函数，如map、reduce、sorted、filter等。</li></ol></blockquote><h2 id="数据映射">数据映射</h2><p>定义一个函数后，如果希望对每条数据都应用这个函数，要用到<code>map()</code>方法。一个简单的方式是将函数名作为map的参数传入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):<br>    example[<span class="hljs-string">&quot;title&quot;</span>] = <span class="hljs-string">&#x27;Prefix: &#x27;</span> + example[<span class="hljs-string">&quot;title&quot;</span>]<br>    <span class="hljs-keyword">return</span> example<br><br><span class="hljs-comment"># 使用map将函数add_prefix映射到每一条数据上</span><br>prefix_dataset = datasets.<span class="hljs-built_in">map</span>(add_prefix)<br>prefix_dataset[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">10</span>][<span class="hljs-string">&quot;title&quot;</span>]<br></code></pre></td></tr></table></figure><p><img src="/images/05-基础组件之Datasets/1721562726121-55607039-86a1-45d9-8094-e4e0bd140eae.png"></p><p>更为复杂的一个使用方式是结合tokenizer做数据处理。这里定义一个数据处理的函数，对模型的输入（title和content)进行分词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-chinese&quot;</span>)<br><span class="hljs-comment"># 若选择多线程处理，tokenizer要作为一个参数传给preprocess_function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">example, tokenizer=tokenizer</span>):<br>    model_inputs = tokenizer(example[<span class="hljs-string">&quot;content&quot;</span>], max_length=<span class="hljs-number">512</span>, truncation=<span class="hljs-literal">True</span>)<br>    labels = tokenizer(example[<span class="hljs-string">&quot;title&quot;</span>], max_length=<span class="hljs-number">32</span>, truncation=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># label就是title编码的结果</span><br>    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>    <span class="hljs-keyword">return</span> model_inputs<br><br>processed_datasets = datasets.<span class="hljs-built_in">map</span>(preprocess_function)<br></code></pre></td></tr></table></figure><p>将这个函数映射到整个数据集上，得到以下结果。可以看到此时的数据集多了几个字段，其实就是label（titile的编码结果）以及tokenizer返回的input_ids、token_type_ids和attention_mask（详见<a href>tokenizer</a>）。</p><p><img src="/images/05-基础组件之Datasets/1721563180795-32a54957-024c-4904-84ce-dfddea89b7f7.png"></p><p>此外，<code>map()</code>还支持batch和多线程处理数据，使用<code>num_proc</code>可以指定线程的数量。<strong>要注意的是，如果选择多线程处理，那么tokenizer要作为一个参数传给数据处理的函数，因为线程之间资源是不共享的，所以在调用函数时要将tokenizer传给每个线程。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批处理</span><br>processed_datasets = datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 多线程处理</span><br>processed_datasets = datasets.<span class="hljs-built_in">map</span>(preprocess_function, num_proc=<span class="hljs-number">4</span>)<br><br></code></pre></td></tr></table></figure><p>数据处理后，如果不需要原始字段，可以使用<code>map()</code>自带的参数<code>remove_columns</code>移除原始数据集的字段。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">processed_datasets = datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>, remove_columns=datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names)<br></code></pre></td></tr></table></figure></p><p><img src="/images/05-基础组件之Datasets/1721564054425-68c522c4-ede8-4f7f-b446-aef4bd6a127d.png"></p><h2 id="保存与加载">保存与加载</h2><p>处理完数据后，<code>save_to_disk()</code>用于保存处理后的数据到本地，<code>load_from_disk()</code>用于从本地加载数据。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据保存</span><br>processed_datasets.save_to_disk(<span class="hljs-string">&quot;./processed_data&quot;</span>)<br><span class="hljs-comment"># 数据加载</span><br>processed_datasets = load_from_disk(<span class="hljs-string">&quot;./processed_data&quot;</span>)<br></code></pre></td></tr></table></figure></p><h1 id="加载本地数据集">加载本地数据集</h1><h2 id="直接加载文件">直接加载文件</h2><p>与远程加载数据相比，<code>load_dataset()</code>内需要指定加载的数据集类型、路径等。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=<span class="hljs-string">&quot;./ChnSentiCorp_htl_all.csv&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br></code></pre></td></tr></table></figure>除了<code>load_dataset()</code>，Dataset里的<code>from_csv()</code>也可以用来加载csv文件。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = Dataset.from_csv(<span class="hljs-string">&quot;ChnSentiCorp_htl_all.csv&quot;</span>)<br></code></pre></td></tr></table></figure></p><h2 id="加载文件夹内的文件">加载文件夹内的文件</h2><p>使用<code>data_dir="path"</code>指定文件夹路径，即可加载该文件夹内的所有文件作为dataset。如果是加载文件夹内的指定文件，则通过<code>data_files=["a1_path","a2_path"]</code>指明要加载的文件路径。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载./all_data内的所有文件</span><br>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_dir=<span class="hljs-string">&quot;./all_data/&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-comment"># 加载./all_data内的指定文件</span><br>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=[<span class="hljs-string">&quot;./all_data/ChnSentiCorp_htl_all.csv&quot;</span>, <span class="hljs-string">&quot;./all_data/ChnSentiCorp_htl_all copy.csv&quot;</span>], split=<span class="hljs-string">&#x27;train&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="通过预先加载的其他格式转换加载数据集">通过预先加载的其他格式转换加载数据集</h2><p>通过pandas读取数据后，可以使用<code>Dataset.from_pandas()</code>将数据转化为Dataset类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>data = pd.read_csv(<span class="hljs-string">&quot;./ChnSentiCorp_htl_all.csv&quot;</span>)<br>dataset = Dataset.from_pandas(data)<br></code></pre></td></tr></table></figure><p>Dataset还提供了很多其他格式数据的转换方式，如json、dict等。</p><p><img src="/images/05-基础组件之Datasets/1721568962884-ffc45c3f-5bf6-4e25-8c74-7b5a0f1ad710.png"></p><p>需要注意的是，在加载list数据时，需要明确数据字段，否则会报错，因为Dataset中的feature就是数据集中的字段，需要根据字段名去读取该字段下的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># List格式的数据需要内嵌&#123;&#125;，明确数据字段</span><br>data = [&#123;<span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;abc&quot;</span>&#125;, &#123;<span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;def&quot;</span>&#125;]<br><span class="hljs-comment"># 报错写法：data = [&quot;abc&quot;, &quot;def&quot;]</span><br>Dataset.from_list(data)<br></code></pre></td></tr></table></figure><h2 id="配合自定义脚本加载数据集">配合自定义脚本加载数据集</h2><p>有时数据集格式复杂，需要配合自定义脚本进行数据集加载。以问答数据集cmrc2018_trial为例。</p><figure><img src="/images/05-基础组件之Datasets/1721570155024-b9353c8c-aa0c-45b8-9b33-d4e6b88ce45c.png" title="问答数据集cmrc2018_trial" alt="问答数据集cmrc2018_trial"><figcaption aria-hidden="true">问答数据集cmrc2018_trial</figcaption></figure><p>采用直接加载的方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;./cmrc2018_trial.json&quot;</span>, field=<span class="hljs-string">&quot;data&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/05-基础组件之Datasets/1721570451320-fe35332f-12d2-4358-a2ab-313dd10425f6.png"></p><p>可以看到，如果直接加载数据是读取不全的。它只读取到了paragraphs、id和title字段，paragraphs里有多个字段被忽略了，id、context以及qas里的question、answers这种嵌套在里面的数据都没有读到。（这里<code>field</code>用于指明数据存放在data字段里）<img src="/images/05-基础组件之Datasets/1721570199605-f0d4b4e0-82b5-43a4-921b-29c3388c82b1.png"></p><p>为了将数据读全，需要配合自定义脚本进行数据读取。主要实现三个方法：<code>_info</code>用于定义字段的数据类型，<code>_split_generators</code>用于指定数据集路径和数据集划分，<code>_generate_examples</code>用于数据处理。up的脚本可以在<a href="https://github.com/zyds/transformers-code/tree/master/01-Getting%20Started/05-datasets">github</a>上找到。</p><p><img src="/images/05-基础组件之Datasets/1721653420920-3d2bff3a-42f1-49a6-9b26-921a8604cc11.png">配合自定义脚本加载数据，得到想要的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = load_dataset(<span class="hljs-string">&quot;./load_script.py&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/05-基础组件之Datasets/1721653523347-2e31137e-e196-4877-8a0b-0aca2f310dc1.png"></p><h1 id="文本分类代码优化">【文本分类代码优化】</h1><p>上一章的文本分类示例在进行数据加载的时候，写了一个数据聚合函数传给DataLoader，其实Dataset提供了一些简单格式数据处理下预制好的DataCollator。以最简单的DataCollatorWithPadding为例，对上一章给出的文本分类代码进行优化（主要是步骤1~4的优化）。</p><ol type="1"><li>可是使用<code>filter</code>搭配<code>lambda</code>进行数据过滤；</li><li>通过<code>map</code>将数据处理函数映射到每条数据上；</li><li>使用<code>DataCollatorWithPadding</code>作为聚合函数，这里需要注意的是，如果有自定义的数据字段（也就是除了input_ids,token_type_ids, attention_mask,labels以外的字段），是无法使用官方提供的DataCollator进行padding和数据聚合的。</li></ol><blockquote><p>使用huggingface的Dataset加载数据集，用tokenizer对文本数据进行编码后，此时的特征数据还不是tensor，需要转换为深度学习框架所需的tensor类型。<strong>data_collator的作用就是将features特征数据转换为tensor类型的dataset。</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding<br><br><span class="hljs-comment"># 1. 读取数据 + 简单清洗数据(filter + lambda)</span><br>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=<span class="hljs-string">&quot;./ChnSentiCorp_htl_all.csv&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br>dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;review&quot;</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br><br><span class="hljs-comment"># 2. 数据处理</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;hfl/rbt3&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_function</span>(<span class="hljs-params">examples</span>):<br>    tokenized_examples = tokenizer(examples[<span class="hljs-string">&quot;review&quot;</span>], max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)<br>    tokenized_examples[<span class="hljs-string">&quot;labels&quot;</span>] = examples[<span class="hljs-string">&quot;label&quot;</span>]<br>    <span class="hljs-keyword">return</span> tokenized_examples<br><br><span class="hljs-comment"># 3. 划分数据集(map)</span><br>datasets = dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)<br>tokenized_datasets = datasets.<span class="hljs-built_in">map</span>(process_function, batched=<span class="hljs-literal">True</span>, remove_columns=datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names)<br>trainset, validset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>], tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>]<br><br><span class="hljs-comment"># 4. 定义dataloader(DataCollatorWithPadding)</span><br>trainloader = DataLoader(trainset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=DataCollatorWithPadding(tokenizer))<br>validloader = DataLoader(validset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">False</span>, collate_fn=DataCollatorWithPadding(tokenizer))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Model（下）</title>
    <link href="/posts/f568602f/"/>
    <url>/posts/f568602f/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV18T411t7h6/">【HuggingFaceTransformers-入门篇】基础组件之Model（下）</a></p></blockquote><h1 id="文本分类示例">文本分类示例</h1><h2 id="导入相关包">导入相关包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification<br></code></pre></td></tr></table></figure><h2 id="加载数据">加载数据</h2><p>本次用到的数据集是一个用于情感分类的中文数据集，<a href="https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv">下载地址</a>。<a href="https://github.com/SophonPlus/ChineseNlpCorpus">这里</a>还有很多用于其他任务的中文语料库。</p><p><img src="/images/04-基础组件之Model（下）/1720948836399-198a3709-3542-4aa7-b8d2-34727dbf375f.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = pd.read_csv(<span class="hljs-string">&quot;./ChnSentiCorp_htl_all.csv&quot;</span>)<br>data = data.dropna() <span class="hljs-comment"># 去除空数据</span><br></code></pre></td></tr></table></figure><p>数据分为两列，label和review，正面评论的label为1反之为0。</p><p><img src="/images/04-基础组件之Model（下）/1720679079179-87398f7f-8ec2-41cc-beb8-8be3ff924db9.png"><code>dropna()</code>用于数据中的空数据，即丢弃含空值的行或列。</p><p><img src="/images/04-基础组件之Model（下）/1720679235353-b3e9d836-af82-46eb-9504-97c075df2ec1.png"></p><h2 id="创建dataset">创建Dataset</h2><p>然后要创建一个自己的数据类，它继承自Dataset类，包括三个方法。</p><ol type="1"><li><code>__init__</code>：初始化方法，用于读取数据，并进行简单的数据清洗。</li><li><code>__getitem__</code>：可以根据索引index返回数据，这里让它分开返回label和review。</li><li><code>__len__</code>：返回当前数据的大小size。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.data = pd.read_csv(<span class="hljs-string">&quot;./ChnSentiCorp_htl_all.csv&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.data = <span class="hljs-variable language_">self</span>.data.dropna()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.data.iloc[index][<span class="hljs-string">&quot;review&quot;</span>], <span class="hljs-variable language_">self</span>.data.iloc[index][<span class="hljs-string">&quot;label&quot;</span>]<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br></code></pre></td></tr></table></figure><p>然后创建一个<code>MyDataset()</code>对象，现在dataset是一条一条的。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = MyDataset()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(dataset[i])<br></code></pre></td></tr></table></figure></p><p><img src="/images/04-基础组件之Model（下）/1720700116114-5c6376e9-aa1f-4593-9a35-7c11d62d52ab.png"></p><h2 id="划分数据集">划分数据集</h2><p>有了总的数据集之后，要进行训练集、验证集的划分。这里用到<code>random_split()</code>，它需要传入两个参数，一个是dataset，一个是分割的比例（如果用比例写的话和要为1）。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> random_split<br><br>trainset, validset = random_split(dataset, lengths=[<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>])<br></code></pre></td></tr></table></figure></p><p><img src="/images/04-基础组件之Model（下）/1720949453985-dfebfccc-5e33-4fa1-b735-5e3b7efaa94e.png"></p><h2 id="创建dataloader">创建Dataloader</h2><p>划分好数据集之后，需要针对不同的数据集创建不同的<code>DataLoader</code>。因为现在的dataset是一条一条数据的返回，如果希望以batch形式返回，就要用到<code>DataLoader</code>，这里的参数<code>shuffle=True</code>意味着要打乱数据。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>trainloader = DataLoader(trainset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>validloader = DataLoader(validset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></p><p>这个时候validloader里面的数据如下图，可以看到存储的是原始文本数据和label的tensor向量形式，但我们还需要把原始文本数据处理为编码数据，因此要用到参数<code>collate_fn</code>。</p><p><img src="/images/04-基础组件之Model（下）/1720957450499-12110312-74d7-4c3e-88bc-c381455fa925.png"></p><blockquote><p><code>collate_fn</code>是用于整理数据的函数，这个<a href="https://mp.weixin.qq.com/s/Uc2LYM6tIOY8KyxB7aQrOw">可视化视频</a>可以帮助理解。它让dataloader按照batch取数据时，</p><ol type="1"><li>取出大小等同于batch size的index列表；</li><li>将index输入到dataset的<code>getitem()</code>中取出index对应的数据；</li><li>对每个index对应的数据进行堆叠，形成一个batch的数据。</li></ol></blockquote><p>在这里，指定<code>collate_fn=collate_func</code>来整理数据，我们需要编写<code>collate_func</code>这个函数。首先我们要将文本内容和label都拿到，所以创建两个list，通过遍历来分别存储text和label。然后用tokenizer进行分词，设置填充和截断。</p><p>input是个字典，且上一篇有提到如果传入了label模型会自动计算loss，所以最后新增一个label的键来存储label的tensor向量，不需要额外写计算loss的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_func</span>(<span class="hljs-params">batch</span>):<br>    texts, labels = [], []<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch:<br>        texts.append(item[<span class="hljs-number">0</span>])<br>        labels.append(item[<span class="hljs-number">1</span>])<br>    inputs = tokenizer(texts, max_length=<span class="hljs-number">128</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor(labels)<br>    <span class="hljs-keyword">return</span> inputs<br><br>trainloader = DataLoader(trainset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=collate_func)<br>validloader = DataLoader(validset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">False</span>, collate_fn=collate_func)<br></code></pre></td></tr></table></figure><p>这里的input返回的是list，所以要设置参数<code>return_tensors="pt"</code>让他返回tensor。可以看到数据以batchsize的大小进行堆叠，便于后续计算。</p><p><img src="/images/04-基础组件之Model（下）/1720950285943-72e6ee62-820e-492f-8daf-cac5874e1796.png"></p><h2 id="创建模型及优化器">创建模型及优化器</h2><p>优化器可以从torch里面直接导，使用Adam优化器，要传入两个参数：模型要优化的参数<code>model.parameters()</code>和学习率<code>lr=2e-5</code>，因为是做迁移学习所以不需要很高的学习率。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam<br><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br><br><span class="hljs-comment"># 如果有gpu，则将模型放到gpu上</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    model = model.cuda()<br>optimizer = Adam(model.parameters(), lr=<span class="hljs-number">2e-5</span>)<br></code></pre></td></tr></table></figure></p><h2 id="训练与验证">训练与验证</h2><p>在pytorch中，通过调用<code>model.train()</code>和<code>model.eval()</code>来实现训练、评估两种模式的切换。这两种模式下一些层的运行方式会有不同，具体可以看<a href="https://blog.csdn.net/qq_41813454/article/details/135128770">这篇博客</a>。</p><p><img src="/images/04-基础组件之Model（下）/1720960637888-b4638c4d-ec5a-4ebf-8130-5775dac873e1.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 验证部分</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>():<br>    model.<span class="hljs-built_in">eval</span>()<span class="hljs-comment"># 将模型设置为评估模式</span><br>    acc_num = <span class="hljs-number">0</span><span class="hljs-comment"># 统计预测正确的个数</span><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> validloader:<br>            <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>                batch = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>            output = model(**batch)<br>            pred = torch.argmax(output.logits, dim=-<span class="hljs-number">1</span>)<br>            acc_num += (pred.long() == batch[<span class="hljs-string">&quot;labels&quot;</span>].long()).<span class="hljs-built_in">float</span>().<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> acc_num / <span class="hljs-built_in">len</span>(validset)<br><br><span class="hljs-comment"># 训练三轮，每一百步打印一次</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch=<span class="hljs-number">3</span>, log_step=<span class="hljs-number">100</span></span>):<br>    global_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        model.train()<span class="hljs-comment"># 将模型设置为训练模式</span><br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainloader:<br>            <span class="hljs-comment"># 如果有gpu，就把要计算的数据都放到gpu上</span><br>            <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>                batch = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>            optimizer.zero_grad()<span class="hljs-comment"># 优化器梯度归零</span><br>            output = model(**batch)<br>            output.loss.backward()<br>            optimizer.step()<span class="hljs-comment"># 更新模型</span><br>            <span class="hljs-keyword">if</span> global_step % log_step == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ep: <span class="hljs-subst">&#123;ep&#125;</span>, global_step: <span class="hljs-subst">&#123;global_step&#125;</span>, loss: <span class="hljs-subst">&#123;output.loss.item()&#125;</span>&quot;</span>)<br>            global_step += <span class="hljs-number">1</span><br>        acc = evaluate()<span class="hljs-comment"># 每轮训练结束后评估一次模型</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ep: <span class="hljs-subst">&#123;ep&#125;</span>, acc: <span class="hljs-subst">&#123;acc&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="模型训练">模型训练</h2><p>直接调用train()函数。有gpu还是好，我是纯cpu挂着跑，四十多分钟才跑出来真是泪目了……</p><p><img src="/images/04-基础组件之Model（下）/1720950170067-90ac958d-65a4-467d-8dc1-d74ae508e5a1.png"></p><h2 id="模型预测">模型预测</h2><p>做一个id到label的映射，切换到模型评估模式进行预测。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">sen = <span class="hljs-string">&quot;我觉得这家酒店不错，饭很好吃！&quot;</span><br>id2_label = &#123;<span class="hljs-number">0</span>: <span class="hljs-string">&quot;差评！&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;好评！&quot;</span>&#125;<br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.inference_mode():<br>    inputs = tokenizer(sen, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>    <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>        inputs = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()&#125;<br>    logits = model(**inputs).logits<br>    pred = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;输入：<span class="hljs-subst">&#123;sen&#125;</span>\n模型预测结果:<span class="hljs-subst">&#123;id2_label.get(pred.item())&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></p><p><img src="/images/04-基础组件之Model（下）/1720950184460-067f5040-8189-4410-ac12-46defc6b8a9e.png">除此之外，也可以用pipeline来实现预测。 <img src="/images/04-基础组件之Model（下）/1720964989317-e585a4be-7534-49bc-9fe7-b86c88a4aa92.png"></p><h1 id="代码汇总">【代码汇总】</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> random_split<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam<br><br>csv_path = <span class="hljs-string">&quot;D:\\transformers_test\\01 Getting Started\\04 model\\ChnSentiCorp_htl_all.csv&quot;</span><br><br><span class="hljs-comment"># 0.定义MyDataset类，继承自Dataset类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.data = pd.read_csv(csv_path)<br>        <span class="hljs-variable language_">self</span>.data = <span class="hljs-variable language_">self</span>.data.dropna()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.data.iloc[index][<span class="hljs-string">&quot;review&quot;</span>], <span class="hljs-variable language_">self</span>.data.iloc[index][<span class="hljs-string">&quot;label&quot;</span>]<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br>    <br><span class="hljs-comment"># 1. 读取数据 + 简单清洗数据</span><br>dataset = MyDataset()<br><br><span class="hljs-comment"># 2. 划分数据集</span><br>trainset, validset = random_split(dataset, lengths=[<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>])<br><br><span class="hljs-comment"># 3. 分词</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br><br><span class="hljs-comment"># 4. 定义dataloader</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_func</span>(<span class="hljs-params">batch</span>):<br>    texts, labels = [], []<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch:<br>        texts.append(item[<span class="hljs-number">0</span>])<br>        labels.append(item[<span class="hljs-number">1</span>])<br>    inputs = tokenizer(texts, max_length=<span class="hljs-number">128</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor(labels)<br>    <span class="hljs-keyword">return</span> inputs<br><br>trainloader = DataLoader(trainset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=collate_func)<br>validloader = DataLoader(validset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">False</span>, collate_fn=collate_func)<br><br><span class="hljs-comment"># 5. 创建模型和优化器</span><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    model = model.cuda()<br><br>optimizer = Adam(model.parameters(), lr=<span class="hljs-number">2e-5</span>)<br><br><span class="hljs-comment"># 6. 训练与验证</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>():<br>    model.<span class="hljs-built_in">eval</span>()<br>    acc_num = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> validloader:<br>            <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>                batch = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>            output = model(**batch)<br>            pred = torch.argmax(output.logits, dim=-<span class="hljs-number">1</span>)<br>            acc_num += (pred.long() == batch[<span class="hljs-string">&quot;labels&quot;</span>].long()).<span class="hljs-built_in">float</span>().<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> acc_num / <span class="hljs-built_in">len</span>(validset)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch=<span class="hljs-number">3</span>, log_step=<span class="hljs-number">100</span></span>):<br>    global_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        model.train()<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainloader:<br>            <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>                batch = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>            optimizer.zero_grad()<br>            output = model(**batch)<br>            output.loss.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">if</span> global_step % log_step == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ep: <span class="hljs-subst">&#123;ep&#125;</span>, global_step: <span class="hljs-subst">&#123;global_step&#125;</span>, loss: <span class="hljs-subst">&#123;output.loss.item()&#125;</span>&quot;</span>)<br>            global_step += <span class="hljs-number">1</span><br>        acc = evaluate()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ep: <span class="hljs-subst">&#123;ep&#125;</span>, acc: <span class="hljs-subst">&#123;acc&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 7. 模型训练</span><br>train()<br><br><span class="hljs-comment"># 8. 模型预测</span><br>sen = <span class="hljs-string">&quot;我觉得这家酒店不错，饭很好吃！&quot;</span><br>id2_label = &#123;<span class="hljs-number">0</span>: <span class="hljs-string">&quot;差评！&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;好评！&quot;</span>&#125;<br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.inference_mode():<br>    inputs = tokenizer(sen, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>    <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>        inputs = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()&#125;<br>    logits = model(**inputs).logits<br>    pred = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;输入：<span class="hljs-subst">&#123;sen&#125;</span>\n模型预测结果:<span class="hljs-subst">&#123;id2_label.get(pred.item())&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/04-基础组件之Model（下）/1721040663050-3d8ac50f-b5d5-4e38-951b-77996e1c62af.png"></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Model（上）</title>
    <link href="/posts/4dd4074a/"/>
    <url>/posts/4dd4074a/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1KM4y1q7Js/">【HuggingFaceTransformers-入门篇】基础组件之Model（上）</a>，<a href="https://huggingface.co/learn/nlp-course">Huggingface NLPCourse</a></p></blockquote><h1 id="背景知识">背景知识</h1><h2 id="encoder-decoder">Encoder &amp; Decoder</h2><p>补充一下Transformer的背景知识，简单来说，Transformer分为Encoder（编码器）和Decoder（解码器）两个部分，其中基于掩码的注意力机制是模型的重点，是面向seq2seq任务的，一些细节可以参考之前写的一个<a href="https://jiangcara.github.io/posts/bda47da5/">Transformer学习笔记</a>。</p><p><img src="/images/04-基础组件之Model（上）/1720798084999-b94c1f2b-7437-49fa-a310-71706c4b4d9f.png">现在大部分的预训练语言模型（PLMs）都是基于Transformerblock的堆叠，堆叠的方式有三种：</p><ol type="1"><li>基于Encoder，适合文本分类、命名实体识别、抽取式问答任务等；</li><li>基于Decoder，适合文本生成任务；</li><li>结合Encoder和Decoder，适合总结摘要、翻译、生成式问答任务等。</li></ol><p>官网给出了这三种堆叠方式下的经典模型，如下图。 <img src="/images/04-基础组件之Model（上）/1720796097629-3d5e1b48-5eab-4236-9a5a-313975b30286.png">up在视频中也给出了这三类模型的简要介绍。 <img src="/images/04-基础组件之Model（上）/1720798189571-2c0d6a66-266d-4a2f-9708-5b9e64170a37.png"></p><h2 id="model-head">Model Head</h2><p>对于modelhead，官网给出的解释是将高维向量映射映射到不同的维度上。我觉得可以理解为模型在output前经过的最后一层，通常为1或多个全连接层，作用是将模型的编码结果根据不同类型的任务映射成不同类型的内容。<img src="/images/04-基础组件之Model（上）/1720798745650-0160aab1-9de4-4411-afb6-747525fedf81.png">调用模型本身只会返回一个基于这个模型的编码结果（hiddenstates)，所以需要model head来当任务头。不同的modelhead对应不同的任务，有很多种选择：</p><ul><li>*Model（检索隐藏状态）</li><li>*ForCausalLM</li><li>*ForMaskedLM</li><li>*ForMultipleChoice</li><li>*ForQuestionAnswering</li><li>*ForSequenceClassification</li><li>*ForTokenClassification</li></ul><h1 id="基本使用">基本使用</h1><p>这次使用的模型是哈工大推出的三层Roberta模型，在<a href="https://huggingface.co/hfl/rbt3">官网</a>可以找到，大小在一百多兆。（好在不大不然我这破机子真的跑不起来...）</p><h2 id="模型的加载与保存">模型的加载与保存</h2><p>先导包。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel, AutoTokenizer<br></code></pre></td></tr></table></figure> ### 在线加载这是从官网直接下载下来，只有初次调用会下载，后续调用的时候会使用缓存。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;hfl/rbt3&quot;</span>, force_download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></p><p><img src="/images/04-基础组件之Model（上）/1720876849958-13271808-b265-45b6-bb50-5120dbd67bf4.png"></p><p>还可以使用gitclone的方式进行下载，模型会保存到当前目录的rbt3文件夹下。由于我没配置git，就没有尝试。总之第一种git方式是下了所有文件，第二种git方式是下载指定的pytorch版本的bin文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">!git clone <span class="hljs-string">&quot;https://huggingface.co/hfl/rbt3&quot;</span><br>!git lfs clone <span class="hljs-string">&quot;https://huggingface.co/hfl/rbt3&quot;</span> --include=<span class="hljs-string">&quot;*.bin&quot;</span><br></code></pre></td></tr></table></figure><h3 id="离线加载">离线加载</h3><p>如果模型比较大，网不好的话下载不了，可以采取离线下载的方式。也就是进到<a href="https://huggingface.co/hfl/rbt3">模型详情页</a>把文件都下载下来。</p><p><img src="/images/04-基础组件之Model（上）/1720877086943-5cbd5b73-b935-461f-ad99-373dcb0bf241.png"></p><p>可以看到有三个较大的文件，这三个文件时这个模型用不同框架实现的三个版本，即flax版、tensorflow版和pytorch版。我们用到的是pytorch版，因此其他两个版本模型不用下载，只要其他文件和pytorch版模型就行。下载后存储到指定路径中，即可进行加载。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br></code></pre></td></tr></table></figure><p>我这里就是把模型保存在同一个目录下的文件夹内。 <img src="/images/04-基础组件之Model（上）/1720877436916-a5740903-98c4-4447-bfa3-bf37a6fee9c7.png"></p><h2 id="模型加载参数">模型加载参数</h2><p>加载模型后，可以看到模型本身的一些参数，可以对模型进行配置。 <img src="/images/04-基础组件之Model（上）/1720701569721-aa2d3703-86a9-40f6-b1d6-bc218599ca34.png">除此之外，还可以在训练过程中修改模型的一些参数，这里要调用<code>AutoConfig</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./hflrbt3/&quot;</span>)<br></code></pre></td></tr></table></figure><p>打印出来的结果和之前一样，但是输入config.的时候会出来很多选项，都是可以设置的参数。<img src="/images/04-基础组件之Model（上）/1720878008296-b53ef402-b2eb-4227-af52-62f00c87ed32.png">这个模型是一个bert模型，所以可以进入BertConfig中去看更加具体的参数。而BertConfig又继承自PretrainConfig类，里面的参数同样可以进行配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig<br></code></pre></td></tr></table></figure><p><img src="/images/04-基础组件之Model（上）/1720878190929-df04ac35-78e2-49c2-bf3e-de8b687959d7.png">以设置参数<code>output_attentions</code>为例，如果调用模型时没有将它设置为True，可以看到模型会输出隐藏状态、池化结果等，但最后的attention都是none。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br>output = model(**inputs)<br></code></pre></td></tr></table></figure><p><img src="/images/04-基础组件之Model（上）/1720878864087-46b1c3f8-f200-49ad-b394-1df9530b50d2.png">如果设置参数<code>output_attentions=True</code>，则会输出attention。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/04-基础组件之Model（上）/1720879143144-e9da4a3f-f2cf-4bf0-9ba9-358c702c8cd3.png"></p><h1 id="模型调用">模型调用</h1><p>这里的<code>return_tensors="pt"</code>是指定tokenizer返回pytorchtensor形式的数据。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">sen = <span class="hljs-string">&quot;弱小的我也有大梦想！&quot;</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>)<br>inputs = tokenizer(sen, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br></code></pre></td></tr></table></figure></p><p><img src="/images/04-基础组件之Model（上）/1720878527358-56217ea6-b04d-410a-8da4-74d3b7c3b7d7.png"></p><h2 id="无model-head的模型调用">无Model Head的模型调用</h2><p><code>AutoModel</code>不带modelhead编码，模型实际上是对输入的序列进行编码，输出的编码结果就是last_hidden_state。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)<br>output = model(**inputs)<br></code></pre></td></tr></table></figure><p>可以看到last_hidden_state的维度是1<em>12</em>768，其中1是batch的维度（只有一个句子），12是这个句子中token的个数，也就是<code>"input_ids"</code>的长度。</p><p><img src="/images/04-基础组件之Model（上）/1720879984749-84004725-797b-425e-ad42-e8fe270b831b.png"></p><h2 id="有model-head的模型调用">有Model Head的模型调用</h2><p><code>AutoModelForSequenceClassification</code>就是一个带modelhead编码的模型，它会将last_hidden_state从高维度向量映射成指定的低维向量，如这里指定<code>num_label=10</code>，即十分类任务，就为映射成十维向量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, BertForSequenceClassification<br><br>clz_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;hflrbt3&quot;</span>, num_labels=<span class="hljs-number">10</span>)<br>output = clz_model(**inputs)<br></code></pre></td></tr></table></figure></p><p><img src="/images/04-基础组件之Model（上）/1720880637842-471dae9c-61ab-420d-b42f-52a84dd70912.png"></p><p>至于能够通过修改<code>num_label</code>就能修改映射维度，是因为这个模型是继承自Bert文本分类模型的，里面定义了将<code>hidden_size</code>映射成<code>num_label</code>的维度，所以可以这么做。</p><p><img src="/images/04-基础组件之Model（上）/1720880758137-4d60b034-c4d6-4abc-be11-a7af737b0fa2.png"></p><p><img src="/images/04-基础组件之Model（上）/1720880791109-4cbd4c21-8a81-4cb1-9c25-578b9488615d.png">深入看下这个bert文本分类模型的细节，可以发现他就是三层连在一起，即bert层、dropout层和一个全连接层。</p><p><img src="/images/04-基础组件之Model（上）/1720880945445-1ff60e4f-f3ba-4ebf-895f-269250edb538.png"></p><p>模型的input先经过forward函数，得到output，再将output中的池化结果取出来，也就是[CLS]的向量结果。这是因为在分类任务中，[CLS]是整个序列的类别标签，用于表示该序列所属类。在训练过程中，[CLS]充当预测目标，Bert通过预测[CLS]的类别来学习整个序列的语义信息，并将该类别作为整个序列的预测结果。</p><p><img src="/images/04-基础组件之Model（上）/1720881417531-26573853-b970-4f1d-9e73-f7802a74f32f.png"></p><p>而后模型会对我们有没有输入label进行判断，进而判断这是回归任务、二分类任务还是多分类任务，并且根据不同的任务类型去计算loss。最后还指定了返回值的格式，如果没有指定以字典形式返回，则以默认方式返回。</p><p><img src="/images/04-基础组件之Model（上）/1720881978945-0c8c29aa-8ba8-4672-8576-59157f7a71e9.png"></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Tokenizer</title>
    <link href="/posts/1bcd8d8b/"/>
    <url>/posts/1bcd8d8b/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1NX4y1177c/">【HuggingFaceTransformers-入门篇】基础组件之Tokenizer</a>，<a href="https://huggingface.co/learn/nlp-course">Huggingface NLPCourse</a></p></blockquote><p><strong>Transformer模型只接受张量作为输入。</strong>与其他神经网络一样，Transformer模型不能直接处理原始文本，因此的第一步是将文本输入转换为模型可以理解的数字。为此，我们使用一个分词器tokenizer，它将负责：</p><ul><li>将输入拆分为称为标记的单词、子单词或符号（如标点符号）</li><li>将每个标记映射到整数</li><li>添加可能对模型有用的其他输入</li></ul><p><img src="/images/03-基础组件之Tokenizer/1719991156922-b5d170cc-4068-4558-8e71-1b01ebd25b2c.png"></p><h1 id="tokenizer基本使用">Tokenizer基本使用</h1><p>我们可以直接给tokenizer传入句子，随后会得到一个字典，该字典可以提供给我们的模型。剩下唯一要做的就是将输入ID 列表转换为张量。 <img src="/images/03-基础组件之Tokenizer/1719995644821-77a0e9b1-347d-4c96-8b1d-4f5961e7afc2.png"></p><h2 id="加载与保存">加载与保存</h2><p><code>AutoTokenizer</code>会根据传入的参数自动分配默认的tokenizer。加载的时候会用到<code>from_pretrained()</code>方法，即从预训练阶段开始加载。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br>sen = <span class="hljs-string">&quot;弱小的我也有大梦想!&quot;</span><br><span class="hljs-comment"># 从HuggingFace加载，输入模型名称，即可加载对于的分词器</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br></code></pre></td></tr></table></figure> 可以看到这是一个基于Bert的模型。</p><p><img src="/images/03-基础组件之Tokenizer/1720008250054-10de9259-a429-4f2e-83a8-e320d35f5417.png"></p><p>默认的下载路径是<code>C:\Users\Administrator\.cache\huggingface\hub\</code>，也可以指定路径存储。</p><p><img src="/images/03-基础组件之Tokenizer/1719995981031-d91fd65c-0c2f-4481-819b-e73040f36021.png"></p><p>模型文件里面的snapshots文件夹里是模型有用的文件。</p><p><img src="/images/03-基础组件之Tokenizer/1719995996708-1178f0f1-53a3-4b89-bd1e-2aeaf85701ae.png"><img src="/images/03-基础组件之Tokenizer/1720011478338-a0f042a7-cb5d-4a9e-b61d-960091945bec.png"></p><p>使用<code>save_pretrained()</code>方法来将tokenizer保存到指定路径。./roberta_tokenizer表示将路径设置为代码文件所在文件夹的roberta_tokenizer文件夹中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># tokenizer 保存到本地</span><br>tokenizer.save_pretrained(<span class="hljs-string">&quot;./roberta_tokenizer&quot;</span>)<br><span class="hljs-comment"># 从本地加载tokenizer</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./roberta_tokenizer/&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/03-基础组件之Tokenizer/1720008301755-08871446-61f2-4931-8f0d-77cf93044cfd.png"></p><blockquote><p>这里记一下从<a href="https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt">官方文档</a>看到的两个英文词概念：</p><ul><li><strong>Architecture:</strong>模型的骨架，即模型中每个层和每个操作的定义。</li><li><strong>Checkpoints: </strong>在给定架构中加载的权重。</li></ul><p>例如，BERT是一个architecture，而 bert-base-cased(Google为 BERT第一个版本训练的一组权重)则是一个checkpoint。</p></blockquote><h2 id="句子分词">句子分词</h2><p>加载完tokenizer之后需要对句子进行分词，可以使用<code>tokenize()</code>方法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tokens = tokenizer.tokenize(sen)<br></code></pre></td></tr></table></figure> 当前选择的基于BERT的模型分词结果如下。</p><p><img src="/images/03-基础组件之Tokenizer/1720008404601-0cfadb68-5ef8-42ec-9ef3-b952736f6c76.png"></p><h2 id="查看词典">查看词典</h2><p>可以查看这个分词器的词典，井号我觉得可以理解为占位符，意思是这个字通常出现在一个词的第二个位置，也就是说比起这个字单独出现，它和其他字一起出现的概率更高，具体可以看搜搜subword的概念。</p><blockquote><p>BERT的分词方式是WordPiece，词表的构造是基于subword(子词)的，有助于缓解OOV(Outof Vocabulary)问题，即测试集有很多模型训练时没见过的单词。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(tokenizer.vocab)<br><span class="hljs-built_in">print</span>(tokenizer.vocab_size)<br></code></pre></td></tr></table></figure><p><img src="/images/03-基础组件之Tokenizer/1720008435037-8495efde-9d8b-4f85-a6f4-cff641b5e9b2.png"><img src="/images/03-基础组件之Tokenizer/1720009355001-fb0528c2-f001-48c0-972a-24b484e966a9.png"></p><h2 id="索引转换">索引转换</h2><p>使用<code>convert_tokens_to_ids()</code>将分词后的token序列转换成id序列送入神经网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将词序列转换为id序列</span><br>ids = tokenizer.convert_tokens_to_ids(tokens)<br><span class="hljs-comment"># 将id序列转换为token序列</span><br>tokens = tokenizer.convert_ids_to_tokens(ids)<br><span class="hljs-comment"># 将token序列转换为string</span><br>str_sen = tokenizer.convert_tokens_to_string(tokens)<br></code></pre></td></tr></table></figure></p><p><img src="/images/03-基础组件之Tokenizer/1720009414913-7cdc6476-cc0d-4ee3-9016-9f1d9876c9f5.png"></p><p>更简便的方法是使用<code>encode()</code>一步实现句子序列到id序列的转换。这里有两个参数，sen即输入的句子，add_special_tokens用来控制是否给id序列添加特殊符号。</p><blockquote><p>在BERT的分词方法中，会在id序列的前后或者中间加入一些特殊符号，如这里的[CLS]和[SEP]，前者代表一句话的开始，后者标志着一句话的结束。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将字符串转换为id序列，又称之为编码</span><br>ids = tokenizer.encode(sen, add_special_tokens=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 将id序列转换为字符串，又称之为解码</span><br>str_sen = tokenizer.decode(ids, skip_special_tokens=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>可以看到使用<code>encode()</code>方法会在句子前后多了101和102，其实就对应特殊符号[CLS]和[SEP]，因为add_special_tokens=True，不添加的话可以设为False。同理，解码<code>decode()</code>也可以通过这个参数选择要不要将特殊符号及进行解码。<img src="/images/03-基础组件之Tokenizer/1720009456448-b0728275-a923-4e77-9e03-16e0bfcfac6a.png"></p><h2 id="填充与截断">填充与截断</h2><p>让模型处理大量数据时会涉及到数据的填充和截断，以统一每条数据长度。即短数据填充<code>padding</code>，长数据截断<code>truncation</code>。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 填充</span><br>ids = tokenizer.encode(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br><span class="hljs-comment"># 截断</span><br>ids = tokenizer.encode(sen, max_length=<span class="hljs-number">5</span>, truncation=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>以下是效果展示。需要注意的是每个处理后的句子前后都至少会有两个特殊符号，每条数据以5为最大长度截断但实际上只能存放三个词。</p><p><img src="/images/03-基础组件之Tokenizer/1720009463876-cdd31ecc-3bda-47f6-b05f-31f5ac89bd41.png"></p><h2 id="其他输入部分">其他输入部分</h2><p>因为存在填充，所以我们需要告诉模型哪些是填充的哪些是真实有效的输入，这就需要<code>attention_mask</code>。此外，对于BERT模型，它需要<code>token_type_ids</code>这个参数来区分哪些词属于第一句，哪些属于第二句，也就是段编码。</p><blockquote><p>这是因为BERT的input由三部分组成，除了分词后的词向量(tokenembeddings)，还有段编码(segment embeddings)和位置编码(PositionEmbedding)，详见之前的PLMs的笔记。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ids = tokenizer.encode(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br>attention_mask = [<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> idx != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> ids]<br>token_type_ids = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(ids)<br></code></pre></td></tr></table></figure><p><code>attention_mask</code>就是id为0记为0否则记为1。由于这是一句话的句间文本，所以<code>token_type_ids</code>都是0，长度就是整个token的长度。</p><p><img src="/images/03-基础组件之Tokenizer/1720009499879-c9d284ff-77f2-4252-a83b-26fa6b375ec1.png"><a href="https://www.yuque.com/juanjuan-rytsj/whmr52/ehu2oqg8dwco8ozw?view=doc_embed">PLMs(GPT,BERT)</a></p><h2 id="快速调用方式">快速调用方式</h2><p>HuggingfaceTransformers提供了快捷调用方式，可以一步实现上述步骤，也就是<code>encode_plus()</code>方法。调用后返回一个dic，里面包含了id序列和<code>attention_mask</code>。除此之外，直接调用<code>tokenizer</code>（不是<code>tokenize()</code>，注意区分）也可以得到一样的效果。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = tokenizer.encode_plus(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br>inputs = tokenizer(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br></code></pre></td></tr></table></figure></p><p><img src="/images/03-基础组件之Tokenizer/1720009513401-5797874b-98ce-472f-915b-2ff203038f42.png"></p><h2 id="处理batch数据">处理batch数据</h2><p>批量处理数据也可以用同样的方法，返回的值形如注释。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">sens = [<span class="hljs-string">&quot;弱小的我也有大梦想&quot;</span>,<br>        <span class="hljs-string">&quot;有梦想谁都了不起&quot;</span>,<br>        <span class="hljs-string">&quot;追逐梦想的心，比梦想本身，更可贵&quot;</span>]<br>res = tokenizer(sens)<br><br><span class="hljs-string">&#x27;&#x27;&#x27; &#123;&#x27;input_id&#x27;  : [[..1..],[..2..],[..3..]],</span><br><span class="hljs-string">     &#x27;token_type_ids&#x27; : [[..1..],[..2..],[..3..]],</span><br><span class="hljs-string">     &#x27;attention_mask&#x27; : [[..1..],[..2..],[..3..]]&#125;&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure></p><p><img src="/images/03-基础组件之Tokenizer/1720009548233-b0e00fe5-f059-4dea-8955-b7a4c1196add.png">对比for循环与batch处理的时间，可以发现batch明显快于循环计算。</p><p><img src="/images/03-基础组件之Tokenizer/1720009574961-edf651fd-2a9c-47ab-8643-b4184be2ef67.png"></p><h1 id="fastslow-tokenizer">Fast/Slow Tokenizer</h1><ul><li>FastTokenizer：基于Rust实现，速度块，有两个额外返回值<code>offsets_mapping</code>、<code>word_ids</code>。</li><li>SlowTokenizer：基于Python实现，速度较慢。</li></ul><p>使用<code>from_pretrained()</code>方法分配tokenizer时默认分配FastTokenizer，如果要设置成SlowTokenizer需括号内增加参数<code>use_fast=False</code>。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">sen = <span class="hljs-string">&quot;弱小的我也有大Dreaming!&quot;</span><br>fast_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>slow_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>, use_fast=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></p><p><img src="/images/03-基础组件之Tokenizer/1720009839915-f9e8872e-e430-4371-9e89-558d639fe3ba.png"></p><p>FastTokenizer有个额外返回值<code>offsets_mapping</code>，使用时将需要设置return_offsets_mapping=True。随之还会返回<code>word_ids</code>，这两个值相互对应。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = fast_tokenizer(sen, return_offsets_mapping=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(inputs.word_ids)<br></code></pre></td></tr></table></figure><ul><li>sen = "弱小的我也有大Dreaming!"。</li><li>'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5),(5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]</li><li>word_ids: [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]</li></ul><p>word_ids会记录每个词在句子中的位置，None分别对应句首句尾的特殊字符[CLS]和[SEP]，也就是offset_mapping里的(0,0)。(0,1)对应第一个字"弱"的位置，(6,7)对应第7个字"大"的位置。</p><p>因为截断，所以Dreaming被分为两个数据段，所以word_ids里有两个7，对应offset_mapping的(7,12)和(12,15)，表示这两个部分合起来是一个完整的token。</p><p><img src="/images/03-基础组件之Tokenizer/1720009877972-0e6e8450-a3c0-4ccc-aed4-fbfdfccf5e86.png">这里我还试了其他的例子： <img src="/images/03-基础组件之Tokenizer/1720198475404-4124eb2a-31f3-436e-be7c-f067613dc0c9.png">这个更多的会在QA中用到，因为回答问题时需要记录答案在原始文本中开始和结束的位置。</p><h1 id="特定tokenizer的加载">特定Tokenizer的加载</h1><p>还可以加载特定模型的分词器（只要<a href>HuggingfaceModel</a>里有），比如这里想用<a href="https://huggingface.co/Skywork/Skywork-13B-base/tree/main">天工</a>的分词器。<img src="/images/03-基础组件之Tokenizer/1720010071718-12430c74-27d0-495e-96b0-798862a56d77.png">找到模型名使用from_pretrained进行加载，加载时必须添加参数<code>trust_remote_code=True</code>，不然会报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br><span class="hljs-comment"># 新版本的transformers（&gt;4.34），加载 THUDM/chatglm 会报错，因此这里替换为了天工的模型</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Skywork/Skywork-13B-base&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br>tokenizer.save_pretrained(<span class="hljs-string">&quot;skywork_tokenizer&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;skywork_tokenizer&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br>tokenizer.decode(tokenizer.encode(sen))<br></code></pre></td></tr></table></figure><p><img src="/images/03-基础组件之Tokenizer/1720009967366-cd480930-ca11-4d42-9bfe-442299d46cc0.png"></p><h1 id="代码汇总">【代码汇总】</h1><p>以换行为分割线，单独运行每块代码。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>sen = <span class="hljs-string">&quot;弱小的我也有大梦想!&quot;</span><br><span class="hljs-comment"># 从HuggingFace加载，输入模型名称，即可加载对于的分词器</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br><br><span class="hljs-comment"># tokenizer 保存到本地</span><br>tokenizer.save_pretrained(<span class="hljs-string">&quot;./roberta_tokenizer&quot;</span>)<br><span class="hljs-comment"># 从本地加载tokenizer</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./roberta_tokenizer/&quot;</span>)<br><br>tokens = tokenizer.tokenize(sen)<br><span class="hljs-built_in">print</span>(tokenizer.vocab)<br><span class="hljs-built_in">print</span>(tokenizer.vocab_size)<br><br><span class="hljs-comment"># 将词序列转换为id序列</span><br>ids = tokenizer.convert_tokens_to_ids(tokens)<br><span class="hljs-comment"># 将id序列转换为token序列</span><br>tokens = tokenizer.convert_ids_to_tokens(ids)<br><span class="hljs-comment"># 将token序列转换为string</span><br>str_sen = tokenizer.convert_tokens_to_string(tokens)<br><span class="hljs-comment"># 将字符串转换为id序列，又称之为编码</span><br>ids = tokenizer.encode(sen, add_special_tokens=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 将id序列转换为字符串，又称之为解码</span><br>str_sen = tokenizer.decode(ids, skip_special_tokens=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 填充</span><br>ids = tokenizer.encode(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br><span class="hljs-comment"># 截断</span><br>ids = tokenizer.encode(sen, max_length=<span class="hljs-number">5</span>, truncation=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 其他的输入</span><br>ids = tokenizer.encode(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br>attention_mask = [<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> idx != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> ids]<br>token_type_ids = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(ids)<br><br><span class="hljs-comment"># 快捷调用方式</span><br>inputs = tokenizer.encode_plus(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br>inputs = tokenizer(sen, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">15</span>)<br><br><span class="hljs-comment"># 处理batch数据</span><br>sens = [<span class="hljs-string">&quot;弱小的我也有大梦想&quot;</span>,<br>        <span class="hljs-string">&quot;有梦想谁都了不起&quot;</span>,<br>        <span class="hljs-string">&quot;追逐梦想的心，比梦想本身，更可贵&quot;</span>]<br>res = tokenizer(sens)<br><br><span class="hljs-comment"># Fast/Slow Tokenizer</span><br>sen = <span class="hljs-string">&quot;弱小的我也有大Dreaming!&quot;</span><br>fast_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>slow_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>, use_fast=<span class="hljs-literal">False</span>)<br><br>inputs = fast_tokenizer(sen, return_offsets_mapping=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(inputs.word_ids)<br><br><span class="hljs-comment"># 指定天工tokenizer的加载</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Skywork/Skywork-13B-base&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br>tokenizer.save_pretrained(<span class="hljs-string">&quot;skywork_tokenizer&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;skywork_tokenizer&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br>tokenizer.decode(tokenizer.encode(sen))<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础组件之Pipeline</title>
    <link href="/posts/c96e5647/"/>
    <url>/posts/c96e5647/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Reference：<a href="https://www.bilibili.com/video/BV1ta4y1g7bq/">【HuggingFaceTransformers-入门篇】基础组件之Pipeline</a>，<a href="https://huggingface.co/learn/nlp-course">Huggingface NLPCourse</a></p></blockquote><p>Transformers库中最基本的对象是函数。<code>pipeline()</code>将模型与其必要的预处理和后处理(PostProcessing)步骤联系起来，允许我们直接输入任何文本并获得可理解的答案。</p><p><img src="/images/02-基础组件之Pipeline/1719911161185-2fb141a1-00ff-40a5-8e22-380c4eaf2120.png">默认情况下，pipeline选择一个特定的预训练模型，该模型已针对指定的任务进行了微调。创建对象时，将下载并缓存模型。如果重新运行该命令，将改用缓存的模型，无需再次下载该模型。</p><h1 id="查看pipeline支持的任务类型">查看Pipeline支持的任务类型</h1><p><code>support_tasks</code>可以查看支持的任务类型，导入包的时候报错。</p><p><img src="/images/02-基础组件之Pipeline/1719913936764-b049e566-583f-46a8-8ada-0ff233b90474.png">按照报错提示更新对应包后重新运行。<code>pip install -U ipywidgets</code> <img src="/images/02-基础组件之Pipeline/1719914029475-43a6054c-3559-4fd8-a664-d0f48f616320.png"></p><p><img src="/images/02-基础组件之Pipeline/1719925161005-a691a7ee-d693-43c2-94c6-910dba43050d.png">打印出来，可以看到支持的任务名，其中<code>default</code>字段描述了任务默认使用的pytorch模型是哪一个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers.pipelines <span class="hljs-keyword">import</span> SUPPORTED_TASKS<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> SUPPORTED_TASKS.items():<br>    <span class="hljs-built_in">print</span>(k, v)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719925636923-cb925aac-ca7b-4087-9b7a-2f4180b5133f.png">huggingface transformers的<a href="https://huggingface.co/learn/nlp-course/chapter1/3?fw=pt">官网</a>说明也有。<img src="/images/02-基础组件之Pipeline/1719925859150-16b44c0c-ad14-4653-895e-f3c8c1ac746c.png"></p><h1 id="pipeline的创建与使用方式">Pipeline的创建与使用方式</h1><p><img src="/images/02-基础组件之Pipeline/1719927090426-1de79e13-dde7-4ca0-bb6c-6ff6321a9d82.png"></p><h2 id="根据任务类型直接创建pipeline">根据任务类型直接创建Pipeline</h2><p>这里warning的原因是因为没有指定模型，所以他自动给你分配了默认模型。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>)<br><span class="hljs-built_in">print</span>(pipe([<span class="hljs-string">&quot;very good!&quot;</span>, <span class="hljs-string">&quot;vary bad!&quot;</span>]))<br></code></pre></td></tr></table></figure></p><p><img src="/images/02-基础组件之Pipeline/1719926528702-c1cff285-d042-42d0-b675-02f5353dd727.png"></p><p><img src="/images/02-基础组件之Pipeline/1719926631053-1a02af4c-a2d4-4646-8ba6-54fb0146a891.png"></p><h2 id="指定任务类型指定模型创建基于指定模型的pipeline">指定任务类型、指定模型，创建基于指定模型的Pipeline</h2><p><a href="https://huggingface.co/models">官网这里</a>可以查看所有huggingface支持的model。 <img src="/images/02-基础组件之Pipeline/1719927298609-f439c91a-a5b3-4ab4-b677-066d3fd91731.png">很多默认的模型都是英文，这里可以指定一个中文模型，即添加参数<code>model</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719927414020-cbf9c534-e6d7-4fa8-9d21-12bc62eced39.png"></p><h2 id="预先加载模型再创建pipeline">预先加载模型，再创建Pipeline</h2><p><strong>使用这种方式时，必须同时指定model和tokenizer，不然会报错。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=model, tokenizer=tokenizer)<br><span class="hljs-built_in">print</span>(pipe(<span class="hljs-string">&quot;我觉得不太行！&quot;</span>))<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719928097184-12228345-e0a6-4142-9a30-fdad03fa0a30.png"></p><h2 id="使用gpu进行推理">使用GPU进行推理</h2><p>因为我这电脑不行，只有cpu，没法演示gpu运行的效果。（掩面痛哭.jpg）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(pipe.model.device)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719928272541-657c8316-7cca-4ec2-9e04-f26f1f4c6c88.png"></p><p>查看进行文本分类任务所用的时间代码如下。因为我装的是cpu版本，所以运行不了，会报一个AssertionError:Torch not compiled with CUDA enabled的错，装了cuda版pytorch的就不会。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> time<br>times = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    torch.cuda.synchronize()<br>    start = time.time()<br>    pipe(<span class="hljs-string">&quot;我觉得不太行！&quot;</span>)<br>    torch.cuda.synchronize()<br>    end = time.time()<br>    times.append(end - start)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">sum</span>(times) / <span class="hljs-number">100</span>)<br><br></code></pre></td></tr></table></figure> <img src="/images/02-基础组件之Pipeline/1719929357667-ffd4b5d2-9e0e-43ad-aaf8-2c5800407e9e.png"></p><p>指定在第几块cpu上运行的代码如下，即添加参数<code>device</code>。显卡是从0开始的，是数字0不是字符串0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>, device=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>指定成功的话运行效果如图。第一次使用gpu跑的时候较慢，多跑几次就快了（可能是要热热身吧）。cpu运行约20ms，gpu在6-8ms摆动。</p><p><img src="/images/02-基础组件之Pipeline/1719928921741-9a16323a-e222-435c-8220-c94d604c4ddb.png"></p><h2 id="查看pipeline参数">查看Pipeline参数</h2><p>pipeline的参数有很多个，具体字段可以去官网查，也可以用以下方法查看。定义一个qa_pipe后，发现它是属于QuestionAnsweringPipeline这个类的，代码中输入这个类然后按住Ctrl点积，即可查看这个类里参数的细节说明。</p><p><img src="/images/02-基础组件之Pipeline/1719929572945-3d7c7c73-26fa-42ac-8764-a6d2d9fa52e9.png">找到call方法，里面的args就是具体的参数说明。 <img src="/images/02-基础组件之Pipeline/1719929766259-ddfdd943-d8c7-4616-b364-8c36e94defe3.png"></p><h1 id="其他pipeline示例">其他Pipeline示例</h1><p>这是一个目标检测的示例。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">checkpoint = <span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span><br>detector = pipeline(model=checkpoint, task=<span class="hljs-string">&quot;zero-shot-object-detection&quot;</span>)<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image,ImageDraw<br><br>url = <span class="hljs-string">&quot;https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&amp;force=true&amp;w=640&quot;</span><br>im = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)<br>predictions = detector(<br>    im,<br>    candidate_labels=[<span class="hljs-string">&quot;hat&quot;</span>, <span class="hljs-string">&quot;sunglasses&quot;</span>, <span class="hljs-string">&quot;book&quot;</span>],<br>)<br><br>draw = ImageDraw.Draw(im)<br><span class="hljs-keyword">for</span> prediction <span class="hljs-keyword">in</span> predictions:<br>    box = prediction[<span class="hljs-string">&quot;box&quot;</span>]<br>    label = prediction[<span class="hljs-string">&quot;label&quot;</span>]<br>    score = prediction[<span class="hljs-string">&quot;score&quot;</span>]<br>    xmin, ymin, xmax, ymax = box.values()<br>    draw.rectangle((xmin, ymin, xmax, ymax), outline=<span class="hljs-string">&quot;red&quot;</span>, width=<span class="hljs-number">1</span>)<br>    draw.text((xmin, ymin), <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;label&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">round</span>(score,<span class="hljs-number">2</span>)&#125;</span>&quot;</span>, fill=<span class="hljs-string">&quot;red&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>给定一张图片。</p><p><img src="/images/02-基础组件之Pipeline/1719936313725-02915c61-ce34-42bb-8f55-928b9c754035.png"></p><p>输入检测目标，输出检测结果。</p><p><img src="/images/02-基础组件之Pipeline/1719936320043-0a98d290-ea8a-42cb-969a-0838f848d67f.png"><img src="/images/02-基础组件之Pipeline/1719936383047-71753763-ea7a-4a8a-b9bf-bb231347f6f9.png"></p><h1 id="解析pipeline背后的实现过程">解析Pipeline背后的实现过程</h1><h2 id="定义并加载分词器和模型">1. 定义并加载分词器和模型。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932123331-2fb7c38d-21d3-4a43-b76e-2e678dff636a.png"></p><h2 id="输入目标文本并进行分词">2. 输入目标文本并进行分词。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">input_text = <span class="hljs-string">&quot;我觉得不太行！&quot;</span><br>inputs = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932162678-fa24900d-9cb9-4833-b757-03fd4b88f4da.png"></p><h2 id="将input塞给模型rec包括模型输出的所有信息">3.将input塞给模型，rec包括模型输出的所有信息。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">res = model(**inputs)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932189844-20592fea-d6ab-4d66-b148-ad28be57211a.png"></p><h2 id="提取出logits字段将其进行归一化即softmax后的值是进行文本分类的依据">4.提取出logits字段，将其进行归一化即softmax后的值是进行文本分类的依据。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">logits = res.logits<br>logits = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932203843-38bf2d9d-0721-49ff-be1e-cadacf81998b.png"></p><h2 id="提取出softmax后logits字段中最大的值的索引id这里是0">5.提取出softmax后logits字段中最大的值的索引id，这里是0。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pred = torch.argmax(logits).item()<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932223233-b6eba2a6-c6a0-4781-a2ba-bdc8644e0834.png"></p><h2 id="id2label方法会将索引id映射为标签0对应negative1对应positive">6.id2label方法会将索引id映射为标签，0对应negative，1对应positive。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">result = model.config.id2label.get(pred)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932235704-50f241b1-f794-4cbc-a90b-cb4e42a0fe57.png"></p><h2 id="打印分类结果">7. 打印分类结果。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure><p><img src="/images/02-基础组件之Pipeline/1719932256810-aef807f2-8fbe-4347-94ff-a2807015f43c.png"></p><h1 id="代码汇总">代码汇总</h1><p>以注释为分割线，单独运行每块代码。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> transformers.pipelines <span class="hljs-keyword">import</span> SUPPORTED_TASKS<br><br><span class="hljs-comment"># 查看支持的任务类型</span><br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> SUPPORTED_TASKS.items():<br>    <span class="hljs-built_in">print</span>(k, v)<br><br><span class="hljs-comment"># 根据任务直接创建pipeline</span><br>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>)<br><span class="hljs-built_in">print</span>(pipe([<span class="hljs-string">&quot;very good!&quot;</span>, <span class="hljs-string">&quot;vary bad!&quot;</span>]))<br><br><span class="hljs-comment"># 指定任务类型、指定模型，创建基于指定模型的Pipeline</span><br>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br><span class="hljs-built_in">print</span>(pipe(<span class="hljs-string">&quot;我觉得不太行！&quot;</span>))<br><br><span class="hljs-comment"># 预先加载模型，再创建Pipeline</span><br><span class="hljs-comment"># 这种方式，必须同时指定model和tokenizer</span><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=model, tokenizer=tokenizer)<br><span class="hljs-built_in">print</span>(pipe(<span class="hljs-string">&quot;我觉得不太行！&quot;</span>))<br><br><span class="hljs-comment"># 使用GPU进行推理</span><br><span class="hljs-built_in">print</span>(pipe.model.device)<br>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>, device=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 其他pipeline实例</span><br>checkpoint = <span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span><br>detector = pipeline(model=checkpoint, task=<span class="hljs-string">&quot;zero-shot-object-detection&quot;</span>)<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image,ImageDraw<br><br>url = <span class="hljs-string">&quot;https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&amp;force=true&amp;w=640&quot;</span><br>im = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)<br><br>predictions = detector(<br>    im,<br>    candidate_labels=[<span class="hljs-string">&quot;hat&quot;</span>, <span class="hljs-string">&quot;sunglasses&quot;</span>, <span class="hljs-string">&quot;book&quot;</span>],<br>)<br><br>draw = ImageDraw.Draw(im)<br><span class="hljs-keyword">for</span> prediction <span class="hljs-keyword">in</span> predictions:<br>    box = prediction[<span class="hljs-string">&quot;box&quot;</span>]<br>    label = prediction[<span class="hljs-string">&quot;label&quot;</span>]<br>    score = prediction[<span class="hljs-string">&quot;score&quot;</span>]<br>    xmin, ymin, xmax, ymax = box.values()<br>    draw.rectangle((xmin, ymin, xmax, ymax), outline=<span class="hljs-string">&quot;red&quot;</span>, width=<span class="hljs-number">1</span>)<br>    draw.text((xmin, ymin), <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;label&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">round</span>(score,<span class="hljs-number">2</span>)&#125;</span>&quot;</span>, fill=<span class="hljs-string">&quot;red&quot;</span>)<br><br><span class="hljs-comment"># 解析Pipeline背后的实现过程</span><br><span class="hljs-keyword">import</span> torch<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)<br><br>input_text = <span class="hljs-string">&quot;我觉得不太行！&quot;</span><br>inputs = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br><br>res = model(**inputs)<br><br>logits = res.logits<br>logits = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)<br><br>pred = torch.argmax(logits).item()<br><br>result = model.config.id2label.get(pred)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace Transformers 基础知识与环境安装</title>
    <link href="/posts/31beefe2/"/>
    <url>/posts/31beefe2/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考视频：<a href="https://www.bilibili.com/video/BV1ma4y1g791/">【HuggingFaceTransformers-入门篇】基础知识与环境安装</a></p></blockquote><h1 id="创建虚拟环境">创建虚拟环境</h1><p>直接去官网安装anaconda，打开Anaconda Prompt。</p><p><img src="/images/01-Transformers环境安装/image-20240724195342704.png"></p><p>使用conda命令新建一个虚拟环境transformers，中间的y/n输入y就行。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> create -n transformers_cpu python=<span class="hljs-number">3</span>.<span class="hljs-number">9</span><br></code></pre></td></tr></table></figure><p><img src="/images/01-Transformers环境安装/1719733109005-5c71a950-a482-4052-8f71-6c5a535bbad1.png"></p><p>安装好了之后激活环境，括号里的字变成创建的环境名则成功激活。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">conda <span class="hljs-built_in">activate</span> transformers_cpu<br></code></pre></td></tr></table></figure><p><img src="/images/01-Transformers环境安装/1719750679098-1be20f9a-72a5-46c5-865c-28e91b63248a.png"></p><p>推荐换一下默认源，换成清华的pypi镜像，下载速度会快很多。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">pip config <span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span>.<span class="hljs-built_in">index</span>-url https://pypi.tuna.tsinghua.edu.<span class="hljs-keyword">cn</span>/simple <br></code></pre></td></tr></table></figure><h1 id="安装pytorch">安装pytorch</h1><p><img src="/images/01-Transformers环境安装/1719732799604-879be6ef-77d5-401f-bba2-5f28d2dca703.png"></p><h2 id="正常步骤">正常步骤</h2><p>安装pytorch，鼠标右键选择NVIDIA控制面板，点击系统信息-&gt;组件查看NVIDIAcuda版本。</p><p><img src="/images/01-Transformers环境安装/1719732410123-f746d55e-893a-451a-b719-564399bc195c.png"></p><p>到<a href="https://pytorch.org/get-started/locally/">pytorch官网</a>按照自己的cuda版本，安装比cuda低版本的pytorch版本，推荐使用pip进行安装。</p><p><img src="/images/01-Transformers环境安装/1719749802692-01bd15e9-d119-4905-a709-0dc0ce5d0fd5.png"></p><p>如果想装pytorch以前的版本在这：<a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a>，找对应版本的命令。</p><p>我电脑驱动太老了，直接装的cpu版本。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip3 <span class="hljs-keyword">install</span> torch torchvision torchaudio<br></code></pre></td></tr></table></figure><p>安装完成后，依次输入以下命令，返回True则安装成功：</p><ol type="1"><li><code>import torch</code></li><li><code>torch.cuda.is_available()</code></li><li><code>print(torch.__version__)</code></li></ol><p>因为我装的是cpu版本，所以返回的是False。</p><p><img src="/images/01-Transformers环境安装/1719751363725-1bd79a84-53dd-47c2-b8c7-329824fc22db.png"></p><h2 id="踩坑未解决">踩坑【未解决】</h2><p>因为我的cuda是9.1，最开始想安装的是cu9.0版本的，对应版本是torch-0.4.1，但这个版本太老了踩了很多坑！！！记录一下装torch-0.4.1的全过程。</p><p>安装网址：<a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a>，下载对应文件到本地。</p><p><img src="/images/01-Transformers环境安装/1719738388692-55821673-fba0-4454-8e05-5c33cbc69984.png"></p><p>然后pip install+下载路径，会提示error，说没有whl。</p><p><img src="/images/01-Transformers环境安装/1719737592714-93671e26-4dd8-4a4c-a9d3-d8b89ef6bde6.png"></p><p>找了很久，看到<a href="https://blog.csdn.net/qq_44832009/article/details/129351554">这篇博客</a>，按照里面的方法解决了这个报错。总结如下：</p><ol type="1"><li>进入下载目录，我这里下载路径是C:；</li><li>输入命令<code>pip debug --verbose</code>；</li></ol><p><img src="/images/01-Transformers环境安装/1719738279838-124fdf86-d13e-4360-81f3-31685f33c8ef.png"></p><ol type="1"><li>我原本下载文件的名字是torch-0.4.1-cp35-cp35m-win_amd64，那么找到对应的将文件名修改为torch-0.4.1-cp35-abi3-win_amd64a。</li><li><code>pip install torch-0.4.1-cp35-abi3-win_amd64.whl</code>，安装成功。</li></ol><p><img src="/images/01-Transformers环境安装/1719738423004-e487aace-3592-4bf7-832b-9a2e6f6d8337.png"></p><p><img src="/images/01-Transformers环境安装/1719749879421-9b2a4436-5433-440b-84e6-06e8a25f7e97.png"></p><p>结果到<code>import torch</code>的时候直接找不到torch，没找到解决办法......</p><p><img src="/images/01-Transformers环境安装/1719750144030-a4c3cf76-f9a7-46cf-913f-9c8330f76c9c.png"></p><h1 id="安装vscode">安装vscode</h1><p>pycharm只有专业版才能写jupyter notebook，所以这里选择vscode。</p><p>官网安装vscode，下载一些必要插件：Python、Remote-ssh(远程连服务器)、ChineseLanguage Pack(汉化包)、Jupyter等。</p><p><img src="/images/01-Transformers环境安装/1719736179242-4eb0867f-e6b6-425d-a56d-79af30889a2c.png"></p><p>设置终端，vscode默认终端是powershell，改成cmd更方便。调出终端的快捷键是ctrl+` 。</p><p><img src="/images/01-Transformers环境安装/1719736429065-a6e0c9e9-aa63-41a2-9a2d-230f22188e41.png"></p><p>重新打开vscode后配置生效。</p><p><img src="/images/01-Transformers环境安装/1719736455650-021a06e6-18c4-40fb-8c11-a92f53075987.png"></p><h1 id="安装transformers">安装transformers</h1><p>安装transfomers的基本库。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> transformers datasets evaluate peft accelerate gradio optimum sentencepiece<br></code></pre></td></tr></table></figure><p><img src="/images/01-Transformers环境安装/1719751494054-577f6692-1b44-41ad-9822-9250c938629a.png"></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">pip <span class="hljs-keyword">install </span><span class="hljs-keyword">jupyterlab </span><span class="hljs-keyword">scikit-learn </span>pandas matplotlib tensorboard nltk rouge<br></code></pre></td></tr></table></figure><p><img src="/images/01-Transformers环境安装/1719751853333-3aeb5bdf-7a8a-48c1-ac71-70774c1d51eb.png"></p><ul><li>Transformers：核心库，模型加载，模型训练，流水线等</li><li>Tokenizer：分词器，对数据进行预处理，文本到token序列的互相转换</li><li>Datasets：数据集库，提供了数据集的加载，处理等方法</li><li>Evaluate：评估函数，提供各种评价指标的计算函数</li><li>PEFT：高效微调模型的库，提供了几种高效微调的方法，小参数量撬动大模型</li><li>Accelerate：分布式训练，提供了分布式训练解决方案，包括大模型的加载与推理解决方案</li><li>Optimum：优化加速库，支持多种后端，如onnxruntime,openvino等</li><li>Gradio：可视化部署库，几行代码快速实现基于web交互的算法演示系统</li></ul><p>修改hosts文件，避免下载模型时可能出现的问题，文件路径<code>C:\Windows\System32\drivers\etc\hosts</code>。</p><p><img src="/images/01-Transformers环境安装/1719736841100-5285f6d6-6df3-4a07-9900-f9f44773f0a1.png"></p><p>拷贝以下内容到hosts文件中，保存。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plain">185.199.108.133 raw.githubusercontent.com<br>185.199.109.133 raw.githubusercontent.com<br>185.199.110.133 raw.githubusercontent.com<br>185.199.111.133 raw.githubusercontent.com<br>2606:50c0:8000::154 raw.githubusercontent.com<br>2606:50c0:8001::154 raw.githubusercontent.com<br>2606:50c0:8002::154 raw.githubusercontent.com<br>2606:50c0:8003::154 raw.githubusercontent.com<br></code></pre></td></tr></table></figure><p><img src="/images/01-Transformers环境安装/1719751252792-155eea3c-9453-46dc-b845-f51c4f3c46a4.png"></p><p>环境配置完成。</p><p>进入vscode，创建一个juoyter demo，检查环境。</p><p><img src="/images/01-Transformers环境安装/1719752386257-b4aced34-9c37-42ab-8760-74e2fec9eee9.png"></p><p>选择刚刚配置好的transformers_cpu的环境。</p><p><img src="/images/01-Transformers环境安装/1719752441413-46ec7bcb-1435-43eb-9964-3392d3908eaa.png"></p><p>运行示例代码<code>from transformers import *</code>。</p><p>第一次运行时有报错，提示版本不兼容，大概率是scipy和numpy库的版本不兼容，scipy的版本兼容numpy版本在1.16.5 到 1.23.0 之间，我的numpy版本可以看到是2.0.0。</p><p><img src="/images/01-Transformers环境安装/1719753402908-83079dc2-238f-4c5d-aedf-44c4157d5bda.png"></p><p>这时可以选择升级scipy或者降级numpy。我选的是前者，因为降级numpy报了一大堆错误orz。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install --upgrade scipy`或`pip install numpy==<span class="hljs-number">1</span>.<span class="hljs-number">16</span>.<span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><p>成功解决，环境配置完成！</p><p><img src="/images/01-Transformers环境安装/1719753117739-6c4a7437-01e0-4ed9-b8af-0a25052cfe31.png"></p><h1 id="简单实例">简单实例</h1><p>先配置好python解释器的环境，选择transformers_cpu。</p><p><img src="/images/01-Transformers环境安装/1719753600848-53df9d07-c154-4b51-9532-cb13ac129b1f.png"></p><h2 id="文本分类">文本分类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入gradio</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-comment"># 导入transformers相关包</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> *<br><span class="hljs-comment"># 通过Interface加载pipeline并启动文本分类服务</span><br>gr.Interface.from_pipeline(pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=<span class="hljs-string">&quot;uer/roberta-base-finetuned-dianping-chinese&quot;</span>)).launch()<br></code></pre></td></tr></table></figure><p>运行代码，可以看到在下载对应的模型，默认下载路径<code>C:\Users\Administrator\.cache\huggingface\hub\</code>。</p><p><img src="/images/01-Transformers环境安装/1719753795221-2fe914db-5ce8-4d2c-9c95-26cb42d32a94.png"></p><p>可以在调用from_pretrained函数时使用cache_dir入参，指定缓存文件夹名。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-chinese&#x27;</span>, <span class="hljs-attribute">cache_dir</span>=<span class="hljs-string">&#x27;./&#x27;</span>)<br></code></pre></td></tr></table></figure><p>下载完成后，点击这个连接，访问文本分类模型。</p><p><img src="/images/01-Transformers环境安装/1719757014381-1d67a062-a9d4-4361-9911-e963813d57fa.png"></p><p>但是有报错，查了一下应该还是numpy版本的问题，还是太高了，所以需要降级numpy。</p><p><img src="/images/01-Transformers环境安装/1719758136956-e7765538-acdc-4f1c-96cb-67d87ea8b92a.png"></p><p><img src="/images/01-Transformers环境安装/1719758091420-4568e1bc-0c72-4482-9131-9ad0b584e598.png"></p><p><code>pip install -U numpy==1.16.5</code>，报了一大堆错……</p><p><img src="/images/01-Transformers环境安装/1719758337219-1c8a9777-5af9-4bc3-96fd-8bae85e3a202.png"></p><p><code>pip install -U numpy==1.20.3</code>，查了下python3.9对应的numpy版本时1.20.3，试了一下，这次装成功了，但还是报错。</p><p><img src="/images/01-Transformers环境安装/1719760719065-a93df1ff-f823-461f-a500-fcda037055ac.png"></p><p><img src="/images/01-Transformers环境安装/1719760079764-1022b772-f4a6-49ac-b486-d78abd9f212a.png"></p><p>matplotlib 3.9.0需要numpy&gt;=1.23，pandas2.2.2需要numpy&gt;=1.22.4，scipy1.13.1需要2.3&gt;numpy&gt;=1.22.4。尝试安装1.23.0，<code>pip install -U numpy==1.23.0</code>，没报错！</p><p><img src="/images/01-Transformers环境安装/1719761348042-04e72a51-c095-42e4-8ab0-6358c94574aa.png"></p><p>重新运行代码，成功了！！！！</p><p><img src="/images/01-Transformers环境安装/1719761236687-9b19f056-008a-4832-b34c-ed4e0cbc32d7.png"></p><h2 id="阅读理解">阅读理解</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入gradio</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-comment"># 导入transformers相关包</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br><span class="hljs-comment"># 通过Interface加载pipeline并启动阅读理解服务</span><br><span class="hljs-comment"># 如果无法通过这种方式加载，可以采用离线加载的方式</span><br>gr.Interface.from_pipeline(pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=<span class="hljs-string">&quot;uer/roberta-base-chinese-extractive-qa&quot;</span>)).launch()<br></code></pre></td></tr></table></figure><p>有了之前的经验，第二个实例就很顺利了！</p><p><img src="/images/01-Transformers环境安装/1719812400053-bca4f9bf-4c17-4092-af2c-7b8e7f42da40.png"></p><p><img src="/images/01-Transformers环境安装/1719812470403-d4848f3c-252a-4598-bdd8-e0a13b021822.png"></p>]]></content>
    
    
    <categories>
      
      <category>Huggingface Transformers-入门篇</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Transformer学习笔记</title>
    <link href="/posts/bda47da5/"/>
    <url>/posts/bda47da5/</url>
    
    <content type="html"><![CDATA[<h1 id="transformer整体结构">Transformer整体结构</h1><p>Transformer是一个用于NLP序列到序列（seq2seq）任务的模型架构，创新的引入了自注意力机制，在处理序列数据时表现出色。它主要由以下两个部分组成：Encoder和Decoder，整体结构如下图。</p><figure><img src="/images/Transformer学习笔记/5a7bf9578472e74ba3a1b1f99b2f8789.png" alt="Transformer整体结构"><figcaption aria-hidden="true">Transformer整体结构</figcaption></figure><ul><li><strong>Encoder</strong>：红色部分；<strong>Decoder</strong>：蓝色部分。</li><li><strong>Input</strong>(BPE+PE)：输入由两部分组成，分别是BPE向量和位置向量。<ol type="1"><li><a href="https://blog.csdn.net/qq_41020633/article/details/123622667"><strong>BPE</strong></a>(BytePairEncoding)：分词算法，通过将之前没见过的单词切分成见过的subword，从而表达更多的单词，再通过embedding获得向量表示。</li><li><strong>PE</strong>(Positional Encoding)：在原有的embedding上加上一个位置向量，以区分不同位置的相同单词。</li></ol></li></ul><figure><img src="/images/Transformer学习笔记/3fd640ca201181e8c42ce1fe2b92033c.png" alt="位置编码计算方式"><figcaption aria-hidden="true">位置编码计算方式</figcaption></figure><ul><li><strong>Model</strong>：Encoder和Decoder块的堆叠。Nx是堆叠的Transformerblock的数量，论文中Nx=6。</li><li><strong>Onput</strong>(Liner+Softmax)：输出在词表上的概率分布。</li><li><strong>Loss function</strong>：标准交叉熵函数。</li></ul><p>总的来说，Transformer的工作流程如下：</p><ol type="1"><li>输入的文本进行分词后，每个词会获得一个向量表示即embedding，这个embedding由BPE和PE相加得到。因此，一句话的向量表示时词向量的堆叠，即一个向量矩阵。</li><li>将得到的表示向量矩阵传入Encoder中，经过六个Encoder后得到输入句子的编码矩阵<span class="math inline">\(X\)</span>。</li><li>编码矩阵 <span class="math inline">\(X\)</span>会进入Decoder中，Decoder会根据已输入的 <span class="math inline">\(i\)</span> 个词依次预测第 <span class="math inline">\(i+1\)</span>个词。为了防止模型看到后面的词来预测前面的输入，会采取mask操作盖住<span class="math inline">\(i+1\)</span> 后的词。</li></ol><h1 id="encoder">Encoder</h1><p><img src="/images/Transformer学习笔记/915ecd1d98dd650291c3eb894bc37462.png" alt="Encoder"> Encoder部分包含三个部分：</p><ol type="1"><li>多头注意力层（<code>Multi-Head Attention</code>）</li><li>前馈神经网络（<code>Feed Forward</code>）</li><li>残差连接（红线部分，即<code>Add</code>操作）和正则化层（<code>Norm</code>操作）</li></ol><h2 id="muti-head-attention多头注意力机制">1. Muti-HeadAttention：多头注意力机制</h2><p>Muti-HeadAttention是由多个Self-Attention组成的，所以先着重关注Self-Attention的内部结构。Self-Attention希望每个token自主选择应该关注这句话中的哪些token，并进行信息的整合。</p><h3 id="masked-self-attention掩码自注意力机制">1.1 Maskedself-attention：掩码自注意力机制</h3><p><img src="/images/Transformer学习笔记/f681d244dc2fc4c4a00686948cd65567.png" alt="Masked self-attention"> 可以看到，这一部分接收的输入是 $ Q, K, V$三个矩阵，这部分的计算方式如下：</p><p><span class="math display">\[\rm Attention(\it Q,K,V)=\rm softmax(\it  QK^T)V.\]</span></p><p>其中， <span class="math inline">\(Q, K, V\)</span> 是由输入 <span class="math inline">\(X\)</span> 与三个不同的权重矩阵 <span class="math inline">\(W^Q,W^K,W^V\)</span> 相乘的结果，本质上都是 <span class="math inline">\(X\)</span> 的线性变换。</p><blockquote><p>设 <span class="math inline">\(X\)</span> 是 <span class="math inline">\(m\times n\)</span> 的矩阵， <span class="math inline">\(W^Q,W^K,W^V\)</span> 是 <span class="math inline">\(n\times k\)</span> 的矩阵，最后得到的 <span class="math inline">\(Q,K,V\)</span> 则是<span class="math inline">\(m\times k\)</span>的矩阵。</p><ul><li><span class="math inline">\({QK}^T\)</span> 相乘后得到<span class="math inline">\(m\times m\)</span>的矩阵；</li><li>经过<span class="math inline">\(\rmsoftmax\)</span>层后得到注意力分数的分布，矩阵维度仍是 <span class="math inline">\(m\times m\)</span>；</li><li>注意力分数与<span class="math inline">\(V\)</span>相乘，还原成维度为<span class="math inline">\(m\times k\)</span> 的矩阵输出，与输入的<span class="math inline">\(X\)</span>一一对应。</li></ul></blockquote><figure><img src="/images/Transformer学习笔记/a6e22592627c416790470fb11e11fbe7.png" alt="Attention计算过程"><figcaption aria-hidden="true">Attention计算过程</figcaption></figure><p>总结来说，整个Attention计算过程就是，矩阵 <span class="math inline">\(Q,K\)</span>相乘得到注意力分数，通过scale系数对规模进行放缩，再通过softmax将注意力分数变成一个概率分布，最后与对应的矩阵<span class="math inline">\(V\)</span> 进行矩阵乘法，实现对矩阵 <span class="math inline">\(V\)</span> 的加权平均。</p><p>详细计算过程可以看这篇博客：<a href="https://blog.csdn.net/weixin_42110638/article/details/134016569">一文搞定自注意力机制</a>。</p><blockquote><p>在这里要先和RNN里的注意力机制做一下区分：</p><ul><li>RNN的注意力机制：给定一个 <span class="math inline">\(query\)</span>向量和 <span class="math inline">\(value\)</span> 向量的集合，基于 <span class="math inline">\(query\)</span> 向量对 <span class="math inline">\(value\)</span> 向量进行加权平均，采用 <span class="math inline">\(q\)</span> 和 <span class="math inline">\(v\)</span> 计算注意力分数。</li><li>Transformer采用的是基于点积的注意力机制，给定 <span class="math inline">\(query\)</span> 向量、<span class="math inline">\(key\)</span>和<span class="math inline">\(value\)</span>向量对的集合，采用$ q$ 和 <span class="math inline">\(k\)</span> 的点积来计算注意力分数。</li></ul></blockquote><h3 id="scaled-dot-product-attention引入放缩系数">1.2 Scaled dot-productattention：引入放缩系数</h3><p>随着key向量维度的增长，<span class="math inline">\(q\)</span> 和<span class="math inline">\(k\)</span>的点积得到的标量(值)的方差也会变大，如果直接作用到softmax函数，得到的概率分布会变得更加尖锐，大部分概率分布接近于0，导致梯度越来越小，不利于参数的更新。</p><p>所以在这个基础上，引入一个<code>scaled系数</code>，组成一个完整的Scaleddot-product attention模块，即在原有的基础上除以一个 <span class="math inline">\(\sqrt{d_k}\)</span>，即向量维度的开方。因此，最终的计算方式如下：</p><p><span class="math display">\[\rm Attention(\it Q,K,V)=\rm softmax(\it\frac {QK^T}{\sqrt {d_k}})V.\]</span></p><h3 id="muti-head-attention多头注意力机制-1">1.3 Muti-HeadAttention：多头注意力机制</h3><figure><img src="/images/Transformer学习笔记/21235d74288f0568af03a6193ee4436e.png" alt="矩阵Q,K,V都是从文本的表示向量乘上一个变换矩阵的到的"><figcaption aria-hidden="true">矩阵Q,K,V都是从文本的表示向量乘上一个变换矩阵的到的</figcaption></figure><p>Transformer采用了多个结构相同，但是参数不同的注意力模块(h个)组成多头注意力机制。每个注意力头的计算方式相同，但每个注意力头<span class="math inline">\(i\)</span> 对应权重矩阵的<span class="math inline">\(W_i^Q,W_i^K,W_i^V\)</span>不同。</p><p><code>即每个注意力头中 X 乘上的权重矩阵不同，每个注意力头进行不同的线性变换</code>。</p><figure><img src="/images/Transformer学习笔记/af1f23643d9a4ec93d8f69756d04c24f.png" alt="多注意力头"><figcaption aria-hidden="true">多注意力头</figcaption></figure><ul><li><strong>Input</strong>：如果是第一层Encoder，输入是embedding和位置编码的相加，非第一层的Encoder的输入是前一层的输出。</li><li><strong>Scaled dot-productattention层</strong>：计算注意力分数。</li><li><strong>Concat层</strong>：将注意力头得到的输出进行拼接，再通过线性层整合得到最终输出。</li><li><strong>Output</strong>：Muti-headattention模块的输出经过残差连接和正则化后，输入到后面的前馈神经网络。</li></ul><h3 id="一个embedding计算例子">1.4 一个embedding计算例子</h3><p><strong>Step1.</strong> 计算单词Thinking、Machines的embedding，得到<span class="math inline">\({x_1,x_2}\)</span>。</p><p><strong>Step2.</strong> <span class="math inline">\({x_1,x_2}\)</span> 与权重矩阵 $ W^Q, W^K, W^V$相乘，得到 <span class="math inline">\({q_1,k_1,v_1}\)</span>。</p><p><strong>Step3.</strong> 计算注意力分数，即${q_1,k_1}$的点积，这里假设 <span class="math inline">\({q_1 · k_1}=112,{q_2· k_2}=96\)</span> （$ {q_i,k_i}$ 堆叠起来就是矩阵 <span class="math inline">\(Q, K\)</span> ，即 $ {QK}^T$ 计算的结果）。</p><p><strong>Step4.</strong> 除以scaled系数<span class="math inline">\(\sqrt{d_k}\)</span>，一般来说是向量维度的开方，这里假设<span class="math inline">\(d_k=64\)</span>。</p><p><strong>Step5.</strong> 通过计算softmax得到注意力分数的概率分布：<span class="math display">\[\rm softmax\it (x)=\frac{e^{x_i}}{\sum_i e^{x_i}},\rm softmax(x_1)=\frac{e^{12}}{e^{14}+e^{12}}=0.88,\rm softmax(x_2)=\frac{e^{14}}{e^{14}+e^{12}}=0.12.\]</span></p><p><strong>Step6.</strong> 将得到的概率分布矩阵与$ v_1$相乘，得到最后的输出$ z_1$ （矩阵$ V$ 是每个$ v_i$堆叠起来的结果，相乘后的$ z_i$ 堆叠起来的得到最后的输出矩阵 <span class="math inline">\(Z\)</span> 。</p><figure><img src="/images/Transformer学习笔记/efc34d5b592f444c977eb8f3c028a7ad.png" alt="一个embedding计算例子"><figcaption aria-hidden="true">一个embedding计算例子</figcaption></figure><h2 id="feed-forward-network前馈神经网络">2. Feed-ForwardNetwork：前馈神经网络</h2><p>这部分时两层的MLP层（全连接层），第一层的激活函数为Relu，第二层不使用激活函数，对应的公式如下： <span class="math display">\[max(0,XW_1+b_1)W_2+b_2.\]</span></p><h2 id="addnorm残差连接与正则化">3. Add&amp;Norm：残差连接与正则化</h2><p>这一层包括<code>残差连接</code>（Residualconnection）和<code>正则化</code>（Layernormalization），就是将每一小块的输入和这一小块的输出相加后再输入到下一块，即图中的红线部分。计算公式如下：</p><ul><li>第一个Add&amp;Norm层：<span class="math inline">\(LayerNorm(X+MultiHeadAttention(X))\)</span></li><li>第二个Add&amp;Norm层：<span class="math inline">\(LayerNorm(X+FeedForward(X))\)</span></li></ul><p><code>残差连接</code>借鉴于CV领域中的ResNet，将输入输出直接相加，缓解模型过深导致的梯度消失问题。</p><p><code>正则化</code>会将输入的向量变成一个均值为0，方差为1的分布，缓解梯度消失/爆炸问题。</p><h1 id="decoder">Decoder</h1><p><img src="/images/Transformer学习笔记/8f895e501141b24f140798198b7f82da.png" alt="Decoder"> Decoder部分包含四个部分：</p><ol type="1"><li>采用了掩码的多头注意力层（<code>Masked Multi-Head Attention</code>）</li><li>多头注意力层（<code>Multi-Head Attention</code>）</li><li>前馈神经网络（<code>Feed Forward</code>）</li><li>残差连接（<code>Add</code>操作）和正则化层（<code>Norm</code>操作）</li></ol><h2 id="masked-self-attention掩码自注意力机制-1">1. Maskedself-attention：掩码自注意力机制</h2><p>为了限制保证Decoder端生成文本的时候是顺序生成的，<strong>不会在生成第i个位置的时候参考i+1位置的信息</strong>，使用mask让$Q, K$相乘得到的注意力分数的上三角部分的值变成负无穷大。</p><p>这样一来，它们经过softmax之后那些位置对应的概率会变为0，使得模型在当前输出的步骤看不到后面的单词。<strong>注意Mask 操作是在 Self-Attention 里 Softmax 层之前使用的。</strong></p><figure><img src="/images/Transformer学习笔记/b48ef80600624032a938afb748709178.png" alt="带掩码的注意力计算"><figcaption aria-hidden="true">带掩码的注意力计算</figcaption></figure><h2 id="muti-head-attention多头注意力机制-2">2. Muti-headattention：多头注意力机制</h2><p>Decoder里这个Muti-headattention模块与Encoder里的略有不同，简而言之，Decoder的$Q$分为两种情况：</p><ul><li>如果是第一个Decoder block，则通过输入矩阵$ X$ 计算得到$ Q$；</li><li>如果不是第一个，则通过上一Decoder block的输出$ Z$ 计算得到 $Q$。</li></ul><p>而Decoder的 <span class="math inline">\({K,V}\)</span>则是通过Encoder的输出 <span class="math inline">\(C\)</span>计算得到的。</p><p>这个设计是为了帮助Decoder每一步生成都可以关注和整合Encoder端每个位置的信息，让每一个单词都可以在Decoder的时候利用到Encoder所有单词的信息。</p><h2 id="具体计算过程">3. 具体计算过程</h2><p>详细计算过程可以参考这篇博客：<a href="https://blog.csdn.net/Xiao_Ya__/article/details/139303221">Transformer模型详解</a>。这里引用其中的几张图总结一下：</p><p><strong>Step1.</strong> 输入单词向量矩阵 $ X$ 和Mask矩阵 $ M$ 。 <img src="/images/Transformer学习笔记/3b4d9042b0814e3c909dee3915183a5d.png" alt="Step1"></p><p><strong>Step2.</strong> 像之前一样通过输入矩阵$ X$ 得到矩阵 <span class="math inline">\({Q,K,V}\)</span> 后，计算 $ {QK}^T$ 。 <img src="/images/Transformer学习笔记/ebd999b0fed345ddbc43f46bd871086d.png" alt="Step2"></p><p><strong>Step3.</strong> 将 <span class="math inline">\({QK}^T\)</span> 与Mask矩阵 <span class="math inline">\(M\)</span> 相乘，使得矩阵上三角值为0（被遮盖）。<img src="/images/Transformer学习笔记/241ee8414f094a5fbee7c8649445868b.png" alt="Step3"></p><p><strong>Step4.</strong> <span class="math inline">\({Mask \QK}^T\)</span> 与矩阵<span class="math inline">\(V\)</span> 相乘得到输出<span class="math inline">\(Z\)</span> ，其中 <span class="math inline">\(Z\)</span> 的每一行 <span class="math inline">\(z_i\)</span> 只包含单词 <span class="math inline">\(i\)</span> 的信息。 <img src="/images/Transformer学习笔记/1d9d4083b8154e99be4cdbf82f7d7eab.png" alt="Step4"></p><p><strong>Step5.</strong> Softmax 根据输出矩阵的每一行预测下一个单词。<img src="/images/Transformer学习笔记/c27607428849412d8f32c3a33073509c.png" alt="Step5"></p><h1 id="other-tricks">Other tricks</h1><ul><li>训练过程采用了Checkpointaveraging技术，使用Adam优化器进行参数更新。</li><li>为了提高模型的训练效果防止过拟合，在残差连接之前加上了dropout。</li><li>在输出层加入label smoothing来提高训练效率。</li><li>在生产过程中采用了更复杂的一个生成策略(Auto-regressive decoding withbeam search and length penalties)。</li></ul><h1 id="advantage">Advantage</h1><ul><li>强表示能力，可以迁移到别的NLP任务中；</li><li>结构本身适合并行计算，对目前GPU等加速设备非常友好；</li><li>对attenition机制进行可视化，可以发现注意力模块很好地建模了句子中token之间的关系；</li><li>给预训练语言模型带来了很多启发(Bert,GPT)，成为目前预训练模型最主要的一个框架。</li></ul><h1 id="disadvantage">Disadvantage</h1><ul><li>模型本身对参数非常敏感，优化过程很困难；</li><li>模型复杂度是文本长度n的平方，导致对长文本束手无策，当前很多模型会设置一个最大输入长度512。</li></ul><hr><blockquote><p>原始论文：《Attention is all you need》(NeurIPS 2017)</p><p>参考视频：<a href="https://www.bilibili.com/video/BV1rS411F735">THUNLP-Transformer</a>，<a href="https://www.bilibili.com/video/BV1Q7421f75Y?p=4">马士兵Transformer详解</a></p><p>参考文章：<a href="https://blog.csdn.net/weixin_42110638/article/details/134016569">一文搞定自注意力机制</a>，<a href="https://blog.csdn.net/Xiao_Ya__/article/details/139303221">Transformer模型详解</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>PLMs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
